
::: {.callout-note appearance="simple" collapse="true"}

### Expand to see environment variables

We are running mothur in [Batch Mode](https://mothur.org/wiki/batch_mode/) and using Environment Variables to generalize the batch commands for reuse. The format of environmental variables is `[tag]=[value]`. mothur will automatically pull in the systems environment variable, which we can set in bash and then run the batch file:

```         
$ export DATA=01_TRIMMED_DATA/
$ export TYPE=fastq
$ export PROC=30

$ export REF_LOC=reference_dbs
$ export TAXREF_FASTA=gsrdb.fasta
$ export TAXREF_TAX=gsrdb.tax
$ export ALIGNREF=silva.v4.fasta

$ export CONTAMINENTS=Chloroplast-Mitochondria-unknown-Eukaryota
```
:::

```{r}
#| echo: false
#| eval: true
load("files/rdata/otu_part1.rdata")
```

## Getting Started


::: {.callout-note appearance="simple" collapse="true"}
### Expand for the MOTHUR batchfile

{{< include include/_mothur_batchfile.qmd>}}
:::

::: {.column-margin}
{{< include /include/_chunk_colors.qmd >}}
:::

```{verbatim}
set.dir(output=pipelineFiles/)
```

```         
Mothur's directories:
outputDir=pipelineFiles/
```

```{verbatim}
make.file(inputdir=$DATA, type=$TYPE, prefix=shrimp)
```

```         
Setting input directories to: 
    01_TRIMMED_DATA/

Output File Names: 
shrimp.files
```

## Reducing Sequencing & PCR Errors

```{verbatim}
make.contigs(file=current, processors=$PROC)
```

We will get the following message if sample names contain dash (`-`) characters. Mothur will change this for us.

```         
[WARNING]: group Control-10 contains illegal characters in the name. 
Group names should not include :, -, or / characters.  The ':' character 
is a special character used in trees. Using ':' will result in your tree 
being unreadable by tree reading software.  The '-' character is a special 
character used by mothur to parse group names.  Using the '-' character 
will prevent you from selecting groups. The '/' character will created 
unreadable filenames when mothur includes the group in an output filename.

[NOTE] Updating Control-10 to Control_10 to avoid downstream issues.

...

Total of all groups is 44710450

It took 1257 secs to process 44710450 sequences.

Output File Names: 
shrimp.trim.contigs.fasta
shrimp.scrap.contigs.fasta
shrimp.contigs_report
shrimp.contigs.count_table
```

```{verbatim}
summary.seqs(fasta=current, count=current, processors=$PROC)
```

::: {.callout-note appearance="simple" collapse="true"}

### Expand to see data set summary

|             | Start | End | NBases | Ambigs | Polymer | NumSeqs  |
|:------------|:-----:|:---:|:------:|:------:|:-------:|:--------:|
| Minimum     |   1   | 226 |  226   |   0    |    2    |    1     |
| 2.5%-tile:  |   1   | 252 |  252   |   0    |    3    | 1117762  |
| 25%-tile:   |   1   | 253 |  253   |   0    |    4    | 11177613 |
| Median:     |   1   | 253 |  253   |   0    |    4    | 22355226 |
| 75%-tile:   |   1   | 253 |  253   |   0    |    5    | 33532838 |
| 97.5%-tile: |   1   | 254 |  254   |   6    |    6    | 43592689 |
| Maximum:    |   1   | 480 |  480   |   95   |   233   | 44710450 |
| Mean:       |   1   | 254 |  254   |   0    |    4    |          |

```         
# of unique seqs:   44710450
total # of seqs:    44710450

It took 782 secs to summarize 44710450 sequences.

Output File Names:
shrimp.trim.contigs.summary
```
:::

```{verbatim}
count.groups(count=current)
```

```         
Size of smallest group: 58.

Total seqs: 44710450.
```

```{verbatim}
screen.seqs(fasta=current, count=current, maxambig=0, minlength=252, maxlength=254, maxhomop=6, processors=$PROC)
```

```         
Using 30 processors.

It took 107 secs to screen 44710450 sequences, removed 8308318.

Output File Names:
shrimp.contigs.pick.count_table

/******************************************/

Output File Names:
shrimp.trim.contigs.good.fasta
shrimp.trim.contigs.bad.accnos
shrimp.contigs.good.count_table

It took 557 secs to screen 44710450 sequences.
```

```{verbatim}
summary.seqs(fasta=current, count=current, processors=$PROC)
```

::: {.callout-note appearance="simple" collapse="true"}

### Expand to see data set summary

|             | Start | End | NBases | Ambigs | Polymer | NumSeqs  |
|:------------|:-----:|:---:|:------:|:------:|:-------:|:--------:|
| Minimum     |   1   | 252 |  252   |   0    |    3    |    1     |
| 2.5%-tile:  |   1   | 252 |  252   |   0    |    3    |  910054  |
| 25%-tile:   |   1   | 253 |  253   |   0    |    4    | 9100534  |
| Median:     |   1   | 253 |  253   |   0    |    4    | 18201067 |
| 75%-tile:   |   1   | 253 |  253   |   0    |    5    | 27301600 |
| 97.5%-tile: |   1   | 254 |  254   |   6    |    6    | 35492079 |
| Maximum:    |   1   | 254 |  254   |   0    |    6    | 36402132 |
| Mean:       |   1   | 252 |  252   |   0    |    4    |          |

```         
# of unique seqs:   36402132
total # of seqs:    36402132

It took 610 secs to summarize 36402132 sequences.

Output File Names:
shrimp.trim.contigs.good.summary
```
:::

```{verbatim}
count.groups(count=current)
```

```         
Size of smallest group: 57.

Total seqs: 36402132.
Output File Names: 
shrimp.contigs.good.count.summary
```

## Processing Improved Reads

```{verbatim}
unique.seqs(fasta=current, count=current)
```

```         
36402132    4224192

Output File Names: 
shrimp.trim.contigs.good.unique.fasta
shrimp.trim.contigs.good.count_table
```

```{verbatim}
summary.seqs(count=current, processors=$PROC)
```

::: {.callout-note appearance="simple" collapse="true"}

### Expand to see data set summary

|             | Start | End | NBases | Ambigs | Polymer | NumSeqs  |
|:------------|:-----:|:---:|:------:|:------:|:-------:|:--------:|
| Minimum     |   1   | 252 |  252   |   0    |    3    |    1     |
| 2.5%-tile:  |   1   | 252 |  252   |   0    |    3    |  910054  |
| 25%-tile:   |   1   | 253 |  253   |   0    |    4    | 9100534  |
| Median:     |   1   | 253 |  253   |   0    |    4    | 18201067 |
| 75%-tile:   |   1   | 253 |  253   |   0    |    5    | 27301600 |
| 97.5%-tile: |   1   | 254 |  254   |   6    |    6    | 35492079 |
| Maximum:    |   1   | 254 |  254   |   0    |    6    | 36402132 |
| Mean:       |   1   | 252 |  252   |   0    |    4    |          |

```         
# of unique seqs:   4224192
total # of seqs:    36402132

It took 78 secs to summarize 36402132 sequences.

Output File Names:
shrimp.trim.contigs.good.unique.summary
```
:::

## Aligning Reads

Since we are using the `silva.nr_v132.align` to align sequences, we can check where our reads start and end with the [ARB-SILVA web aligner](https://www.arb-silva.de/aligner/). After uploading a few sequences we find they start at postion 13862 and end at position 23445. Neat. We will pad these numbers to make sure we do not miss anything. 

```{verbatim}
pcr.seqs(fasta=$REF_LOC/silva.nr_v132.align, start=11895, end=25318, keepdots=F, processors=$PROC)
```

```         
Using 30 processors.
[NOTE]: no sequences were bad, removing silva.nr_v132.bad.accnos

It took 10 secs to screen 213119 sequences.

Output File Names: 
silva.nr_v132.pcr.align
```

```{verbatim}
rename.file(input=$REF_LOC/silva.nr_v132.pcr.align, new=$REF_LOC/$ALIGNREF)
summary.seqs(fasta=$REF_LOC/$ALIGNREF, processors=$PROC)
```

::: {.callout-note appearance="simple" collapse="true"}

### Expand to see data set summary

|             | Start |  End  | NBases | Ambigs | Polymer | NumSeqs |
|:------------|:-----:|:-----:|:------:|:------:|:-------:|:-------:|
| Minimum     |   1   | 9876  |  218   |   0    |    3    |    1    |
| 2.5%-tile:  |   1   | 13424 |  291   |   0    |    4    |  5328   |
| 25%-tile:   |   1   | 13424 |  292   |   0    |    4    |  53280  |
| Median:     |   1   | 13424 |  292   |   0    |    5    | 106560  |
| 75%-tile:   |   1   | 13424 |  292   |   0    |    5    | 159840  |
| 97.5%-tile: |   1   | 13424 |  458   |   1    |    6    | 207792  |
| Maximum:    | 4225  | 13424 |  1520  |   5    |   16    | 213119  |
| Mean:       |   1   | 13423 |  307   |   0    |    4    |         |

```         
# of Seqs:  213119

It took 5 secs to summarize 213119 sequences.

Output File Names:
silva.v4.summary
```
:::

```{verbatim}
align.seqs(fasta=current, reference=$REF_LOC/$ALIGNREF, processors=$PROC)
```

```         
Using 30 processors.

Reading in the silva.v4.fasta template sequences... DONE.
It took 34 to read  213119 sequences.

Aligning sequences from shrimp.trim.contigs.good.unique.fasta ...
It took 780 secs to align 4224192 sequences.

[WARNING]: 217 of your sequences generated alignments that 
eliminated too many bases, a list is provided in 
shrimp.trim.contigs.good.unique.flip.accnos.
[NOTE]: 136 of your sequences were reversed to produce a better alignment.

It took 780 seconds to align 4224192 sequences.

Output File Names: 
shrimp.trim.contigs.good.unique.align
shrimp.trim.contigs.good.unique.align_report
shrimp.trim.contigs.good.unique.flip.accnos
```

```{verbatim}
summary.seqs(fasta=current, count=current, processors=$PROC)
```

::: {.callout-note appearance="simple" collapse="true"}

### Expand to see data set summary

|             | Start |  End  | NBases | Ambigs | Polymer | NumSeqs  |
|:------------|:-----:|:-----:|:------:|:------:|:-------:|:--------:|
| Minimum     |   1   | 1235  |   2    |   0    |    1    |    1     |
| 2.5%-tile:  | 1968  | 11550 |  252   |   0    |    3    |  910054  |
| 25%-tile:   | 1968  | 11550 |  253   |   0    |    4    | 9100534  |
| Median:     | 1968  | 11550 |  253   |   0    |    4    | 18201067 |
| 75%-tile:   | 1968  | 11550 |  253   |   0    |    5    | 27301600 |
| 97.5%-tile: | 1968  | 11550 |  253   |   0    |    6    | 35492079 |
| Maximum:    | 13422 | 13424 |  254   |   0    |    6    | 36402132 |
| Mean:       | 1968  | 11549 |  252   |   0    |    4    |          |

```         
# of unique seqs:   4224192
total # of seqs:    36402132

It took 141 secs to summarize 36402132 sequences.

Output File Names:
shrimp.trim.contigs.good.unique.summary
```
:::

```{verbatim}
screen.seqs(fasta=current, count=current, start=1968, end=11550, processors=$PROC)
```

```         
Using 30 processors.

It took 66 secs to screen 4224192 sequences, removed 7623.

/******************************************/
Running command: 
remove.seqs(accnos=shrimp.trim.contigs.good.unique.bad.accnos.temp, 
count=shrimp.trim.contigs.good.count_table)
Removed 25154 sequences from shrimp.trim.contigs.good.count_table.

Output File Names:
shrimp.trim.contigs.good.pick.count_table

/******************************************/

Output File Names:
shrimp.trim.contigs.good.unique.good.align
shrimp.trim.contigs.good.unique.bad.accnos
shrimp.trim.contigs.good.good.count_table
```

```{verbatim}
summary.seqs(fasta=current, count=current, processors=$PROC)
```

::: {.callout-note appearance="simple" collapse="true"}

### Expand to see data set summary

|             | Start |  End  | NBases | Ambigs | Polymer | NumSeqs  |
|:------------|:-----:|:-----:|:------:|:------:|:-------:|:--------:|
| Minimum     |   1   | 11550 |  240   |   0    |    3    |    1     |
| 2.5%-tile:  | 1968  | 11550 |  252   |   0    |    3    |  909425  |
| 25%-tile:   | 1968  | 11550 |  253   |   0    |    4    | 9094245  |
| Median:     | 1968  | 11550 |  253   |   0    |    4    | 18188490 |
| 75%-tile:   | 1968  | 11550 |  253   |   0    |    5    | 27282734 |
| 97.5%-tile: | 1968  | 11550 |  253   |   0    |    6    | 35467554 |
| Maximum:    | 1968  | 13424 |  254   |   0    |    6    | 36376978 |
| Mean:       | 1967  | 11550 |  252   |   0    |    4    |          |

```         
# of unique seqs:   4216569
total # of seqs:    36376978

It took 152 secs to summarize 36376978 sequences.

Output File Names:
shrimp.trim.contigs.good.unique.good.summary
```
:::

```{verbatim}
count.groups(count=current)
```

```         
Size of smallest group: 57.

Total seqs: 36376978.

Output File Names: 
shrimp.trim.contigs.good.good.count.summary
```

```{verbatim}
filter.seqs(fasta=current, vertical=T, trump=., processors=$PROC)
```

```         
Using 30 processors.
Creating Filter...
It took 31 secs to create filter for 4216569 sequences.


Running Filter...
It took 26 secs to filter 4216569 sequences.



Length of filtered alignment: 736
Number of columns removed: 12688
Length of the original alignment: 13424
Number of sequences used to construct filter: 4216569

Output File Names: 
shrimp.filter
shrimp.trim.contigs.good.unique.good.filter.fasta
```

```{verbatim}
unique.seqs(fasta=current, count=current)
```

```         
4216569 4192289

Output File Names: 
shrimp.trim.contigs.good.unique.good.filter.unique.fasta
shrimp.trim.contigs.good.unique.good.filter.count_table
```

```{verbatim}
count.groups(count=current)
```

```         
Size of smallest group: 57.

Total seqs: 36376978.

Output File Names: 
shrimp.trim.contigs.good.unique.good.filter.count.summary
```

```{verbatim}
summary.seqs(fasta=current, count=current, processors=$PROC)
```

::: {.callout-note appearance="simple" collapse="true"}

### Expand to see data set summary

|             | Start | End | NBases | Ambigs | Polymer | NumSeqs  |
|:------------|:-----:|:---:|:------:|:------:|:-------:|:--------:|
| Minimum     |   1   | 735 |  208   |   0    |    3    |    1     |
| 2.5%-tile:  |   1   | 736 |  252   |   0    |    3    |  909425  |
| 25%-tile:   |   1   | 736 |  253   |   0    |    4    | 9094245  |
| Median:     |   1   | 736 |  253   |   0    |    4    | 18188490 |
| 75%-tile:   |   1   | 736 |  253   |   0    |    5    | 27282734 |
| 97.5%-tile: |   1   | 736 |  253   |   0    |    6    | 35467554 |
| Maximum:    |   4   | 736 |  254   |   0    |    6    | 36376978 |
| Mean:       |   1   | 735 |  252   |   0    |    4    |          |

```         
# of unique seqs:   4192289
total # of seqs:    36376978

It took 88 secs to summarize 36376978 sequences.

Output File Names:
shrimp.trim.contigs.good.unique.good.filter.unique.summary
```
:::

## Precluster

```{verbatim}
pre.cluster(fasta=current, count=current, diffs=2, processors=$PROC)
```

::: {.callout-note appearance="simple" collapse="true"}

### Expand to see partial output of `pre.cluster`

```         
Using 30 processors.

/******************************************/
Splitting by sample: 

Using 30 processors.

Selecting sequences for groups Control_1-Control_10-Control_11-Control_12-
Control_13-Control_14-Control_15-Control_16-Control_17-Control_18


Selecting sequences for groups EP_A_PANA_EG_9332-EP_A_PANA_EG_9338-
EP_A_PANA_EG_9339-EP_A_PANA_GL_7322-EP_A_PANA_GL_7326-
EP_A_PANA_GL_7327-EP_A_PANA_GL_7329-EP_A_PANA_GL_7330-
EP_A_PANA_GL_7331-EP_A_PANA_GL_7332
....

Selected 1054 sequences from Control_1.
Selected 695 sequences from Control_10.
Selected 902 sequences from Control_11.
Selected 216 sequences from Control_12.
Selected 212 sequences from Control_13.
Selected 75 sequences from Control_14.
Selected 488 sequences from Control_15.
Selected 246 sequences from Control_16.
Selected 47 sequences from Control_17.
Selected 171 sequences from Control_18.

Selecting sequences for groups Control_19-Control_2-
Control_20-Control_21-Control_22-Control_23-Control_24-
Control_25-Control_26-Control_27

Selected 4720 sequences from EP_A_PANA_EG_9332.
Selected 4278 sequences from EP_A_PANA_EG_9338.
Selected 689 sequences from EP_A_PANA_EG_9339.
Selected 3148 sequences from EP_A_PANA_GL_7322.
Selected 3989 sequences from EP_A_PANA_GL_7326.
Selected 3552 sequences from EP_A_PANA_GL_7327.
Selected 3621 sequences from EP_A_PANA_GL_7329.
Selected 2322 sequences from EP_A_PANA_GL_7330.
Selected 6021 sequences from EP_A_PANA_GL_7331.
Selected 4267 sequences from EP_A_PANA_GL_7332.

Selecting sequences for groups EP_A_PANA_GL_8353-
EP_A_PANA_GL_8792-EP_A_PANA_GL_8948-EP_A_PANA_GL_8951-
EP_A_PANA_GL_8952-EP_A_PANA_GL_9021-EP_A_PANA_GL_9022-
EP_A_PANA_GL_9131-EP_A_PANA_GL_9264-EP_A_PANA_GL_9332
```
:::

```{verbatim}
summary.seqs(fasta=current, count=current, processors=$PROC)
```

::: {.callout-note appearance="simple" collapse="true"}

### Expand to see data set summary

|             | Start | End | NBases | Ambigs | Polymer | NumSeqs  |
|:------------|:-----:|:---:|:------:|:------:|:-------:|:--------:|
| Minimum     |   1   | 735 |  208   |   0    |    3    |    1     |
| 2.5%-tile:  |   1   | 736 |  252   |   0    |    3    |  909425  |
| 25%-tile:   |   1   | 736 |  253   |   0    |    4    | 9094245  |
| Median:     |   1   | 736 |  253   |   0    |    4    | 18188490 |
| 75%-tile:   |   1   | 736 |  253   |   0    |    5    | 27282734 |
| 97.5%-tile: |   1   | 736 |  253   |   0    |    6    | 35467554 |
| Maximum:    |   4   | 736 |  254   |   0    |    6    | 36376978 |
| Mean:       |   1   | 735 |  252   |   0    |    4    |          |

```         
# of unique seqs:   1649819
total # of seqs:    36376978

It took 35 secs to summarize 36376978 sequences.

Output File Names:
shrimp.trim.contigs.good.unique.good.filter.unique.precluster.summary
```
:::

```{verbatim}
count.groups(count=current)
```

```         
Size of smallest group: 57.

Total seqs: 36376978.

Output File Names: 
shrimp.trim.contigs.good.unique.good.filter.unique.precluster.count.summary
```

## Remove Negative Controls

Now we need to remove the NC samples *and* reads found in those sample. We first identified all reads that were present in at least one NC sample represented by at least 1 read. We did this by subsetting the NC samples from dataset.

```{verbatim}
get.groups(fasta=current, count=current, groups=Control_49-Control_23-Control_17-Control_24-Control_14-Control_44-Control_20-Control_33-Control_41-Control_29-Control_50-Control_22-Control_19-Control_18-Control_48-Control_13-Control_21-Control_16-Control_30-Control_5-Control_42-Control_25-Control_51-Control_40-Control_15-Control_36-Control_47-Control_27-Control_32-Control_8-Control_3-Control_4-Control_6-Control_45-Control_26-Control_46-Control_53-Control_7-Control_12-Control_10-Control_9-Control_35-Control_54-Control_2-Control_43-Control_1-Control_11-Control_52-Control_38-Control_34-Control_56-Control_37-Control_28-Control_57-Control_31-Control_39-Control_59-Control_55-Control_60-Control_58)
```

```         
Selected 193014 sequences from your count file.
Selected 4133 sequences from your fasta file.

Output File names:
shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.count_table
shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.fasta
```

```{verbatim}
rename.file(input=current, new=neg_control.fasta)
rename.file(input=current, new=neg_control.count_table)
```

```{verbatim}
summary.seqs(fasta=neg_control.fasta, count=neg_control.count_table, processors=$PROC)
```

::: {.callout-note appearance="simple" collapse="true"}

### Expand to see negative control summary

|             | Start | End | NBases | Ambigs | Polymer | NumSeqs |
|:------------|:-----:|:---:|:------:|:------:|:-------:|:-------:|
| Minimum     |   1   | 735 |  251   |   0    |    3    |    1    |
| 2.5%-tile:  |   1   | 736 |  252   |   0    |    4    |  4826   |
| 25%-tile:   |   1   | 736 |  253   |   0    |    4    |  48254  |
| Median:     |   1   | 736 |  253   |   0    |    5    |  96508  |
| 75%-tile:   |   1   | 736 |  253   |   0    |    5    | 144761  |
| 97.5%-tile: |   1   | 736 |  254   |   0    |    6    | 188189  |
| Maximum:    |   4   | 736 |  254   |   0    |    6    | 193014  |
| Mean:       |   1   | 735 |  253   |   0    |    4    |         |

```         
# of unique seqs:   4133
total # of seqs:    193014

It took 0 secs to summarize 193014 sequences.

Output File Names:
neg_control.summary
```
:::

```{verbatim}
list.seqs(count=neg_control.count_table)
```

```         
Output File Names: 
neg_control.accnos
```

The next step is to use the `*.accnos` file from the previous step to remove any reads found in negative control (NC) samples. This seems reasonable enough except mothur will remove *any* read found in a NC sample. For example, let's say we have two reads:

`read01` is found in most NC samples and not found in any remaining samples.   
`read02` on the other hand is represented by one read in a single NC sample but very abundant in remaining samples.   

It makes a lot of sense to remove `read01` but not so for `read02`. So we need to make a custom `*.accnos` that only contains reads that are abundant in NC samples. To do this we will do a little data wrangling in R. For this we need two `*.count_table` files--one from the `precluster` step and the other from the step above. We got the idea on how best to do this from the [mothur forum](https://forum.mothur.org/t/negative-control/2754).

Essentially, for each repseq, the code below calculates:

-   The total number of NC samples containing at least 1 read.\
-   The total number of reads in NC samples.\
-   The total number of non-NC samples containing at least 1 read.\
-   The total number of reads in non-NC samples.\
-   The percent of reads in the NC samples and the percent of NC samples containing reads.

```{r}
#| echo: false
#| comment: i dont thik this needs to be run?
sed 1,2d neg_control.count_table > tmp_neg.count_table
sed 1,2d shrimp.trim.contigs.good.unique.good.filter.unique.precluster.count_table > tmp.count_table
```

The first command parses out the necessary data from the  `.count_table` files. 

```{r}
#| echo: true
#| eval: false
tmp_nc_reads <- read_tsv(
  "files/tables/neg_control.count_table",
  col_names = TRUE,
  col_types = NULL,
  skip = 2,
  col_select = c("Representative_Sequence", "total")
)
tmp_non_nc_reads <- read_tsv(
  "files/tables/shrimp.trim.contigs.good.unique.good.filter.unique.precluster.count_table",
  col_names = TRUE,
  col_types = NULL,
  skip = 2,
  col_select = c("Representative_Sequence", "total"),
)
tmp_reads <- dplyr::left_join(tmp_nc_reads, tmp_non_nc_reads,
                              by = "Representative_Sequence")
tmp_reads <- tmp_reads %>% dplyr::rename(c(
                            "total_reads_NC" = "total.x", 
                            "total_reads_samps" = "total.y")
                            ) 
```

And here is what the new dataframe looks like. Three columns where the first is the repseq name, the second the total number of reads in NC samples, and the third the total number of reads in the entire dataset. 

```{r}
#| echo: false
#| eval: true
nc_tmp1
```


We identified 4,133 reads that were potential contaminants. 

```{r}
#| echo: false
#| eval: false
write.table(tmp_reads, "files/tables/neg_control_screen.txt", 
            row.names = FALSE, quote = FALSE, sep = "\t")
```

Now we add in a column that calculates the percent of reads in the NC samples. 

```{r}
#| echo: true
#| eval: false
tmp_nc_check <- tmp_reads %>%
  dplyr::mutate(perc_in_neg = 100*(
    total_reads_NC / (total_reads_NC + total_reads_samps)),
                .after = "total_reads_samps")
tmp_nc_check$perc_in_neg <- round(tmp_nc_check$perc_in_neg, digits = 6)
```

Now we use the `count.seqs` command in mothur to generate a file describing the distribution of NC reads across all samples. This information will tell us how many samples contain these reads. 

```{verbatim}
count.seqs(count=neg_control.count_table, compress=f)
count.seqs(count=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.count_table, compress=f)
```

```{r}
#| echo: true
#| eval: false
tmp_nc_dist <- read_tsv("files/tables/neg_control.full.count_table")

tmp_all_dist <- read_tsv("files/tables/shrimp.trim.contigs.good.unique.good.filter.unique.precluster.full.count_table")
tmp_all_dist <- tmp_all_dist %>% select(-starts_with("Control_"))

tmp_nc_dist$total <- NULL
tmp_all_dist$total <- NULL

tmp_nc_read_dist <- dplyr::left_join(
                    tmp_nc_dist, 
                    tmp_all_dist, 
                    by = "Representative_Sequence")
```

```{r}
#| echo: false
#| eval: false
write.table(tmp_nc_read_dist, "nc_read_dist_full.count_table", 
            row.names = FALSE, quote = FALSE, sep = "\t")
tmp_nc_read_dist <- read_tsv(
  "files/tables/nc_read_dist_full.count_table",
  col_names = TRUE
)
```

After a little wrangling, the dataframe looks like this: 

```{r}
#| echo: false
#| eval: true
#nc_tmp2 <- tmp_nc_read_dist[1:6, 1:5]
nc_tmp2
```

And then we calculate row sums to get the number of NC and non-NC samples containing these reads. 

```{r}
#| echo: true
#| eval: false
tmp_1 <- data.frame(rowSums(tmp_nc_read_dist != 0))
tmp_2 <- dplyr::select(tmp_nc_read_dist, contains("Control"))
tmp_2$num_samp_nc <- rowSums(tmp_2 != 0)
tmp_2 <- dplyr::select(tmp_2, -contains("Control"))
tmp_3 <- dplyr::select(tmp_nc_read_dist, -contains("Control"))
tmp_3$num_samp_no_nc <- rowSums(tmp_3 != 0)
tmp_3 <- dplyr::select(tmp_3, contains("num_samp_no_nc"))
```

And again calculate the percent of NC samples containing these reads. 

```{r}
#| echo: true
#| eval: false
tmp_nc_check <- cbind(tmp_nc_check, tmp_1, tmp_2, tmp_3)
tmp_nc_check <- tmp_nc_check %>% dplyr::rename("total_samples" = 5)  
colnames(tmp_nc_check)
tmp_nc_check <- tmp_nc_check %>%
  dplyr::mutate(perc_in_neg_samp = 
                  100*( num_samp_nc / (num_samp_nc + num_samp_no_nc)),
                  .after = "num_samp_no_nc")
```

```{r}
#| echo: false
#| eval: false
nc_check <- tmp_nc_check
```

```{r}
#| echo: false
#| eval: true
  reactable(nc_check,
  defaultColDef = colDef(
    header = function(value) gsub("_", " ", value, fixed = TRUE),
    #cell = function(value) format(value, nsmall = 0),
    align = "center", filterable = FALSE, sortable = TRUE, resizable = TRUE,
    footerStyle = list(fontWeight = "bold")
    ), 
  columns = list(
    Representative_Sequence = colDef(name = "Rep Seq", 
                       sticky = "left", 
                       style = list(borderRight = "1px solid #eee"),
                       headerStyle = list(borderRight = "1px solid #eee"), 
                       align = "left",
                       minWidth = 100),
  total_reads_NC = colDef(name = "reads in NC"),
  total_reads_samps = colDef(name = "reads in non NC"),
  perc_in_neg = colDef(name = "% in NC", format = colFormat(digits = 4)),
  total_samples = colDef(name = "total samples"),
  num_samp_nc = colDef(name = "Total NC samples"),
  num_samp_no_nc = colDef(name = "Total non-NC samples"),
  perc_in_neg_samp = colDef(name = "% of samples", format = colFormat(digits = 4))
  ),
  searchable = FALSE, defaultPageSize = 5, 
  pageSizeOptions = c(5, 10, nrow(nc_check)), 
  showPageSizeOptions = TRUE, highlight = TRUE, 
  bordered = TRUE, striped = TRUE, compact = FALSE, 
  wrap = FALSE, showSortable = TRUE, fullWidth = TRUE,
  theme = reactableTheme(style = list(fontSize = "0.8em"))) %>%
  reactablefmtr::add_subtitle("Summary of reads detected in Negative Control (NC) samples.", 
                              font_size = 15)
```

```{r}
#| echo: false
#| eval: false
write_delim(nc_check, "include/tables/reads_in_nc_samples.txt",
    delim = "\t")
```

{{< downloadthis include/tables/reads_in_nc_samples.txt dname=reads_in_nc_samples label="Download summary of reads detected in Negative Control (NC) samples" icon=table type=info class=data-button id=reads_in_nc_samples >}}

Now we remove any repseqs where:

-   The number of reads found in NC samples accounted for more than 10% of total reads OR
-   The percent of NC samples containing the ASV was greater than 10% of total samples.

```{r}
#| echo: true
#| eval: false
nc_remove <- tmp_nc_check %>% 
  filter(perc_in_neg > 10 | perc_in_neg_samp > 10)
tmp_rem_otu <- nc_remove$Representative_Sequence %>% 
  unlist(strsplit(., split = ", ")) 
```

```{r}
#| echo: false
#| eval: false
nc_remain <- dplyr::anti_join(nc_check, nc_remove)

rem_nc_reads <- sum(nc_remove$total_reads_NC)
rem_sam_reads <- sum(nc_remove$total_reads_samps)
per_reads_rem <- round(100*( rem_nc_reads / (rem_nc_reads + rem_sam_reads)), 
                       digits = 3)

ret_nc_reads <- sum(nc_remain$total_reads_NC)
ret_sam_reads <- sum(nc_remain$total_reads_samps)
per_reads_ret <- round(100*( ret_nc_reads / (ret_nc_reads + ret_sam_reads)), 
                       digits = 3)
```

|          | Total OTUs         | NC reads         | non NC reads      | \% NC reads       |
|---------------|---------------|---------------|---------------|---------------|
| Removed  | `r nrow(nc_remove)` | `r rem_nc_reads` | `r rem_sam_reads` | `r per_reads_rem` |
| Retained | `r nrow(nc_remain)` | `r ret_nc_reads` | `r ret_sam_reads` | `r per_reads_ret` |

We identified a total of `r nrow(nc_check)` OTUs that were present in at least 1 NC sample by at least 1 read. We removed any OTUs where more than 10% of total reads were found in NC samples OR any OTU found in more than 10% of NC samples. Based on these criteria we removed `r nrow(nc_remove)` OTUs from the data set, which represented `r rem_nc_reads` total reads in NC samples and `r rem_sam_reads` total reads in non-NC samples. Of the total reads removed `r per_reads_rem`% came from NC samples. Of all OTUs identified in NC samples,`r nrow(nc_remain)` were retained because they fell below the threshhold criteria. These OTUs accounted for `r ret_nc_reads` reads in NC samples and `r ret_sam_reads` reads in non-NC samples. NC samples accounted for `r per_reads_ret`% of these reads.

OK, now we can create a new `neg_control.accnos` containing only repseqs abundant in NC samples.

```{r}
#| echo: true
#| eval: false
write_delim(
  data.frame(nc_remove$Representative_Sequence), 
  "files/tables/neg_control_subset.accnos", 
  col_names = FALSE)
```

And then use this file in conjunction with the mothur command `remove.seqs`. 

```{verbatim}
remove.seqs(accnos=neg_control_subset.accnos, fasta=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.fasta, count=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.count_table)
```

```         
Removed 3831 sequences from shrimp.trim.contigs.good.unique.good.filter.unique.precluster.fasta.
Removed 332770 sequences from shrimp.trim.contigs.good.unique.good.filter.unique.precluster.count_table.

Output File Names:
shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.fasta
shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.count_table
```

```{verbatim}
count.groups(count=current)
```

```         
Size of smallest group: 1.

Total seqs: 36044208.

Output File Names: 
shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.count.summary
```

Before we remove the NC samples we need to check whether some NC samples were already removed. When mothur runs the `remove.seqs` command it will automatically remove any samples where the read count has fallen to zero. If mothur did remove samples and we try to remove all NC samples, we will get an error. To check we can compare the `count.summary` files before and after the previous `remove.seqs` command.

```{r}
#| echo: true
#| eval: false 
tmp_before <- read_tsv(
  "files/tables/shrimp.trim.contigs.good.unique.good.filter.unique.precluster.count.summary",
  col_names = FALSE,
  col_select = 1
)
tmp_after <- read_tsv(
  "files/tables/shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.count.summary",
  col_names = FALSE,
  col_select = 1
)
tmp_nc_lost <- anti_join(tmp_before, tmp_after)
tmp_nc_lost$X1
```

These are the samples that were already removed when `remove.seqs` was run above. We need to remove these from our list of NC samples. 

```         
[1] "Control_15" "Control_18" "Control_21" "Control_29" "Control_5" 
```

```{r}
#| echo: true
#| eval: false 
nc_to_remove <- semi_join(tmp_before, tmp_after)
nc_to_remove <- nc_to_remove %>%
  dplyr::filter(
    stringr::str_starts(X1, "Control")
    )
nc_to_remove <- paste0(nc_to_remove$X1, collapse="-")
```

```{r}
#| echo: true
#| eval: true 
nc_to_remove
```

```{verbatim}
remove.groups(count=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.count_table, fasta=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.fasta, groups=Control_1-Control_10-Control_11-Control_12-Control_13-Control_14-Control_16-Control_17-Control_19-Control_2-Control_20-Control_22-Control_23-Control_24-Control_25-Control_26-Control_27-Control_28-Control_3-Control_30-Control_31-Control_32-Control_33-Control_34-Control_35-Control_36-Control_37-Control_38-Control_39-Control_4-Control_40-Control_41-Control_42-Control_43-Control_44-Control_45-Control_46-Control_47-Control_48-Control_49-Control_50-Control_51-Control_52-Control_53-Control_54-Control_55-Control_56-Control_57-Control_58-Control_59-Control_6-Control_60-Control_7-Control_8-Control_9)
```

```         
Removed 35932 sequences from your count file.
Removed 0 sequences from your fasta file.

Output File names: 
shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.count_table
shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.fasta
```

```{verbatim}
summary.seqs(fasta=current, count=current, processors=$PROC)
```

::: {.callout-note appearance="simple" collapse="true"}

### Expand to see data set summary

|             | Start | End | NBases | Ambigs | Polymer | NumSeqs  |
|:------------|:-----:|:---:|:------:|:------:|:-------:|:--------:|
| Minimum     |   1   | 735 |  208   |   0    |    3    |    1     |
| 2.5%-tile:  |   1   | 736 |  252   |   0    |    3    |  900207  |
| 25%-tile:   |   1   | 736 |  253   |   0    |    4    | 9002070  |
| Median:     |   1   | 736 |  253   |   0    |    4    | 18004139 |
| 75%-tile:   |   1   | 736 |  253   |   0    |    5    | 27006208 |
| 97.5%-tile: |   1   | 736 |  253   |   0    |    6    | 35108070 |
| Maximum:    |   4   | 736 |  254   |   0    |    6    | 36008276 |
| Mean:       |   1   | 735 |  252   |   0    |    4    |          |

```         
# of unique seqs:   1645988
total # of seqs:    36008276

It took 28 secs to summarize 36008276 sequences.

Output File Names:
shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.summary
```
:::

```{verbatim}
count.groups(count=current)
```

```         
Size of smallest group: 14.

Total seqs: 36008276.

Output File Names: 
shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.count.summary
```

## Remove Chimeras

```{verbatim}
chimera.vsearch(fasta=current, count=current, dereplicate=t, processors=$PROC)
summary.seqs(fasta=current, count=current, processors=$PROC)
count.groups(count=current)
```

```         
Using vsearch version v2.15.2.
Checking sequences from shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.fasta ...

/******************************************/
Splitting by sample: 

...

Removing chimeras from your input files:
/******************************************/
Running command: 
remove.seqs(fasta=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.fasta,
accnos=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.accnos)
Removed 622662 sequences from shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.fasta.

Output File Names:
shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.pick.fasta

/******************************************/

Output File Names:
shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.count_table
shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.chimeras
shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.accnos
shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.fasta
```

```{verbatim}
summary.seqs(fasta=current, count=current, processors=30)
```

::: {.callout-note appearance="simple" collapse="true"}

### Expand to see data set summary

|             | Start | End | NBases | Ambigs | Polymer | NumSeqs  |
|:------------|:-----:|:---:|:------:|:------:|:-------:|:--------:|
| Minimum     |   1   | 735 |  208   |   0    |    3    |    1     |
| 2.5%-tile:  |   1   | 736 |  252   |   0    |    3    |  865457  |
| 25%-tile:   |   1   | 736 |  253   |   0    |    4    | 8654567  |
| Median:     |   1   | 736 |  253   |   0    |    4    | 17309134 |
| 75%-tile:   |   1   | 736 |  253   |   0    |    5    | 25963700 |
| 97.5%-tile: |   1   | 736 |  253   |   0    |    6    | 33752810 |
| Maximum:    |   4   | 736 |  254   |   0    |    6    | 34618266 |
| Mean:       |   1   | 735 |  252   |   0    |    4    |          |

```         
# of unique seqs:   1023326
total # of seqs:    34618266

It took 22 secs to summarize 34618266 sequences.

Output File Names:
shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.summary
```
:::

```{verbatim}
count.groups(count=current)
```

```         
Size of smallest group: 14.

Total seqs: 34618266.

Output File Names: 
shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.count.summary
```

## Assign Taxonomy

The `classify.seqs` command requires properly formatted reference and taxonomy databases. For taxonomic assignment, we are using the GSR database [@molano2024gsr]. The developers of mothur maintain [formatted versions of popular databases](https://mothur.org/wiki/taxonomy_outline/), however the GSR-DB has not been formatted by the developers yet.

::: callout-note
You can download an appropriate version of the GSR database [here](https://manichanh.vhir.org/gsrdb/download_db_links2.php).
:::

To create a mothur formatted version GSR-DB[^_merge_runs-1], we perform the following steps.

[^_merge_runs-1]: From the developers: GSR database (Greengenes, SILVA, and RDP database) is an integrated and manually curated database for bacterial and archaeal 16S amplicon taxonomy analysis. Unlike previous integration approaches, this database creation pipeline includes a taxonomy unification step to ensure consistency in taxonomical annotations. The database was validated with three mock communities and two real datasets and compared with existing 16S databases such as Greengenes, GTDB, ITGDB, SILVA, RDP, and MetaSquare. Results showed that the GSR database enhances taxonomical annotations of 16S sequences, outperforming current 16S databases at the species level. The GSR database is available for full-length 16S sequences and the most commonly used hypervariable regions: V4, V1-V3, V3-V4, and V3-V5.

#### Download a data base

Here we are using the [GSR V4 database](https://manichanh.vhir.org/gsrdb/GSR-DB_V4_cluster-1.tar.gz). 

```{zsh}
#| echo: true
#| eval: false
wget https://manichanh.vhir.org/gsrdb/GSR-DB_V4_cluster-1.tar.gz
tar -xvzf GSR-DB_V4_cluster-1.tar.gz
```

First (in the command line) we remove first line of the taxonomy file. 

```{zsh}
cp GSR-DB_V4_cluster-1_taxa.txt tmp0.txt
sed '1d' tmp0.txt > tmp1.txt
```

Next, delete species and remove leading \[a-z\]\_\_ from taxa names

```{zsh}
sed -E 's/s__.*//g' tmp1.txt > tmp2.txt
sed -E 's/[a-zA-Z]__//g' tmp2.txt > gsrdb.tax
cp GSR-DB_V4_cluster-1_seqs.fasta gsrdb.fasta
```

```{verbatim}
classify.seqs(fasta=current, count=current, reference=$REF_LOC/$TAXREF_FASTA, taxonomy=$REF_LOC/$TAXREF_TAX, processors=$PROC)
```

```         
Using 30 processors.
Generating search database...    DONE.
It took 2 seconds generate search database.

Reading in the reference_dbs/gsrdb.txt taxonomy...  DONE.
Calculating template taxonomy tree...     DONE.
Calculating template probabilities...     DONE.
It took 6 seconds get probabilities.
shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.fasta ...
[WARNING]: M06508_9_000000000-JTBW3_1_1102_26159_16839 could not be classified. 
You can use the remove.lineage command with taxon=unknown; 
to remove such sequences.
...

It took 348 secs to classify 1023326 sequences.

It took 503 secs to create the summary file for 1023326 sequences.

Output File Names: 
shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.gsrdb.wang.taxonomy
shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.gsrdb.wang.tax.summary
```

## Remove Contaminants

```{verbatim}
remove.lineage(fasta=current, count=current, taxonomy=current, taxon=$CONTAMINENTS)
```

```         
Running command: 
remove.seqs(accnos=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.gsrdb.wang.accnos, 
count=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.count_table, 
fasta=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.fasta)

Removed 560 sequences from shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.fasta.
Removed 6712 sequences from shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.count_table.

/******************************************/

Output File Names:
shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.gsrdb.wang.pick.taxonomy
shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.gsrdb.wang.accnos
shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.pick.count_table
shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.pick.fasta
```

```{verbatim}
summary.tax(taxonomy=current, count=current)
```

```         
Using shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.pick.count_table 
as input file for the count parameter.
Using shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.gsrdb.wang.pick.taxonomy 
as input file for the taxonomy parameter.

It took 489 secs to create the summary file for 34611554 sequences.

Output File Names: 
shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.gsrdb.wang.pick.tax.summary
```

```{verbatim}
count.groups(count=current)
```

```         
Size of smallest group: 14.

Total seqs: 34611554.

Output File Names: 
shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.pick.count.summary
```

## Track Reads through Workflow

At this point we can look at the number of reads that made it through each step of the workflow for every sample.

```{r}
#| echo: true
#| eval: false
read_change <- read_tsv(
  "include/tables/all_sample_otu_read_changes.txt",
  col_names = TRUE
)
```

```{r}
#| echo: false
#| eval: true
reactable(read_change,
  defaultColDef = colDef(
    header = function(value) gsub("_", " ", value, fixed = TRUE),
    cell = function(value) format(value, nsmall = 1),
    align = "center", filterable = TRUE, sortable = TRUE, resizable = TRUE,
    footerStyle = list(fontWeight = "bold")
    ), 
  columns = list(
    Sample_ID = colDef(name = "Sample ID", 
                       sticky = "left", 
                       style = list(borderRight = "1px solid #eee"),
                       headerStyle = list(borderRight = "1px solid #eee"), 
                       align = "left",
                       minWidth = 150, footer = "Total reads"), 
    input = colDef(name = "input", footer = function(values) sprintf("%.0f", sum(values))),
    screen_seqs = colDef(footer = function(values) sprintf("%.0f", sum(values))), 
    align_screen = colDef(footer = function(values) sprintf("%.0f", sum(values))), 
    remove_nc = colDef(footer = function(values) sprintf("%.0f", sum(values))), 
    nochim = colDef(footer = function(values) sprintf("%.0f", sum(values))), 
    no_contam = colDef(footer = function(values) sprintf("%.0f", sum(values)))
    ), 
  searchable = TRUE, defaultPageSize = 5, 
  pageSizeOptions = c(5, 10, 50, nrow(read_change)), 
  showPageSizeOptions = TRUE, highlight = TRUE, 
  bordered = TRUE, striped = TRUE, compact = FALSE, 
  wrap = FALSE, showSortable = TRUE, fullWidth = TRUE,
  theme = reactableTheme(style = list(fontSize = "0.8em"))) %>%
  reactablefmtr::add_subtitle("Tracking read changes at each step of the mothur workflow.", 
                              font_size = 15)
```

{{< downloadthis include/tables/all_sample_otu_read_changes.txt dname=all_sample_otu_read_changes label="Download read changes for mothur pipeline" icon=table type=info class=data-button id=all_sample_otu_read_changes >}}

## Preparing for analysis

```{verbatim}
rename.file(fasta=current, count=current, taxonomy=current, prefix=final)
```

```         
Current files saved by mothur:
fasta=final.fasta
taxonomy=final.taxonomy
count=final.count_table
```

## Clustering

```{verbatim}
cluster.split(fasta=final.fasta, count=final.count_table, taxonomy=final.taxonomy, taxlevel=4, cluster=f, processors=$PROC)
cluster.split(file=final.file, count=final.count_table, processors=$PROC)
```

```         
Using 30 processors.
Splitting the file...
/******************************************/
Selecting sequences for group Vibrionales (1 of 364)
Number of unique sequences: 92783

Selected 5390956 sequences from final.count_table.

Calculating distances for group Vibrionales (1 of 364):

Sequence    Time    Num_Dists_Below_Cutoff

It took 902 secs to find distances for 92783 sequences. 
477552179 distances below cutoff 0.03.

Output File Names:
final.0.dist

...

It took 8671 seconds to cluster
Merging the clustered files...
It took 14 seconds to merge.
[WARNING]: Cannot run sens.spec analysis without a column file, 
skipping.
Output File Names: 
final.opti_mcc.list
```

```{verbatim}
system(mkdir cluster.split.gsrdb) 
system(mv final.opti_mcc.list cluster.split.gsrdb/) 
system(mv final.file cluster.split.gsrdb/) 
system(mv final.dist cluster.split.gsrdb/)
```

```{verbatim}
dist.seqs(fasta=final.fasta, cutoff=0.03, processors=\$PROC) cluster(column=final.dist, count=final.count_table)
```

```         
Sequence    Time    Num_Dists_Below_Cutoff

It took 91935 secs to find distances for 1022766 sequences. 
1096480673 distances below cutoff 0.03.

Output File Names: 
final.dist

You did not set a cutoff, using 0.03.

Clustering final.dist

iter    time    label   num_otus    cutoff  tp  tn  fp  fn  sensitivity specificity ppv npv fdr accuracy    mcc f1score

0.03
0   0   0.03    1022766 0.03    0   5.21928e+11 0   1.09648e+09 0   1   0   0.997904    1   0.997904    0   0   
1   3187    0.03    130371  0.03    7.80436e+08 5.21829e+11 9.9517e+07  3.16045e+08 0.711764    0.999809    0.886906    0.999395    0.886906    0.999205    0.794146    0.789741    
2   3706    0.03    119919  0.03    7.82225e+08 5.21828e+11 9.99504e+07 3.14256e+08 0.713396    0.999808    0.8867  0.999398    0.8867  0.999208    0.794965    0.790663    
3   3712    0.03    119453  0.03    7.82257e+08 5.21828e+11 9.99331e+07 3.14224e+08 0.713425    0.999809    0.886722    0.999398    0.886722    0.999208    0.794991    0.790689    

It took 21013 seconds to cluster

Output File Names: 
final.opti_mcc.list
final.opti_mcc.steps
final.opti_mcc.sensspec
```

```{r}
#| echo: false
#| eval: false
objects()
gdata::keep(nc_check, nc_remain, nc_remove, nc_to_remove,
            no_read_sum, per_reads_rem, per_reads_ret, 
            rem_nc_reads, rem_sam_reads, ret_nc_reads, 
            ret_sam_reads, read_change, nc_tmp1, nc_tmp2,
            sure = TRUE)
save.image("files/rdata/otu_part1.rdata")
```
