### 1. Set Working Environment

:::{.callout-note}
The following plate were sequenced in this run: ISTHMO S5, S5, S7, S8.  You can access the source code for this section [here](include/_BCS_26.qmd) 
:::

First, we setup the working environment by defining a path for the working directory.

```{r}
#| echo: true
#| eval: false
path <- "BCS_26/"
head(list.files(path)) 
```

```{r}
#| echo: false
#| eval: true
c("Control_10_R1_001.trimmed.fastq", "Control_10_R2_001.trimmed.fastq", 
  "Control_11_R1_001.trimmed.fastq", "Control_11_R2_001.trimmed.fastq", 
  "Control_12_R1_001.trimmed.fastq", "Control_12_R2_001.trimmed.fastq")
```

Then, we generate matched lists of the forward and reverse read files. We also parse out the sample name.

```{r}
#| echo: true
#| eval: false
fnFs <- sort(list.files(path, pattern = "_R1_001.trimmed.fastq"))
fnRs <- sort(list.files(path, pattern = "_R2_001.trimmed.fastq"))
sample.names <- sapply(strsplit(fnFs, "_R1_"), `[`, 1)
fnFs <- file.path(path, fnFs)
fnRs <- file.path(path, fnRs)
```

### 2. Plot quality scores

And next we plot a visual summary of the distribution of quality scores as a function of sequence position for the input fastq files. Here we set the number of records to sample from  each fastq file (`n`) to  20000 reads. 

```{r}
#| echo: true
#| eval: false
#| warning: false
qprofile_fwd <- plotQualityProfile(fnFs[1:x], 
                                   aggregate = TRUE, 
                                   n = 20000)
qprofile_rev <- plotQualityProfile(fnRs[1:x], 
                                   aggregate = TRUE, 
                                   n = 20000)
qprofile <- grid.arrange(qprofile_fwd, qprofile_rev, nrow = 1)
qprofile <- grid.arrange(qprofile_fwd, qprofile_rev, nrow = 1)
```

```{r}
#| echo: false
#| eval: false
ggsave("figures/BCS_26_filt_plot_qscores.png", qprofile, width = 7, height = 3)
```

```{r}
#| echo: false
#| eval: true
system("cp files/figures/BCS_26_filt_plot_qscores.png include/figures/BCS_26_filt_plot_qscores.png")
```

```{r}
#| echo: false
#| eval: true
#| fig-height: 4
#| fig-width: 4
#| warning: false
#| fig-cap: "Aggregated quality score plots for forward (left) & reverse (right) reads."
knitr::include_graphics("include/figures/BCS_26_filt_plot_qscores.png")
```

### 3. Filtering

We again make some path variables and setup a new directory of filtered reads.

```{r}
#| echo: true
#| eval: false
filtFs <- file.path(path, "filtered", 
                    paste0(sample.names, "_F_filt.fastq")
                    )
filtRs <- file.path(path, "filtered", 
                    paste0(sample.names, "_R_filt.fastq")
                    )
names(filtFs) <- sample.names
names(filtRs) <- sample.names
```

```{r}
#| echo: true
#| eval: false
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, 
                     truncLen=c(220,180), 
                     maxN = 0, maxEE = 2, truncQ = 2, 
                     rm.phix = TRUE, compress = TRUE, 
                     multithread = 20)
```

:::{.callout-note}
These parameters should be set based on the anticipated length of the amplicon and the read quality.
:::

And here is a table of how the filtering step affected the number of reads in each sample. 

```{r}
#| echo: false
#| eval: true
samptab <- read.table("files/tables/BCS_26_read_changes.txt",
                       header = TRUE, sep = "\t")
names(samptab)[1] <- "SampleID"

samptab <- samptab %>% 
  select(SampleID, input, filtered)

samptab$per_reads_kept <- round(samptab$filtered/samptab$input, 
                                  digits = 3)
write_delim(samptab, "include/tables/BCS_26_filter_read_changes.txt", 
            delim = "\t")
```

```{r}
#| echo: false
#| eval: true
reactable(samptab,
  defaultColDef = colDef(
    header = function(value) gsub("_", " ", value, fixed = TRUE),
    cell = function(value) format(value, nsmall = 1),
    align = "center", filterable = FALSE, sortable = TRUE, resizable = TRUE,
    footerStyle = list(fontWeight = "bold")
    ), 
  columns = list(
    SampleID = colDef(name = "Sample ID", 
                       sticky = "left", 
                       style = list(borderRight = "1px solid #eee"),
                       headerStyle = list(borderRight = "1px solid #eee"), 
                       align = "left",
                       minWidth = 150, footer = "Total reads"), 
    input = colDef(name = "reads_in", footer = function(values) sprintf("%.0f", sum(values))),
    filtered = colDef(name = "reads_out", footer = function(values) sprintf("%.0f", sum(values)))
    ), 
  searchable = TRUE, defaultPageSize = 5, 
  pageSizeOptions = c(5, 10, 50, nrow(samptab)), 
  showPageSizeOptions = TRUE, highlight = TRUE, 
  bordered = TRUE, striped = TRUE, compact = FALSE, 
  wrap = FALSE, showSortable = TRUE, fullWidth = TRUE,
  theme = reactableTheme(style = list(fontSize = "0.8em"))) %>%
  reactablefmtr::add_subtitle("Total reads per sample before and after filtering.", 
                              font_size = 15)
```

{{< downloadthis include/tables/BCS_26_filter_read_changes.txt dname=BCS_26_filter_read_changes label="Download filtered read count" icon=table type=info class=data-button id=BCS_26_rc >}}

### 4. Learn Error Rates

Now it is time to assess the error rate of the data. The DADA2 algorithm uses a parametric error model. Every amplicon data set has a different set of error rates and the `learnErrors` method learns this error model *from the data*. It does this by alternating estimation of the error rates and inference of sample composition until they converge on a jointly consistent solution. The algorithm begins with an initial guess, for which the maximum possible error rates in the data are used.

#### Forward Reads

```{r}
#| eval: false
#| echo: true
errF <- learnErrors(filtFs, multithread = TRUE)
```

```
112559480 total bases in 511634 reads from 28 samples 
will be used for learning the error rates.
```

We can then plot the error rates for the forward reads. 

```{r}
#| eval: false
#| echo: false
#| warning: false
plotErrors(errF, nominalQ = TRUE)
```

```{r}
#| echo: true
#| eval: false
p3 <- plotErrors(errF, nominalQ = TRUE)
ggsave("figures/BCS_26_plot_errorF_1.png", p3, width = 7, height = 5)
ggsave("figures/BCS_26_plot_errorF_2.png", p3)
```

```{r}
#| eval: true
#| echo: false
system("cp files/figures/BCS_26_plot_errorF_2.png include/figures/BCS_26_plot_errorF_2.png")
```

```{r}
#| echo: false
#| eval: true
#| fig-cap: "Forward reads: Observed frequency of each transition (e.g., T -> G) as a function of the associated quality score."
knitr::include_graphics("include/figures/BCS_26_plot_errorF_2.png")
```

#### Reverse Reads

And now we can process the reverse read error rates. 

```{r}
#| eval: false
#| echo: true
errR <- learnErrors(filtRs, multithread = TRUE)
```

```
100501560 total bases in 558342 reads from 31 samples 
will be used for learning the error rates.
```

```{r}
#| eval: false
#| echo: false
#| warning: false
plotErrors(errR, nominalQ = TRUE)
```

```{r}
#| echo: true
#| eval: false
p4 <- plotErrors(errR, nominalQ = TRUE)
ggsave("figures/BCS_26_plot_errorR_1.png", p4, width = 7, height = 5)
ggsave("figures/BCS_26_plot_errorR_2.png", p4)
```

```{r}
#| eval: true
#| echo: false
system("cp files/figures/BCS_26_plot_errorR_2.png include/figures/BCS_26_plot_errorR_2.png")
```

```{r}
#| echo: false
#| eval: true
#| fig-cap: "Reverse reads: Observed frequency of each transition (e.g., T -> G) as a function of the associated quality score."
knitr::include_graphics("include/figures/BCS_26_plot_errorR_2.png")
```

The error rates for each possible transition (A to C, A to G, etc.) are shown. Points are the observed error rates for each consensus quality score. The black line shows the estimated error rates after convergence of the machine-learning algorithm. The red line shows the error rates expected under the nominal definition of the Q-score. Here the estimated error rates (black line) are a good fit to the observed rates (points), and the error rates drop with increased quality as expected.

### 5. Dereplicate Reads

Now we can use `derepFastq` to identify the unique sequences in the forward and reverse fastq files.

```{r}
#| echo: true
#| eval: false
sam.names <- sapply(strsplit(basename(filtFs), "_F_"), `[`, 1)
derepFs <- derepFastq(filtFs)
names(derepFs) <- sam.names
derepRs <- derepFastq(filtRs)
names(derepRs) <- sam.names
```

### 6. DADA2 & ASV Inference

At this point we are ready to apply the core sample inference algorithm (`dada`) to the filtered and trimmed sequence data. DADA2 offers three options for whether and how to pool samples for ASV inference. 

If `pool = TRUE`, the algorithm will pool together all samples prior to sample inference.  
If `pool = FALSE`, sample inference is performed on each sample individually.  
If `pool = "pseudo"`, the algorithm will perform pseudo-pooling between individually processed samples.

For our final analysis, we chose `pool = pseudo` for this data set. 

```{r}
#| echo: true
#| eval: false
dadaFs <- dada(derepFs, err = errF, pool = "pseudo", 
               multithread = TRUE)
dadaRs <- dada(derepRs, err = errR, pool = "pseudo",
               multithread = TRUE)
```

{{< downloadthis include/tables/BCS_26_dada_results.txt dname=BCS_26_dada_results label="Results of dada on forward & reverse reads" icon=file-earmark-fill type=info class=data-button id=BCS_26_dada >}}

As an example, we can inspect the returned `dada-class` object for the forward reads from the sample #1:

```{r}
#| echo: true
#| eval: false
dadaFs[[1]]
```

```
dada-class: object describing DADA2 denoising results
15 sequence variants were inferred from 813 input unique sequences.
Key parameters: OMEGA_A = 1e-40, OMEGA_C = 1e-40, BAND_SIZE = 16
```

And the corresponding reverse reads.

```{r}
#| echo: true
#| eval: false
dadaRs[[1]]
```

```
dada-class: object describing DADA2 denoising results
13 sequence variants were inferred from 683 input unique sequences.
Key parameters: OMEGA_A = 1e-40, OMEGA_C = 1e-40, BAND_SIZE = 16
```

These outputs tell us how many true sequence variants the DADA2 algorithm inferred from the unique sequences, in this case the sample 1.

### 7. Merge Paired Reads

We now merge the forward and reverse reads together to obtain the full denoised sequence dataset. Merging is performed by aligning the denoised forward reads with the reverse-complement of the corresponding denoised reverse reads, and then constructing the merged “contig” sequences. By default, merged sequences are only output if the forward and reverse reads overlap by at least 12 bases, and are identical to each other in the overlap region (but these conditions can be changed via function arguments).

```{r}
#| echo: true
#| eval: false
mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs)
```

The `mergers` objects are lists of data.frames from each sample. Each data.frame contains the merged sequence, its abundance, and the indices of the forward and reverse sequence variants that were merged. Paired reads that did not exactly overlap were removed by `mergePairs`, further reducing spurious output.

### 8. Construct Sequence Table

Now we construct an amplicon sequence variant (ASV) table.

```{r}
#| echo: true
#| eval: false
BCS_26 <- makeSequenceTable(mergers)
dim(BCS_26)
```

```{r}
#| echo: false
#| eval: true
seqtabX <- c(384, 29430)
seqtabX[2]
```

Looks like we have **`r seqtabX[2]`** sequence variants from **`r seqtabX[1]`** samples.

```{r}
#| echo: true
#| eval: false
table(nchar(getSequences(BCS_26)))
```

```
  220   221   222   223   224   225   226   227   228   229   230   231   234 
   32    30     6    13     3     2     1     9     5     2     1     3     2 
  235   236   237   238   239   240   241   242   243   244   245   246   247 
    2   479    83     3     4    45    31   218    72    20     3     3     7 
  248   249   250   251   252   253   254   255   256   257   258   259   260 
    8    10    14    47   985 25810  1025    80    27    15     6     2     3 
  261   262   263   268   270   272   273   275   276   278   279   280   286 
   10     3     2     1     1     2     1     1     4     1     2     1     1 
  289   293   294   295   296   298   300   311   316   318   319   321   323 
    1     3     3     1     2     1     1     1     2     1     1     1     1 
  324   333   334   335   337   338   339   340   341   342   343   344   345 
    1     1     7     5     9     1    10     5     3     7     1     3     2 
  346   347   348   349   350   351   352   355   356   357   359   360   361 
    3     2     5     8     5     1     7     7     2     2     4     2    22 
  362   363   364   365   366   367   368   369   370   372   373   374   377 
   28     4    30    50     5     3     3     2     1     3    10     1     3 
  378 
    2 
```

The sequence table is a matrix with rows corresponding to (and named by) the samples, and columns corresponding to (and named by) the sequence variants. We have **`r seqtabX[2]`** sequence variants but also a range of sequence lengths. Since many of these sequence variants are singletons or doubletons, we can just select a range that corresponds to the expected amplicon length and eliminate the spurious reads. But we will do this step after merging sequence tables from all 6 runs. 

```{r}
#| eval: true
#| echo: false
system("cp files/figures/read_length_before_pseudo_BCS_26.png include/figures/read_length_before_pseudo_BCS_26.png")
```

```{r}
#| echo: false
#| eval: true
#| fig-cap: "Distribution of read length by total ASVs before removing extreme length variants."
knitr::include_graphics("include/figures/read_length_before_pseudo_BCS_26.png")
```

### 9. Tracking Reads 

Finally, we can look at how reads have changed so far in the pipeline 

```{r}
#| echo: false
#| eval: true
read_changes <- read.table("files/tables/BCS_26_read_changes.txt",
                       header = TRUE, sep = "\t")
names(read_changes)[1] <- "SampleID"

read_changes$per_reads_kept <- round(read_changes$merged/read_changes$input, 
                                  digits = 3)
write_delim(read_changes, "include/tables/BCS_26_read_changes.txt", 
            delim = "\t")
```

```{r}
#| echo: false
#| eval: true
#| label: end-of-BCS26
reactable(read_changes,
  defaultColDef = colDef(
    header = function(value) gsub("_", " ", value, fixed = TRUE),
    cell = function(value) format(value, nsmall = 1),
    align = "center", filterable = FALSE, sortable = TRUE, resizable = TRUE,
    footerStyle = list(fontWeight = "bold")
    ), 
  columns = list(
    SampleID = colDef(name = "Sample ID", 
                       sticky = "left", 
                       style = list(borderRight = "1px solid #eee"),
                       headerStyle = list(borderRight = "1px solid #eee"), 
                       align = "left",
                       minWidth = 150, footer = "Total reads"), 
    input = colDef(name = "input", footer = function(values) sprintf("%.0f", sum(values))),
    filtered = colDef(footer = function(values) sprintf("%.0f", sum(values))), 
    denoisedF = colDef(footer = function(values) sprintf("%.0f", sum(values))), 
    denoisedR = colDef(footer = function(values) sprintf("%.0f", sum(values))), 
    merged = colDef(footer = function(values) sprintf("%.0f", sum(values)))
    ), 
  searchable = TRUE, defaultPageSize = 5, 
  pageSizeOptions = c(5, 10, 50, nrow(read_changes)), 
  showPageSizeOptions = TRUE, highlight = TRUE, 
  bordered = TRUE, striped = TRUE, compact = FALSE, 
  wrap = FALSE, showSortable = TRUE, fullWidth = TRUE,
  theme = reactableTheme(style = list(fontSize = "0.8em"))) %>%
  reactablefmtr::add_subtitle("Total reads per sample from input to merging.", 
                              font_size = 15)
```

{{< downloadthis include/tables/BCS_26_read_changes.txt dname=BCS_26_read_changes label="Download read count changes" icon=table type=info class=data-button id=BCS_26_rc_changes >}}

We retained **`r round(100*sum(read_changes$merged)/sum(read_changes$input), 1)`%** of the reads from this run. 

And save the sequence table to an `RDS` file.

```{r}
#| echo: true
#| eval: false
saveRDS(BCS_26, "BCS_26.rds")
```
