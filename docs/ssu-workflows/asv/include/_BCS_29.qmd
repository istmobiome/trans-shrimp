### 1. Set Working Environment

:::{.callout-note}
The following plate were sequenced in this run: ISTHMO S13, S14, S15, S16. You can access the source code for this section [here](include/_BCS_29.qmd) 
:::

First, we setup the working environment by defining a path for the working directory.

```{r}
#| echo: true
#| eval: false
path <- "BCS_29/"
head(list.files(path)) 
```

```{r}
#| echo: false
#| eval: true
c("Control_37_R1_001.trimmed.fastq", "Control_37_R2_001.trimmed.fastq", 
  "Control_38_R1_001.trimmed.fastq", "Control_38_R2_001.trimmed.fastq", 
  "Control_39_R1_001.trimmed.fastq", "Control_39_R2_001.trimmed.fastq")
```

Then, we generate matched lists of the forward and reverse read files. We also parse out the sample name.

```{r}
#| echo: true
#| eval: false
fnFs <- sort(list.files(path, pattern = "_R1_001.trimmed.fastq"))
fnRs <- sort(list.files(path, pattern = "_R2_001.trimmed.fastq"))
sample.names <- sapply(strsplit(fnFs, "_R1_"), `[`, 1)
fnFs <- file.path(path, fnFs)
fnRs <- file.path(path, fnRs)
```

### 2. Plot quality scores

And next we plot a visual summary of the distribution of quality scores as a function of sequence position for the input fastq files. Here we set the number of records to sample from  each fastq file (`n`) to  20000 reads. 

```{r}
#| echo: true
#| eval: false
#| warning: false
qprofile_fwd <- plotQualityProfile(fnFs[1:x], 
                                   aggregate = TRUE, 
                                   n = 20000)
qprofile_rev <- plotQualityProfile(fnRs[1:x], 
                                   aggregate = TRUE, 
                                   n = 20000)
qprofile <- grid.arrange(qprofile_fwd, qprofile_rev, nrow = 1)
```

```{r}
#| echo: false
#| eval: false
ggsave("figures/BCS_29_filt_plot_qscores.png", qprofile, width = 7, height = 3)
```

```{r}
#| echo: false
#| eval: true
system("cp files/figures/BCS_29_filt_plot_qscores.png include/figures/BCS_29_filt_plot_qscores.png")
```

```{r}
#| echo: false
#| eval: true
#| fig-height: 4
#| fig-width: 4
#| warning: false
#| fig-cap: "Aggregated quality score plots for forward (left) & reverse (right) reads."
knitr::include_graphics("include/figures/BCS_29_filt_plot_qscores.png")
```

### 3. Filtering

We again make some path variables and setup a new directory of filtered reads.

```{r}
#| echo: true
#| eval: false
filtFs <- file.path(path, "filtered", 
                    paste0(sample.names, "_F_filt.fastq")
                    )
filtRs <- file.path(path, "filtered", 
                    paste0(sample.names, "_R_filt.fastq")
                    )
names(filtFs) <- sample.names
names(filtRs) <- sample.names
```

```{r}
#| echo: true
#| eval: false
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, 
                     truncLen=c(220,180), 
                     maxN = 0, maxEE = 2, truncQ = 2, 
                     rm.phix = TRUE, compress = TRUE, 
                     multithread = 20)
```

:::{.callout-note}
These parameters should be set based on the anticipated length of the amplicon and the read quality.
:::

And here is a table of how the filtering step affected the number of reads in each sample. 

```{r}
#| echo: false
#| eval: true
samptab <- read.table("files/tables/BCS_29_read_changes.txt",
                       header = TRUE, sep = "\t")
names(samptab)[1] <- "SampleID"

samptab <- samptab %>% 
  select(SampleID, input, filtered)

samptab$per_reads_kept <- round(samptab$filtered/samptab$input, 
                                  digits = 3)
write_delim(samptab, "include/tables/BCS_29_filter_read_changes.txt", 
            delim = "\t")
```

```{r}
#| echo: false
#| eval: true
reactable(samptab,
  defaultColDef = colDef(
    header = function(value) gsub("_", " ", value, fixed = TRUE),
    cell = function(value) format(value, nsmall = 1),
    align = "center", filterable = FALSE, sortable = TRUE, resizable = TRUE,
    footerStyle = list(fontWeight = "bold")
    ), 
  columns = list(
    SampleID = colDef(name = "Sample ID", 
                       sticky = "left", 
                       style = list(borderRight = "1px solid #eee"),
                       headerStyle = list(borderRight = "1px solid #eee"), 
                       align = "left",
                       minWidth = 150, footer = "Total reads"), 
    input = colDef(name = "reads_in", footer = function(values) sprintf("%.0f", sum(values))),
    filtered = colDef(name = "reads_out", footer = function(values) sprintf("%.0f", sum(values)))
    ), 
  searchable = TRUE, defaultPageSize = 5, 
  pageSizeOptions = c(5, 10, 50, nrow(samptab)), 
  showPageSizeOptions = TRUE, highlight = TRUE, 
  bordered = TRUE, striped = TRUE, compact = FALSE, 
  wrap = FALSE, showSortable = TRUE, fullWidth = TRUE,
  theme = reactableTheme(style = list(fontSize = "0.8em"))) %>%
  reactablefmtr::add_subtitle("Total reads per sample before and after filtering.", 
                              font_size = 15)
```

{{< downloadthis include/tables/BCS_29_filter_read_changes.txt dname=BCS_29_filter_read_changes label="Download filtered read count" icon=table type=info class=data-button id=BCS_29_rc >}}

### 4. Learn Error Rates

Now it is time to assess the error rate of the data. The DADA2 algorithm uses a parametric error model. Every amplicon data set has a different set of error rates and the `learnErrors` method learns this error model *from the data*. It does this by alternating estimation of the error rates and inference of sample composition until they converge on a jointly consistent solution. The algorithm begins with an initial guess, for which the maximum possible error rates in the data are used.

#### Forward Reads

```{r}
#| eval: false
#| echo: true
errF <- learnErrors(filtFs, multithread = TRUE)
```

```
104777640 total bases in 476262 reads from 36 samples 
will be used for learning the error rates.
```

We can then plot the error rates for the forward reads. 

```{r}
#| eval: false
#| echo: false
#| warning: false
plotErrors(errF, nominalQ = TRUE)
```

```{r}
#| echo: true
#| eval: false
p3 <- plotErrors(errF, nominalQ = TRUE)
ggsave("figures/BCS_29_plot_errorF_1.png", p3, width = 7, height = 5)
ggsave("figures/BCS_29_plot_errorF_2.png", p3)
```

```{r}
#| eval: true
#| echo: false
system("cp files/figures/BCS_29_plot_errorF_2.png include/figures/BCS_29_plot_errorF_2.png")
```

```{r}
#| echo: false
#| eval: true
#| fig-cap: "Forward reads: Observed frequency of each transition (e.g., T -> G) as a function of the associated quality score."
knitr::include_graphics("include/figures/BCS_29_plot_errorF_2.png")
```

#### Reverse Reads

And now we can process the reverse read error rates. 

```{r}
#| eval: false
#| echo: true
errR <- learnErrors(filtRs, multithread = TRUE)
```

```
103229460 total bases in 573497 reads from 43 samples 
will be used for learning the error rates.
```

```{r}
#| eval: false
#| echo: false
#| warning: false
plotErrors(errR, nominalQ = TRUE)
```

```{r}
#| echo: true
#| eval: false
p4 <- plotErrors(errR, nominalQ = TRUE)
ggsave("figures/BCS_29_plot_errorR_1.png", p4, width = 7, height = 5)
ggsave("figures/BCS_29_plot_errorR_2.png", p4)
```

```{r}
#| eval: true
#| echo: false
system("cp files/figures/BCS_29_plot_errorR_2.png include/figures/BCS_29_plot_errorR_2.png")
```

```{r}
#| echo: false
#| eval: true
#| fig-cap: "Reverse reads: Observed frequency of each transition (e.g., T -> G) as a function of the associated quality score."
knitr::include_graphics("include/figures/BCS_29_plot_errorR_2.png")
```

The error rates for each possible transition (A to C, A to G, etc.) are shown. Points are the observed error rates for each consensus quality score. The black line shows the estimated error rates after convergence of the machine-learning algorithm. The red line shows the error rates expected under the nominal definition of the Q-score. Here the estimated error rates (black line) are a good fit to the observed rates (points), and the error rates drop with increased quality as expected.

### 5. Dereplicate Reads

Now we can use `derepFastq` to identify the unique sequences in the forward and reverse fastq files.

```{r}
#| echo: true
#| eval: false
sam.names <- sapply(strsplit(basename(filtFs), "_F_"), `[`, 1)
derepFs <- derepFastq(filtFs)
names(derepFs) <- sam.names
derepRs <- derepFastq(filtRs)
names(derepRs) <- sam.names
```

### 6. DADA2 & ASV Inference

At this point we are ready to apply the core sample inference algorithm (`dada`) to the filtered and trimmed sequence data. DADA2 offers three options for whether and how to pool samples for ASV inference. 

If `pool = TRUE`, the algorithm will pool together all samples prior to sample inference.  
If `pool = FALSE`, sample inference is performed on each sample individually.  
If `pool = "pseudo"`, the algorithm will perform pseudo-pooling between individually processed samples.

For our final analysis, we chose `pool = pseudo` for this data set. 

```{r}
#| echo: true
#| eval: false
dadaFs <- dada(derepFs, err = errF, pool = "pseudo", 
               multithread = TRUE)
dadaRs <- dada(derepRs, err = errR, pool = "pseudo",
               multithread = TRUE)
```

{{< downloadthis include/tables/BCS_29_dada_results.txt dname=BCS_29_dada_results label="Results of dada on forward & reverse reads" icon=file-earmark-fill type=info class=data-button id=BCS_29_dada >}}

As an example, we can inspect the returned `dada-class` object for the forward reads from the sample #1:

```{r}
#| echo: true
#| eval: false
dadaFs[[1]]
```

```
dada-class: object describing DADA2 denoising results
46 sequence variants were inferred from 3108 input unique sequences.
Key parameters: OMEGA_A = 1e-40, OMEGA_C = 1e-40, BAND_SIZE = 16
```

And the corresponding reverse reads.

```{r}
#| echo: true
#| eval: false
dadaRs[[1]]
```

```
dada-class: object describing DADA2 denoising results
44 sequence variants were inferred from 2744 input unique sequences.
Key parameters: OMEGA_A = 1e-40, OMEGA_C = 1e-40, BAND_SIZE = 16
```

These outputs tell us how many true sequence variants the DADA2 algorithm inferred from the unique sequences, in this case the sample 1.

### 7. Merge Paired Reads

We now merge the forward and reverse reads together to obtain the full denoised sequence dataset. Merging is performed by aligning the denoised forward reads with the reverse-complement of the corresponding denoised reverse reads, and then constructing the merged “contig” sequences. By default, merged sequences are only output if the forward and reverse reads overlap by at least 12 bases, and are identical to each other in the overlap region (but these conditions can be changed via function arguments).

```{r}
#| echo: true
#| eval: false
mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs)
```

The `mergers` objects are lists of data.frames from each sample. Each data.frame contains the merged sequence, its abundance, and the indices of the forward and reverse sequence variants that were merged. Paired reads that did not exactly overlap were removed by `mergePairs`, further reducing spurious output.

### 8. Construct Sequence Table

Now we construct an amplicon sequence variant (ASV) table.

```{r}
#| echo: true
#| eval: false
BCS_29 <- makeSequenceTable(mergers)
dim(BCS_29)
```

```{r}
#| echo: false
#| eval: true
seqtabX <- c(384, 21105)
seqtabX[2]
```

Looks like we have **`r seqtabX[2]`** sequence variants from **`r seqtabX[1]`** samples.

```{r}
#| echo: true
#| eval: false
table(nchar(getSequences(BCS_29)))
```

```
   220   221   222   223   224   225   226   227   228   229   231   234   236 
   16    22     1    10     5     5     2    10     1     2    11     3   518 
  237   238   240   241   242   243   244   245   246   247   248   249   250 
    3     7     8   155    11    81     1     2     4     4     1     2     8 
  251   252   253   254   255   256   257   258   260   262   269   270   272 
   28   690 18300   857    42    17    18     1     1     2     1     1     1 
  273   274   275   277   278   281   286   288   292   293   294   297   300 
    1     1     1     1     3     1     1     1     1     2     2     1     1 
  303   304   305   308   313   315   318   319   329   330   334   335   336 
    2     1     3     1     2     1     1     1     3     2     3     1     1 
  337   339   340   341   342   343   344   347   348   349   350   352   356 
    2     4     5    21     7     1     1     1     2     3     1     1     2 
  357   358   359   360   361   362   363   364   365   366   367   368   370 
   14     1     2     6    14    65     4    32    13     4     2     2     2 
  379   384 
    1     1  
```

The sequence table is a matrix with rows corresponding to (and named by) the samples, and columns corresponding to (and named by) the sequence variants. We have **`r seqtabX[2]`** sequence variants but also a range of sequence lengths. Since many of these sequence variants are singletons or doubletons, we can just select a range that corresponds to the expected amplicon length and eliminate the spurious reads. But we will do this step after merging sequence tables from all 6 runs. 

```{r}
#| eval: true
#| echo: false
system("cp files/figures/read_length_before_pseudo_BCS_29.png include/figures/read_length_before_pseudo_BCS_29.png")
```

```{r}
#| echo: false
#| eval: true
#| fig-cap: "Distribution of read length by total ASVs before removing extreme length variants."
knitr::include_graphics("include/figures/read_length_before_pseudo_BCS_29.png")
```

### 9. Tracking Reads 

Finally, we can look at how reads have changed so far in the pipeline. 

```{r}
#| echo: false
#| eval: true
read_changes <- read.table("files/tables/BCS_29_read_changes.txt",
                       header = TRUE, sep = "\t")
names(read_changes)[1] <- "SampleID"

read_changes$per_reads_kept <- round(read_changes$merged/read_changes$input, 
                                  digits = 3)
write_delim(read_changes, "include/tables/BCS_29_read_changes.txt", 
            delim = "\t")
```

```{r}
#| echo: false
#| eval: true
#| label: end-of-BCS29
reactable(read_changes,
  defaultColDef = colDef(
    header = function(value) gsub("_", " ", value, fixed = TRUE),
    cell = function(value) format(value, nsmall = 1),
    align = "center", filterable = FALSE, sortable = TRUE, resizable = TRUE,
    footerStyle = list(fontWeight = "bold")
    ), 
  columns = list(
    SampleID = colDef(name = "Sample ID", 
                       sticky = "left", 
                       style = list(borderRight = "1px solid #eee"),
                       headerStyle = list(borderRight = "1px solid #eee"), 
                       align = "left",
                       minWidth = 150, footer = "Total reads"), 
    input = colDef(name = "input", footer = function(values) sprintf("%.0f", sum(values))),
    filtered = colDef(footer = function(values) sprintf("%.0f", sum(values))), 
    denoisedF = colDef(footer = function(values) sprintf("%.0f", sum(values))), 
    denoisedR = colDef(footer = function(values) sprintf("%.0f", sum(values))), 
    merged = colDef(footer = function(values) sprintf("%.0f", sum(values)))
    ), 
  searchable = TRUE, defaultPageSize = 5, 
  pageSizeOptions = c(5, 10, 50, nrow(read_changes)), 
  showPageSizeOptions = TRUE, highlight = TRUE, 
  bordered = TRUE, striped = TRUE, compact = FALSE, 
  wrap = FALSE, showSortable = TRUE, fullWidth = TRUE,
  theme = reactableTheme(style = list(fontSize = "0.8em"))) %>%
  reactablefmtr::add_subtitle("Total reads per sample from input to merging.", 
                              font_size = 15)
```

{{< downloadthis include/tables/BCS_29_read_changes.txt dname=BCS_29_read_changes label="Download read count changes" icon=table type=info class=data-button id=BCS_29_rc_changes >}}

We retained **`r round(100*sum(read_changes$merged)/sum(read_changes$input), 1)`%** of the reads from this run. 

And save the sequence table to an `RDS` file.

```{r}
#| echo: true
#| eval: false
saveRDS(BCS_29, "BCS_29.rds")
```
