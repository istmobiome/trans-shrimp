### 1. Set Working Environment

:::{.callout-note}
The following plate were sequenced in this run: ISTHMO S17, S18, S19, S20.  You can access the source code for this section [here](include/_BCS_30.qmd) 
:::

First, we setup the working environment by defining a path for the working directory.

```{r}
#| echo: true
#| eval: false
path <- "BCS_30/"
head(list.files(path)) 
```

```{r}
#| echo: false
#| eval: true
c("Control_49_R1_001.trimmed.fastq", "Control_49_R2_001.trimmed.fastq", 
  "Control_50_R1_001.trimmed.fastq", "Control_50_R2_001.trimmed.fastq", 
  "Control_51_R1_001.trimmed.fastq", "Control_51_R2_001.trimmed.fastq")
```

Then, we generate matched lists of the forward and reverse read files. We also parse out the sample name.

```{r}
#| echo: true
#| eval: false
fnFs <- sort(list.files(path, pattern = "_R1_001.trimmed.fastq"))
fnRs <- sort(list.files(path, pattern = "_R2_001.trimmed.fastq"))
sample.names <- sapply(strsplit(fnFs, "_R1_"), `[`, 1)
fnFs <- file.path(path, fnFs)
fnRs <- file.path(path, fnRs)
```

### 2. Plot quality scores

And next we plot a visual summary of the distribution of quality scores as a function of sequence position for the input fastq files. Here we set the number of records to sample from  each fastq file (`n`) to  20000 reads. 

```{r}
#| echo: true
#| eval: false
#| warning: false
qprofile_fwd <- plotQualityProfile(fnFs[1:x], 
                                   aggregate = TRUE, 
                                   n = 20000)
qprofile_rev <- plotQualityProfile(fnRs[1:x], 
                                   aggregate = TRUE, 
                                   n = 20000)
qprofile <- grid.arrange(qprofile_fwd, qprofile_rev, nrow = 1)
```

```{r}
#| echo: false
#| eval: false
ggsave("figures/BCS_30_filt_plot_qscores.png", qprofile, width = 7, height = 3)
```

```{r}
#| echo: false
#| eval: true
system("cp files/figures/BCS_30_filt_plot_qscores.png include/figures/BCS_30_filt_plot_qscores.png")
```

```{r}
#| echo: false
#| eval: true
#| fig-height: 4
#| fig-width: 4
#| warning: false
#| fig-cap: "Aggregated quality score plots for forward (left) & reverse (right) reads."
knitr::include_graphics("include/figures/BCS_30_filt_plot_qscores.png")
```

### 3. Filtering

We again make some path variables and setup a new directory of filtered reads.

```{r}
#| echo: true
#| eval: false
filtFs <- file.path(path, "filtered", 
                    paste0(sample.names, "_F_filt.fastq")
                    )
filtRs <- file.path(path, "filtered", 
                    paste0(sample.names, "_R_filt.fastq")
                    )
names(filtFs) <- sample.names
names(filtRs) <- sample.names
```

```{r}
#| echo: true
#| eval: false
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, 
                     truncLen=c(220,180), 
                     maxN = 0, maxEE = 2, truncQ = 2, 
                     rm.phix = TRUE, compress = TRUE, 
                     multithread = 20)
```

:::{.callout-note}
These parameters should be set based on the anticipated length of the amplicon and the read quality.
:::

And here is a table of how the filtering step affected the number of reads in each sample. 

```{r}
#| echo: false
#| eval: true
samptab <- read.table("files/tables/BCS_30_read_changes.txt",
                       header = TRUE, sep = "\t")
names(samptab)[1] <- "SampleID"

samptab <- samptab %>% 
  select(SampleID, input, filtered)

samptab$per_reads_kept <- round(samptab$filtered/samptab$input, 
                                  digits = 3)
write_delim(samptab, "include/tables/BCS_30_filter_read_changes.txt", 
            delim = "\t")
```

```{r}
#| echo: false
#| eval: true
reactable(samptab,
  defaultColDef = colDef(
    header = function(value) gsub("_", " ", value, fixed = TRUE),
    cell = function(value) format(value, nsmall = 1),
    align = "center", filterable = FALSE, sortable = TRUE, resizable = TRUE,
    footerStyle = list(fontWeight = "bold")
    ), 
  columns = list(
    SampleID = colDef(name = "Sample ID", 
                       sticky = "left", 
                       style = list(borderRight = "1px solid #eee"),
                       headerStyle = list(borderRight = "1px solid #eee"), 
                       align = "left",
                       minWidth = 150, footer = "Total reads"), 
    input = colDef(name = "reads_in", footer = function(values) sprintf("%.0f", sum(values))),
    filtered = colDef(name = "reads_out", footer = function(values) sprintf("%.0f", sum(values)))
    ), 
  searchable = TRUE, defaultPageSize = 5, 
  pageSizeOptions = c(5, 10, 50, nrow(samptab)), 
  showPageSizeOptions = TRUE, highlight = TRUE, 
  bordered = TRUE, striped = TRUE, compact = FALSE, 
  wrap = FALSE, showSortable = TRUE, fullWidth = TRUE,
  theme = reactableTheme(style = list(fontSize = "0.8em"))) %>%
  reactablefmtr::add_subtitle("Total reads per sample before and after filtering.", 
                              font_size = 15)
```

{{< downloadthis include/tables/BCS_30_filter_read_changes.txt dname=BCS_30_filter_read_changes label="Download filtered read count" icon=table type=info class=data-button id=BCS_30_rc >}}

### 4. Learn Error Rates

Now it is time to assess the error rate of the data. The DADA2 algorithm uses a parametric error model. Every amplicon data set has a different set of error rates and the `learnErrors` method learns this error model *from the data*. It does this by alternating estimation of the error rates and inference of sample composition until they converge on a jointly consistent solution. The algorithm begins with an initial guess, for which the maximum possible error rates in the data are used.

#### Forward Reads

```{r}
#| eval: false
#| echo: true
errF <- learnErrors(filtFs, multithread = TRUE)
```

```
101887500 total bases in 463125 reads from 26 samples 
will be used for learning the error rates.
```

We can then plot the error rates for the forward reads. 

```{r}
#| eval: false
#| echo: false
#| warning: false
plotErrors(errF, nominalQ = TRUE)
```

```{r}
#| echo: true
#| eval: false
p3 <- plotErrors(errF, nominalQ = TRUE)
ggsave("figures/BCS_30_plot_errorF_1.png", p3, width = 7, height = 5)
ggsave("figures/BCS_30_plot_errorF_2.png", p3)
```

```{r}
#| eval: true
#| echo: false
system("cp files/figures/BCS_30_plot_errorF_2.png include/figures/BCS_30_plot_errorF_2.png")
```

```{r}
#| echo: false
#| eval: true
#| fig-cap: "Forward reads: Observed frequency of each transition (e.g., T -> G) as a function of the associated quality score."
knitr::include_graphics("include/figures/BCS_30_plot_errorF_2.png")
```

#### Reverse Reads

And now we can process the reverse read error rates. 

```{r}
#| eval: false
#| echo: true
errR <- learnErrors(filtRs, multithread = TRUE)
```

```
105320340 total bases in 585113 reads from 30 samples 
will be used for learning the error rates.
```

```{r}
#| eval: false
#| echo: false
#| warning: false
plotErrors(errR, nominalQ = TRUE)
```

```{r}
#| echo: true
#| eval: false
p4 <- plotErrors(errR, nominalQ = TRUE)
ggsave("figures/BCS_30_plot_errorR_1.png", p4, width = 7, height = 5)
ggsave("figures/BCS_30_plot_errorR_2.png", p4)
```

```{r}
#| eval: true
#| echo: false
system("cp files/figures/BCS_30_plot_errorR_2.png include/figures/BCS_30_plot_errorR_2.png")
```

```{r}
#| echo: false
#| eval: true
#| fig-cap: "Reverse reads: Observed frequency of each transition (e.g., T -> G) as a function of the associated quality score."
knitr::include_graphics("include/figures/BCS_30_plot_errorR_2.png")
```

The error rates for each possible transition (A to C, A to G, etc.) are shown. Points are the observed error rates for each consensus quality score. The black line shows the estimated error rates after convergence of the machine-learning algorithm. The red line shows the error rates expected under the nominal definition of the Q-score. Here the estimated error rates (black line) are a good fit to the observed rates (points), and the error rates drop with increased quality as expected.

### 5. Dereplicate Reads

Now we can use `derepFastq` to identify the unique sequences in the forward and reverse fastq files.

```{r}
#| echo: true
#| eval: false
sam.names <- sapply(strsplit(basename(filtFs), "_F_"), `[`, 1)
derepFs <- derepFastq(filtFs)
names(derepFs) <- sam.names
derepRs <- derepFastq(filtRs)
names(derepRs) <- sam.names
```

### 6. DADA2 & ASV Inference

At this point we are ready to apply the core sample inference algorithm (`dada`) to the filtered and trimmed sequence data. DADA2 offers three options for whether and how to pool samples for ASV inference. 

If `pool = TRUE`, the algorithm will pool together all samples prior to sample inference.  
If `pool = FALSE`, sample inference is performed on each sample individually.  
If `pool = "pseudo"`, the algorithm will perform pseudo-pooling between individually processed samples.

For our final analysis, we chose `pool = pseudo` for this data set. 

```{r}
#| echo: true
#| eval: false
dadaFs <- dada(derepFs, err = errF, pool = "pseudo", 
               multithread = TRUE)
dadaRs <- dada(derepRs, err = errR, pool = "pseudo",
               multithread = TRUE)
```

{{< downloadthis include/tables/BCS_30_dada_results.txt dname=BCS_30_dada_results label="Results of dada on forward & reverse reads" icon=file-earmark-fill type=info class=data-button id=BCS_30_dada >}}

As an example, we can inspect the returned `dada-class` object for the forward reads from the sample #1:

```{r}
#| echo: true
#| eval: false
dadaFs[[1]]
```

```
dada-class: object describing DADA2 denoising results
9 sequence variants were inferred from 25 input unique sequences.
Key parameters: OMEGA_A = 1e-40, OMEGA_C = 1e-40, BAND_SIZE = 16
```

And the corresponding reverse reads.

```{r}
#| echo: true
#| eval: false
dadaRs[[1]]
```

```
dada-class: object describing DADA2 denoising results
9 sequence variants were inferred from 21 input unique sequences.
Key parameters: OMEGA_A = 1e-40, OMEGA_C = 1e-40, BAND_SIZE = 16
```

These outputs tell us how many true sequence variants the DADA2 algorithm inferred from the unique sequences, in this case the sample 1.

### 7. Merge Paired Reads

We now merge the forward and reverse reads together to obtain the full denoised sequence dataset. Merging is performed by aligning the denoised forward reads with the reverse-complement of the corresponding denoised reverse reads, and then constructing the merged “contig” sequences. By default, merged sequences are only output if the forward and reverse reads overlap by at least 12 bases, and are identical to each other in the overlap region (but these conditions can be changed via function arguments).

```{r}
#| echo: true
#| eval: false
mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs)
```

The `mergers` objects are lists of data.frames from each sample. Each data.frame contains the merged sequence, its abundance, and the indices of the forward and reverse sequence variants that were merged. Paired reads that did not exactly overlap were removed by `mergePairs`, further reducing spurious output.

### 8. Construct Sequence Table

Now we construct an amplicon sequence variant (ASV) table.

```{r}
#| echo: true
#| eval: false
BCS_30 <- makeSequenceTable(mergers)
dim(BCS_30)
```

```{r}
#| echo: false
#| eval: true
seqtabX <- c(380, 36401)
seqtabX[2]
```

Looks like we have **`r seqtabX[2]`** sequence variants from **`r seqtabX[1]`** samples.

```{r}
#| echo: true
#| eval: false
table(nchar(getSequences(BCS_30)))
```

```
  220   221   222   223   224   225   226   227   228   229   231   232   234 
   45    24     5    12    10     3     2    13     2     1    15     2     3 
  235   236   237   238   240   241   242   243   244   245   246   247   248 
    5   142     2    29    33   128    26   266     5     3     4    10     8 
  249   250   251   252   253   254   255   256   257   258   259   260   261 
    7    16    62  1119 31980  1708   149    56    47     7     6     2     4 
  262   263   264   267   268   269   270   271   272   273   274   276   278 
    3     1     2     1     1     1     4     7     1     2     2     1     5 
  279   282   284   285   286   288   289   291   292   293   294   295   296 
    1     1     1     3     1     1     2     1     1     5     1     1     1 
  297   298   303   305   307   308   311   313   316   317   319   320   321 
    1     1     1     2     1     1     2     2     2     1     2     1     1 
  322   323   325   326   328   332   333   334   335   336   337   338   339 
    2     1     2     1     2     1     1     2     1     6     3     3     5 
  340   341   342   343   344   345   347   348   349   350   351   352   353 
    2    21    23     4     2     4     8     6     6     3     1     2     1 
  354   355   356   357   358   359   360   361   362   363   364   365   366 
    1     1     3     6     2     8     9    76    58    18    33    22     5 
  368   371   372   373   379   380   387 
    8     1     1     1     2     1     2 
```

The sequence table is a matrix with rows corresponding to (and named by) the samples, and columns corresponding to (and named by) the sequence variants. We have **`r seqtabX[2]`** sequence variants but also a range of sequence lengths. Since many of these sequence variants are singletons or doubletons, we can just select a range that corresponds to the expected amplicon length and eliminate the spurious reads. But we will do this step after merging sequence tables from all 6 runs. 

```{r}
#| eval: true
#| echo: false
system("cp files/figures/read_length_before_pseudo_BCS_30.png include/figures/read_length_before_pseudo_BCS_30.png")
```

```{r}
#| echo: false
#| eval: true
#| fig-cap: "Distribution of read length by total ASVs before removing extreme length variants."
knitr::include_graphics("include/figures/read_length_before_pseudo_BCS_30.png")
```

### 9. Tracking Reads 

Finally, we can look at how reads have changed so far in the pipeline. 

```{r}
#| echo: false
#| eval: true
read_changes <- read.table("files/tables/BCS_30_read_changes.txt",
                       header = TRUE, sep = "\t")
names(read_changes)[1] <- "SampleID"

read_changes$per_reads_kept <- round(read_changes$merged/read_changes$input, 
                                  digits = 3)
write_delim(read_changes, "include/tables/BCS_30_read_changes.txt", 
            delim = "\t")
```

```{r}
#| echo: false
#| eval: true
#| label: end-of-BCS30
reactable(read_changes,
  defaultColDef = colDef(
    header = function(value) gsub("_", " ", value, fixed = TRUE),
    cell = function(value) format(value, nsmall = 1),
    align = "center", filterable = FALSE, sortable = TRUE, resizable = TRUE,
    footerStyle = list(fontWeight = "bold")
    ), 
  columns = list(
    SampleID = colDef(name = "Sample ID", 
                       sticky = "left", 
                       style = list(borderRight = "1px solid #eee"),
                       headerStyle = list(borderRight = "1px solid #eee"), 
                       align = "left",
                       minWidth = 150, footer = "Total reads"), 
    input = colDef(name = "input", footer = function(values) sprintf("%.0f", sum(values))),
    filtered = colDef(footer = function(values) sprintf("%.0f", sum(values))), 
    denoisedF = colDef(footer = function(values) sprintf("%.0f", sum(values))), 
    denoisedR = colDef(footer = function(values) sprintf("%.0f", sum(values))), 
    merged = colDef(footer = function(values) sprintf("%.0f", sum(values)))
    ), 
  searchable = TRUE, defaultPageSize = 5, 
  pageSizeOptions = c(5, 10, 50, nrow(read_changes)), 
  showPageSizeOptions = TRUE, highlight = TRUE, 
  bordered = TRUE, striped = TRUE, compact = FALSE, 
  wrap = FALSE, showSortable = TRUE, fullWidth = TRUE,
  theme = reactableTheme(style = list(fontSize = "0.8em"))) %>%
  reactablefmtr::add_subtitle("Total reads per sample from input to merging.", 
                              font_size = 15)
```

{{< downloadthis include/tables/BCS_30_read_changes.txt dname=BCS_30_read_changes label="Download read count changes" icon=table type=info class=data-button id=BCS_30_rc_changes >}}

We retained **`r round(100*sum(read_changes$merged)/sum(read_changes$input), 1)`%** of the reads from this run. 

And save the sequence table to an `RDS` file.

```{r}
#| echo: true
#| eval: false
saveRDS(BCS_30, "BCS_30.rds")
```
