### 1. Set Working Environment

:::{.callout-note}
The following plate were sequenced in this run: ISTHMO S01, S02.  You can access the source code for this section [here](include/_BCS_34.qmd) 
:::

First, we setup the working environment by defining a path for the working directory.

```{r}
#| echo: true
#| eval: false
path <- "BCS_34/"
head(list.files(path)) 
```

```{r}
#| echo: false
#| eval: true
c("Control_25_R1_001.trimmed.fastq", "Control_25_R2_001.trimmed.fastq", 
  "Control_26_R1_001.trimmed.fastq", "Control_26_R2_001.trimmed.fastq", 
  "Control_27_R1_001.trimmed.fastq", "Control_27_R2_001.trimmed.fastq")
```

Then, we generate matched lists of the forward and reverse read files. We also parse out the sample name.

```{r}
#| echo: true
#| eval: false
fnFs <- sort(list.files(path, pattern = "_R1_001.trimmed.fastq"))
fnRs <- sort(list.files(path, pattern = "_R2_001.trimmed.fastq"))
sample.names <- sapply(strsplit(fnFs, "_R1_"), `[`, 1)
fnFs <- file.path(path, fnFs)
fnRs <- file.path(path, fnRs)
```

### 2. Plot quality scores

And next we plot a visual summary of the distribution of quality scores as a function of sequence position for the input fastq files. Here we set the number of records to sample from  each fastq file (`n`) to  20000 reads. 

```{r}
#| echo: true
#| eval: false
#| warning: false
qprofile_fwd <- plotQualityProfile(fnFs[1:x], 
                                   aggregate = TRUE, 
                                   n = 20000)
qprofile_rev <- plotQualityProfile(fnRs[1:x], 
                                   aggregate = TRUE, 
                                   n = 20000)
qprofile <- grid.arrange(qprofile_fwd, qprofile_rev, nrow = 1)
qprofile <- grid.arrange(qprofile_fwd, qprofile_rev, nrow = 1)
```

```{r}
#| echo: false
#| eval: false
ggsave("figures/BCS_34_filt_plot_qscores.png", qprofile, width = 7, height = 3)
```

```{r}
#| echo: false
#| eval: true
system("cp files/figures/BCS_34_filt_plot_qscores.png include/figures/BCS_34_filt_plot_qscores.png")
```

```{r}
#| echo: false
#| eval: true
#| fig-height: 4
#| fig-width: 4
#| warning: false
#| fig-cap: "Aggregated quality score plots for forward (left) & reverse (right) reads."
knitr::include_graphics("include/figures/BCS_34_filt_plot_qscores.png")
```

### 3. Filtering

We again make some path variables and setup a new directory of filtered reads.

```{r}
#| echo: true
#| eval: false
filtFs <- file.path(path, "filtered", 
                    paste0(sample.names, "_F_filt.fastq")
                    )
filtRs <- file.path(path, "filtered", 
                    paste0(sample.names, "_R_filt.fastq")
                    )
names(filtFs) <- sample.names
names(filtRs) <- sample.names
```

```{r}
#| echo: true
#| eval: false
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, 
                     truncLen=c(220,180), 
                     maxN = 0, maxEE = 2, truncQ = 2, 
                     rm.phix = TRUE, compress = TRUE, 
                     multithread = 20)
```

:::{.callout-note}
These parameters should be set based on the anticipated length of the amplicon and the read quality.
:::

And here is a table of how the filtering step affected the number of reads in each sample. 

```{r}
#| echo: false
#| eval: true
samptab <- read.table("files/tables/BCS_34_read_changes.txt",
                       header = TRUE, sep = "\t")
names(samptab)[1] <- "SampleID"

samptab <- samptab %>% 
  select(SampleID, input, filtered)

samptab$per_reads_kept <- round(samptab$filtered/samptab$input, 
                                  digits = 3)
write_delim(samptab, "include/tables/BCS_34_filter_read_changes.txt", 
            delim = "\t")
```

```{r}
#| echo: false
#| eval: true
reactable(samptab,
  defaultColDef = colDef(
    header = function(value) gsub("_", " ", value, fixed = TRUE),
    cell = function(value) format(value, nsmall = 1),
    align = "center", filterable = FALSE, sortable = TRUE, resizable = TRUE,
    footerStyle = list(fontWeight = "bold")
    ), 
  columns = list(
    SampleID = colDef(name = "Sample ID", 
                       sticky = "left", 
                       style = list(borderRight = "1px solid #eee"),
                       headerStyle = list(borderRight = "1px solid #eee"), 
                       align = "left",
                       minWidth = 150, footer = "Total reads"), 
    input = colDef(name = "reads_in", footer = function(values) sprintf("%.0f", sum(values))),
    filtered = colDef(name = "reads_out", footer = function(values) sprintf("%.0f", sum(values)))
    ), 
  searchable = TRUE, defaultPageSize = 5, 
  pageSizeOptions = c(5, 10, 50, nrow(samptab)), 
  showPageSizeOptions = TRUE, highlight = TRUE, 
  bordered = TRUE, striped = TRUE, compact = FALSE, 
  wrap = FALSE, showSortable = TRUE, fullWidth = TRUE,
  theme = reactableTheme(style = list(fontSize = "0.8em"))) %>%
  reactablefmtr::add_subtitle("Total reads per sample before and after filtering.", 
                              font_size = 15)
```

{{< downloadthis include/tables/BCS_34_filter_read_changes.txt dname=BCS_34_filter_read_changes label="Download filtered read count" icon=table type=info class=data-button id=BCS_34_rc >}}

### 4. Learn Error Rates

Now it is time to assess the error rate of the data. The DADA2 algorithm uses a parametric error model. Every amplicon data set has a different set of error rates and the `learnErrors` method learns this error model *from the data*. It does this by alternating estimation of the error rates and inference of sample composition until they converge on a jointly consistent solution. The algorithm begins with an initial guess, for which the maximum possible error rates in the data are used.

#### Forward Reads

```{r}
#| eval: false
#| echo: true
errF <- learnErrors(filtFs, multithread = TRUE)
```

```
100938640 total bases in 458812 reads from 22 samples 
will be used for learning the error rates.
```

We can then plot the error rates for the forward reads. 

```{r}
#| eval: false
#| echo: false
#| warning: false
plotErrors(errF, nominalQ = TRUE)
```

```{r}
#| echo: true
#| eval: false
p3 <- plotErrors(errF, nominalQ = TRUE)
ggsave("figures/BCS_34_plot_errorF_1.png", p3, width = 7, height = 5)
ggsave("figures/BCS_34_plot_errorF_2.png", p3)
```

```{r}
#| eval: true
#| echo: false
system("cp files/figures/BCS_34_plot_errorF_2.png include/figures/BCS_34_plot_errorF_2.png")
```

```{r}
#| echo: false
#| eval: true
#| fig-cap: "Forward reads: Observed frequency of each transition (e.g., T -> G) as a function of the associated quality score."
knitr::include_graphics("include/figures/BCS_34_plot_errorF_2.png")
```

#### Reverse Reads

And now we can process the reverse read error rates. 

```{r}
#| eval: false
#| echo: true
errR <- learnErrors(filtRs, multithread = TRUE)
```

```
104387760 total bases in 579932 reads from 25 samples 
will be used for learning the error rates.
```

```{r}
#| eval: false
#| echo: false
#| warning: false
plotErrors(errR, nominalQ = TRUE)
```

```{r}
#| echo: true
#| eval: false
p4 <- plotErrors(errR, nominalQ = TRUE)
ggsave("figures/BCS_34_plot_errorR_1.png", p4, width = 7, height = 5)
ggsave("figures/BCS_34_plot_errorR_2.png", p4)
```

```{r}
#| eval: true
#| echo: false
system("cp files/figures/BCS_34_plot_errorR_2.png include/figures/BCS_34_plot_errorR_2.png")
```

```{r}
#| echo: false
#| eval: true
#| fig-cap: "Reverse reads: Observed frequency of each transition (e.g., T -> G) as a function of the associated quality score."
knitr::include_graphics("include/figures/BCS_34_plot_errorR_2.png")
```

The error rates for each possible transition (A to C, A to G, etc.) are shown. Points are the observed error rates for each consensus quality score. The black line shows the estimated error rates after convergence of the machine-learning algorithm. The red line shows the error rates expected under the nominal definition of the Q-score. Here the estimated error rates (black line) are a good fit to the observed rates (points), and the error rates drop with increased quality as expected.

### 5. Dereplicate Reads

Now we can use `derepFastq` to identify the unique sequences in the forward and reverse fastq files.

```{r}
#| echo: true
#| eval: false
sam.names <- sapply(strsplit(basename(filtFs), "_F_"), `[`, 1)
derepFs <- derepFastq(filtFs)
names(derepFs) <- sam.names
derepRs <- derepFastq(filtRs)
names(derepRs) <- sam.names
```

### 6. DADA2 & ASV Inference

At this point we are ready to apply the core sample inference algorithm (`dada`) to the filtered and trimmed sequence data. DADA2 offers three options for whether and how to pool samples for ASV inference. 

If `pool = TRUE`, the algorithm will pool together all samples prior to sample inference.  
If `pool = FALSE`, sample inference is performed on each sample individually.  
If `pool = "pseudo"`, the algorithm will perform pseudo-pooling between individually processed samples.

For our final analysis, we chose `pool = pseudo` for this data set. 

```{r}
#| echo: true
#| eval: false
dadaFs <- dada(derepFs, err = errF, pool = "pseudo", 
               multithread = TRUE)
dadaRs <- dada(derepRs, err = errR, pool = "pseudo",
               multithread = TRUE)
```

{{< downloadthis include/tables/BCS_34_dada_results.txt dname=BCS_34_dada_results label="Results of dada on forward & reverse reads" icon=file-earmark-fill type=info class=data-button id=BCS_34_dada >}}

As an example, we can inspect the returned `dada-class` object for the forward reads from the sample #1:

```{r}
#| echo: true
#| eval: false
dadaFs[[1]]
```

```
dada-class: object describing DADA2 denoising results
9 sequence variants were inferred from 327 input unique sequences.
Key parameters: OMEGA_A = 1e-40, OMEGA_C = 1e-40, BAND_SIZE = 16
```

And the corresponding reverse reads.

```{r}
#| echo: true
#| eval: false
dadaRs[[1]]
```

```
dada-class: object describing DADA2 denoising results
8 sequence variants were inferred from 255 input unique sequences.
Key parameters: OMEGA_A = 1e-40, OMEGA_C = 1e-40, BAND_SIZE = 16
```

These outputs tell us how many true sequence variants the DADA2 algorithm inferred from the unique sequences, in this case the sample 1.

### 7. Merge Paired Reads

We now merge the forward and reverse reads together to obtain the full denoised sequence dataset. Merging is performed by aligning the denoised forward reads with the reverse-complement of the corresponding denoised reverse reads, and then constructing the merged “contig” sequences. By default, merged sequences are only output if the forward and reverse reads overlap by at least 12 bases, and are identical to each other in the overlap region (but these conditions can be changed via function arguments).

```{r}
#| echo: true
#| eval: false
mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs)
```

The `mergers` objects are lists of data.frames from each sample. Each data.frame contains the merged sequence, its abundance, and the indices of the forward and reverse sequence variants that were merged. Paired reads that did not exactly overlap were removed by `mergePairs`, further reducing spurious output.

### 8. Construct Sequence Table

Now we construct an amplicon sequence variant (ASV) table.

```{r}
#| echo: true
#| eval: false
BCS_34 <- makeSequenceTable(mergers)
dim(BCS_34)
```

```{r}
#| echo: false
#| eval: true
seqtabX <- c(190, 18373)
seqtabX[2]
```

Looks like we have **`r seqtabX[2]`** sequence variants from **`r seqtabX[1]`** samples.

```{r}
#| echo: true
#| eval: false
table(nchar(getSequences(BCS_34)))
```

```
  220   221   222   223   224   225   226   227   229   230   231   234   236 
   14    21     2     6     1     2     2     9     1     1     2     2   101 
  237   238   239   241   242   243   244   245   246   247   248   249   250 
   62     1     1    11    16     6     4     4     4     2     4     5     6 
  251   252   253   254   255   256   257   258   259   261   265   270   271 
   20   697 16533   584    52    15    11     5     1     2     1     2     1 
  274   278   279   285   286   290   291   295   308   309   322   325   328 
    1     1     1     1     1     1     1     1     1     1     1     1     1 
  335   336   337   338   339   340   341   342   343   344   345   347   348 
    2     1     4     1     5     1     4     8     2     3     2     1     5 
  349   356   357   358   359   360   361   362   363   364   365   366   367 
    1     1     1     3     2     1    14    15     8    25    16     2     1 
  368   371   372   373   376   377   378   385   386 
    3     2     2     4     2     1     1     1     1  
```

The sequence table is a matrix with rows corresponding to (and named by) the samples, and columns corresponding to (and named by) the sequence variants. We have **`r seqtabX[2]`** sequence variants but also a range of sequence lengths. Since many of these sequence variants are singletons or doubletons, we can just select a range that corresponds to the expected amplicon length and eliminate the spurious reads. But we will do this step after merging sequence tables from all 6 runs. 

```{r}
#| eval: true
#| echo: false
system("cp files/figures/read_length_before_pseudo_BCS_34.png include/figures/read_length_before_pseudo_BCS_34.png")
```

```{r}
#| echo: false
#| eval: true
#| fig-cap: "Distribution of read length by total ASVs before removing extreme length variants."
knitr::include_graphics("include/figures/read_length_before_pseudo_BCS_34.png")
```

### 9. Tracking Reads 

Finally, we can look at how reads have changed so far in the pipeline. 

```{r}
#| echo: false
#| eval: true
read_changes <- read.table("files/tables/BCS_34_read_changes.txt",
                       header = TRUE, sep = "\t")
names(read_changes)[1] <- "SampleID"

read_changes$per_reads_kept <- round(read_changes$merged/read_changes$input, 
                                  digits = 3)
write_delim(read_changes, "include/tables/BCS_34_read_changes.txt", 
            delim = "\t")
```

```{r}
#| echo: false
#| eval: true
#| label: end-of-BCS34
reactable(read_changes,
  defaultColDef = colDef(
    header = function(value) gsub("_", " ", value, fixed = TRUE),
    cell = function(value) format(value, nsmall = 1),
    align = "center", filterable = FALSE, sortable = TRUE, resizable = TRUE,
    footerStyle = list(fontWeight = "bold")
    ), 
  columns = list(
    SampleID = colDef(name = "Sample ID", 
                       sticky = "left", 
                       style = list(borderRight = "1px solid #eee"),
                       headerStyle = list(borderRight = "1px solid #eee"), 
                       align = "left",
                       minWidth = 150, footer = "Total reads"), 
    input = colDef(name = "input", footer = function(values) sprintf("%.0f", sum(values))),
    filtered = colDef(footer = function(values) sprintf("%.0f", sum(values))), 
    denoisedF = colDef(footer = function(values) sprintf("%.0f", sum(values))), 
    denoisedR = colDef(footer = function(values) sprintf("%.0f", sum(values))), 
    merged = colDef(footer = function(values) sprintf("%.0f", sum(values)))
    ), 
  searchable = TRUE, defaultPageSize = 5, 
  pageSizeOptions = c(5, 10, 50, nrow(read_changes)), 
  showPageSizeOptions = TRUE, highlight = TRUE, 
  bordered = TRUE, striped = TRUE, compact = FALSE, 
  wrap = FALSE, showSortable = TRUE, fullWidth = TRUE,
  theme = reactableTheme(style = list(fontSize = "0.8em"))) %>%
  reactablefmtr::add_subtitle("Total reads per sample from input to merging.", 
                              font_size = 15)
```

{{< downloadthis include/tables/BCS_34_read_changes.txt dname=BCS_34_read_changes label="Download read count changes" icon=table type=info class=data-button id=BCS_34_rc_changes >}}

We retained **`r round(100*sum(read_changes$merged)/sum(read_changes$input), 1)`%** of the reads from this run. 

And save the sequence table to an `RDS` file.

```{r}
#| echo: true
#| eval: false
saveRDS(BCS_34, "BCS_34.rds")
```