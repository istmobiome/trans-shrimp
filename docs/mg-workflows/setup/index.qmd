---
title: "1. Annotation Databases"
description: |
  Constructing taxonomic & functional annotation databases. 
format:
  html:
    mermaid:
      theme: neutral
    toc: true
    toc-depth: 3
---

{{< include ../_setup.qmd >}}

There are two main types of annotations we are interested in for this metagenomic project---**taxonomic** and **functional**---and there are many, many ways to accomplish both of these goals. This next section involves building the databases and installing any additional tools we need for annotation.

Let’s start with the tools and databases for **taxonomic** classification.

# Taxonomic Classification

There are many algorithms and databases for taxonomic classification. We will use [Kraken2](https://github.com/DerrickWood/kraken2) to classify short reads. We will also use [Kaiju](https://github.com/bioinformatics-centre/kaiju) and [VirSorter2](https://github.com/jiarong/VirSorter2) for contigs. Anvi'o has methods of importing data from each of these approaches but if you have a have a favorite tool/database there are workarounds to get most results into the appropriate anvio database.

## Kaiju

*[Kaiju](https://github.com/bioinformatics-centre/kaiju) [@menzel2016kaiju]... finds maximum (in-)exact matches on the protein-level using the Burrows–Wheeler transform.* *Kaiju is a program for the taxonomic classification... of metagenomic DNA. Reads are directly assigned to taxa using the NCBI taxonomy and a reference database of protein sequences from microbial and viral genomes.*

First we need to install Kaiju. Again, I will run kaiju in a separate conda environment. Now I can either install kaiju from the source code or as conda package. The latter is easier but often conda packages may lag behind the source code versions. I usually compare the release dates of the [conda package](https://anaconda.org/bioconda/kaiju) with the [source code](https://github.com/bioinformatics-centre/kaiju) and look at the number of downloads. In this case, the conda version of kaiju looks fine.

```{zsh}
# create generic environment
conda create -n kaiju
conda activate kaiji
conda install -c bioconda kaiju
```

After kaiju is installed, the next thing to do is generate the database. You can find a description of kaiju databases in the section on  [dreating the Kaiju index](https://github.com/bioinformatics-centre/kaiju?tab=readme-ov-file#creating-the-kaiju-index). I downloaded and formatted the `progenomes`  database, whic contains representative sets of genomes from the [proGenomes](http://progenomes.embl.de/) database and viruses from the NCBI RefSeq database.

Simply run the `kaiju-makedb` command and specify a database.

```{zsh}
source activate kaiju
mkdir kaiju
cd kaiju
kaiju-makedb -s progenomes
```

The `progenomes` database is 95GB.

::: {.callout-note appearance="simple" collapse="true"}

### Expand for the KAIJU_DB_BUILD Hydra script

{{< include include/_JOB_KAIJU_DB_BUILD.qmd >}}

:::

## Kraken2

*[Kraken2](https://github.com/DerrickWood/kraken2) [@wood2019kraken2] is a novel metagenomics classifier that combines the fast k-mer-based classification of Kraken with an efficient algorithm for assessing the coverage of unique k-mers found in each species in a dataset.*

Installed in separate conda environment

```{zsh}
conda create -n kraken2
conda install kraken2
conda activate kraken2
```

The standard way of installing a Kraken2 database is to run `kraken2-build` and calling the `--standard` flag, which will construct a database containing Refeq archaea, bacteria, viral, plasmid, human data plus data from UniVec_Core. Here is the command. 

```{zsh}
kraken2-build --standard --db kraken2_db --threads $NSLOTS
```

The first problem arises because of issues with the NCBI servers and `kraken2-build` use of `rsync`. So we add the flag `--use-ftp`. Like so...

```{zsh}
kraken2-build --standard --db kraken2_db --threads $NSLOTS --use-ftp
```

But this did not work either--it failed repeatedly. If you look at the Kraken2 GitHub [issues page](https://github.com/DerrickWood/kraken2/issues) you will see that this is a common issue.  So instead we tried using one of the [prebuilt database](https://benlangmead.github.io/aws-indexes/k2), however none of these contained files we needed for downstream analysis. Strike 2. So we tried a different approach. 

First, we followed [this suggestion](https://github.com/DerrickWood/kraken2/issues/515#issuecomment-949354093) to change line 16 of the file `PATH_to_KRAKEN2/download_taxonomy.sh` from this...

```{zsh}
FTP_SERVER="ftp://$NCBI_SERVER"
```

...to this

```{zsh}
TO: FTP_SERVER="https://$NCBI_SERVER"
```

Supposedly this is to help with the timeout issues from the NCBI servers. Next, we tried building our own database. The first step was to download the NCBI taxonomy. 

```{zsh}
kraken2-build --download-taxonomy --db kraken2_db --use-ftp
```

Once this step was complete, we downloaded individual libraries in order to build a custom Kraken2 database. 

```{zsh}
kraken2-build --download-library archaea --db kraken2_db --threads $NSLOTS --use-ftp 
kraken2-build --download-library bacteria --db kraken2_db --threads $NSLOTS --use-ftp 
kraken2-build --download-library plasmid --db kraken2_db --threads $NSLOTS --use-ftp 
kraken2-build --download-library viral --db kraken2_db --threads $NSLOTS --use-ftp 
kraken2-build --download-library fungi --db kraken2_db --threads $NSLOTS --use-ftp 
kraken2-build --download-library protozoa --db kraken2_db --threads $NSLOTS --use-ftp 
kraken2-build --download-library UniVec_Core --db kraken2_db --threads $NSLOTS --use-ftp 
```

And finally constructed the database. 

```{zsh}
kraken2-build --build --db kraken2_db
```

And it worked. The whole process took about 8 hours and the final DB is ~90GB. 

::: {.callout-note appearance="simple" collapse="true" .copy}

### Expand for the KRAKEN2_DB_BUILD Hydra script

{{< include include/_JOB_KRAKEN2_DB_BUILD.qmd >}}

:::

## VirSorter2

*[VirSorter](https://github.com/jiarong/VirSorter2) [@guo2021virsorter2], applies a multi-classifier, expert-guided approach to detect diverse DNA and RNA virus genomes. It has made major updates to its previous version.*

I followed [this recipe](https://github.com/jiarong/VirSorter2?tab=readme-ov-file#option-1-bioconda-version) for installing VirSorter2. Piece of cake.

```{zsh}
mamba create -n virsorter2 -c conda-forge -c bioconda virsorter=2
mamba activate virsorter2
```

Good to go? Now time to build the VirSorter2 database. Pretty straightforward actually. The general instructions can be found [here](https://github.com/jiarong/VirSorter2?tab=readme-ov-file#download-database-and-dependencies). 

```{zsh}
source activate virsorter2
virsorter setup --db-dir /pool/genomics/stri_istmobiome/dbs/virsorter2/ -j $NSLOTS
```

And then a quick test to make sure the database is ok.

```{zsh}
mkdir TEST_virsorter2
cd TEST_virsorter2
wget -O test.fa https://raw.githubusercontent.com/jiarong/VirSorter2/master/test/8seq.fa
virsorter run -w test.out -i test.fa -j $NSLOTS --db-dir dbs/virsorter2/ --tmpdir TEST_virsorter2/tmp_vir --rm-tmpdir --min-length 1500 all
```

The uncompressed database is a little over 10GB.

::: {.callout-note appearance="simple" collapse="true"}

### Expand for the VIRSORTER2_DB_BUILD Hydra script

{{< include include/_JOB_VIRSORTER2_DB_BUILD.qmd >}}

:::

## Single Copy Gene (SCG) taxonomy

Anvi'o has native support for building tRNA and Single Copy Gene (SCG) taxonomy from the [Genome Taxonomy Database (GTDB)](https://gtdb.ecogenomic.org/). 

```{zsh}
anvi-setup-scg-taxonomy -T $NSLOTS
anvi-setup-trna-taxonomy -T $NSLOTS
```

# Functional Annotations

Now we can install databases for **functional** annotation. Anvi'o has native support for building [Pfam](https://pfam.xfam.org/), [COG](https://www.ncbi.nlm.nih.gov/COG/),  [KEGG](https://www.genome.jp/tools/kofamkoala/), and [CAZymes](http://www.cazy.org/) databases.

Anvi'o makes this super simple.

```{zsh}
anvi-setup-pfams --pfam-data-dir dbs/pfam_db
anvi-setup-kegg-data --mode all --kegg-data-dir dbs/kegg_kofam  -T $NSLOTS
anvi-setup-ncbi-cogs --cog-data-dir dbs/cog_db -T $NSLOTS
anvi-setup-cazymes --cazyme-data-dir dbs/cazymes
```

And that’s it. We can add more databases as we need them.

# Concluding remarks

At this point we should be good to go with the main setup. We may need to install other tools along the way---we can add those instructions here. Or you may decide to use other tools instead. For example, we are using [megahit](https://github.com/voutcn/megahit) for the assemblies but you may want to use [metaspades](https://github.com/ablab/spades) or [idba_ud](https://github.com/loneknightpy/idba). These packages need to be installed separately.

#### Source Code {.appendix}

{{< include /include/_access_code.qmd >}}

#### Last updated on {.appendix}

```{r}
#| echo: false
#| eval: true
Sys.time()
```
