[
  {
    "objectID": "docs/ssu-workflows/asv/index.html",
    "href": "docs/ssu-workflows/asv/index.html",
    "title": "2. ASV Workflow",
    "section": "",
    "text": "Click here for page build libraries and setup information.knitr::opts_chunk$set(echo = TRUE, eval = FALSE)\nset.seed(919191)\nlibrary(dada2); packageVersion(\"dada2\")\nlibrary(ShortRead); packageVersion(\"ShortRead\")\nlibrary(ggplot2); packageVersion(\"ggplot2\")\nlibrary(Biostrings); packageVersion(\"Biostrings\")\npacman::p_load(tidyverse, gridExtra, grid, miaViz,\n               formatR, reactable, gdata, patchwork,\n               kableExtra, microeco, magrittr, \n               rprojroot,\n               tidySummarizedExperiment, scater,\n               install = FALSE, update = FALSE)\n\noptions(scipen = 999)\nknitr::opts_current$get(c(\n  \"cache\",\n  \"cache.path\",\n  \"cache.rebuild\",\n  \"dependson\",\n  \"autodep\"\n))\n#root &lt;- find_root(has_file(\"_quarto.yml\"))\n#source(file.path(root, \"assets\", \"functions.R\"))",
    "crumbs": [
      "16S rRNA Processing",
      "2. ASV Workflow"
    ]
  },
  {
    "objectID": "docs/ssu-workflows/asv/index.html#overview",
    "href": "docs/ssu-workflows/asv/index.html#overview",
    "title": "2. ASV Workflow",
    "section": "Overview",
    "text": "Overview\nIn order to run this workflow, you either need the raw data, available on the figshare project site (see below), or the trimmed data, available from the European Nucleotide Archive (ENA) under project accession number XXXXXXXX. See the Data Availability page for more details.\nThis workflow contains the code we used to process the 16S rRNA data sets using DADA2 (Callahan et al. 2016). Workflow construction is based on the DADA2 Pipeline Tutorial (1.8) and the primer identification section of the DADA2 ITS Pipeline Workflow (1.8).",
    "crumbs": [
      "16S rRNA Processing",
      "2. ASV Workflow"
    ]
  },
  {
    "objectID": "docs/ssu-workflows/asv/index.html#workflow-input",
    "href": "docs/ssu-workflows/asv/index.html#workflow-input",
    "title": "2. ASV Workflow",
    "section": "Workflow Input",
    "text": "Workflow Input\nAll files needed to run this workflow can be also downloaded from figshare.\nPENDING",
    "crumbs": [
      "16S rRNA Processing",
      "2. ASV Workflow"
    ]
  },
  {
    "objectID": "docs/ssu-workflows/asv/index.html#individual-run-workflows",
    "href": "docs/ssu-workflows/asv/index.html#individual-run-workflows",
    "title": "2. ASV Workflow",
    "section": "Individual Run Workflows",
    "text": "Individual Run Workflows\nThe first part of the workflow consists of the following steps for each of the runs:\n\n\n\n\n\n\n\nStep\nCommand\nWhat we’re doing\n\n\n\n1\nmultiple\nprepare input file names & paths\n\n\n2\nmultiple\nDefine primers (all orientations)\n\n\n3\ncutadapt\nRemove primers\n\n\n4\nplotQualityProfile()\nPlot quality scores.\n\n\n5\nfilterAndTrim()\nAssess quality & filter reads\n\n\n6\nlearnErrors()\nGenerate an error model for the data\n\n\n7\nderepFastq()\nDereplicate sequences\n\n\n8\ndada()\nInfer ASVs (forward & reverse reads).\n\n\n9\n\nmergePairs().\nMerge denoised forward & reverse reads\n\n\n10\nmakeSequenceTable()\nGenerate count table for each run\n\n\n11\n\nTrack reads through workflow\n\n\n\n\n\n\n\n\n\nflowchart LR\n    A(Start with raw&lt;/br&gt;sequence data)\n    A --&gt; B(plotQualityProfile)\n    B --&gt; C(filterAndTrim)\n    C --&gt; D(plotErrors)\n    C --&gt; E(learnErrors)\n    E --&gt; F(derepFastq)\n    F --&gt; G(dada)\n    G --&gt; I(mergePairs) \n    I --&gt; J(makeSequenceTable)\n\n\n\n\n\n\n\nWe processed each of the six sequencing runs separately for the first part of the DADA2 workflow. While some of the outputs are slightly different (e.g. quality scores, filtering, ASV inference, etc.) the code is the same. For posterity, code for each run is included here.\n\nClick here for workflow library information.#!/usr/bin/env Rscript\nset.seed(919191)\npacman::p_load(tidyverse, gridExtra, grid, phyloseq,\n               formatR, reactable, gdata, ff, decontam,\n               install = FALSE, update = FALSE)\nlibrary(dada2); packageVersion(\"dada2\")\nlibrary(ShortRead); packageVersion(\"ShortRead\")\nlibrary(Biostrings); packageVersion(\"Biostrings\")\nlibrary(DECIPHER); packageVersion(\"DECIPHER\")\n\n\n\n\n\n\n\n\nCode chunk coloring by language\n\nColor\nLanguage\n\n\n\nblue\nR\n\n\nvermillion\nbash\n\n\nbluish green\nshell\n\n\nreddish purple\nmothur\n\n\n\n\n\nWe make use of many different coding languages in these workflows. Code chucks are colored by language.\n\n\nBCS_26\nBCS_28\nBCS_29\nBCS_30\nBCS_34\nBCS_35\n\n\n\n1. Set Working Environment\n\n\n\n\n\n\nNote\n\n\n\nThe following plate were sequenced in this run: ISTHMO S5, S5, S7, S8. You can access the source code for this section here\n\n\nFirst, we setup the working environment by defining a path for the working directory.\n\npath &lt;- \"BCS_26/\"\nhead(list.files(path)) \n\n\n\n[1] \"Control_10_R1_001.trimmed.fastq\" \"Control_10_R2_001.trimmed.fastq\"\n[3] \"Control_11_R1_001.trimmed.fastq\" \"Control_11_R2_001.trimmed.fastq\"\n[5] \"Control_12_R1_001.trimmed.fastq\" \"Control_12_R2_001.trimmed.fastq\"\n\n\nThen, we generate matched lists of the forward and reverse read files. We also parse out the sample name.\n\nfnFs &lt;- sort(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\nfnRs &lt;- sort(list.files(path, pattern = \"_R2_001.trimmed.fastq\"))\nsample.names &lt;- sapply(strsplit(fnFs, \"_R1_\"), `[`, 1)\nfnFs &lt;- file.path(path, fnFs)\nfnRs &lt;- file.path(path, fnRs)\n\n2. Plot quality scores\nAnd next we plot a visual summary of the distribution of quality scores as a function of sequence position for the input fastq files. Here we set the number of records to sample from each fastq file (n) to 20000 reads.\n\nqprofile_fwd &lt;- plotQualityProfile(fnFs[1:x], \n                                   aggregate = TRUE, \n                                   n = 20000)\nqprofile_rev &lt;- plotQualityProfile(fnRs[1:x], \n                                   aggregate = TRUE, \n                                   n = 20000)\nqprofile &lt;- grid.arrange(qprofile_fwd, qprofile_rev, nrow = 1)\nqprofile &lt;- grid.arrange(qprofile_fwd, qprofile_rev, nrow = 1)\n\n\n\n\n\nAggregated quality score plots for forward (left) & reverse (right) reads.\n\n\n\n3. Filtering\nWe again make some path variables and setup a new directory of filtered reads.\n\nfiltFs &lt;- file.path(path, \"filtered\", \n                    paste0(sample.names, \"_F_filt.fastq\")\n                    )\nfiltRs &lt;- file.path(path, \"filtered\", \n                    paste0(sample.names, \"_R_filt.fastq\")\n                    )\nnames(filtFs) &lt;- sample.names\nnames(filtRs) &lt;- sample.names\n\n\nout &lt;- filterAndTrim(fnFs, filtFs, fnRs, filtRs, \n                     truncLen=c(220,180), \n                     maxN = 0, maxEE = 2, truncQ = 2, \n                     rm.phix = TRUE, compress = TRUE, \n                     multithread = 20)\n\n\n\n\n\n\n\nNote\n\n\n\nThese parameters should be set based on the anticipated length of the amplicon and the read quality.\n\n\nAnd here is a table of how the filtering step affected the number of reads in each sample.\n\n\nTotal reads per sample before and after filtering.\n\n\n\n\n Download filtered read count \n4. Learn Error Rates\nNow it is time to assess the error rate of the data. The DADA2 algorithm uses a parametric error model. Every amplicon data set has a different set of error rates and the learnErrors method learns this error model from the data. It does this by alternating estimation of the error rates and inference of sample composition until they converge on a jointly consistent solution. The algorithm begins with an initial guess, for which the maximum possible error rates in the data are used.\nForward Reads\n\nerrF &lt;- learnErrors(filtFs, multithread = TRUE)\n\n112559480 total bases in 511634 reads from 28 samples \nwill be used for learning the error rates.\nWe can then plot the error rates for the forward reads.\n\np3 &lt;- plotErrors(errF, nominalQ = TRUE)\nggsave(\"figures/BCS_26_plot_errorF_1.png\", p3, width = 7, height = 5)\nggsave(\"figures/BCS_26_plot_errorF_2.png\", p3)\n\n\n\n\n\nForward reads: Observed frequency of each transition (e.g., T -&gt; G) as a function of the associated quality score.\n\n\n\nReverse Reads\nAnd now we can process the reverse read error rates.\n\nerrR &lt;- learnErrors(filtRs, multithread = TRUE)\n\n100501560 total bases in 558342 reads from 31 samples \nwill be used for learning the error rates.\n\np4 &lt;- plotErrors(errR, nominalQ = TRUE)\nggsave(\"figures/BCS_26_plot_errorR_1.png\", p4, width = 7, height = 5)\nggsave(\"figures/BCS_26_plot_errorR_2.png\", p4)\n\n\n\n\n\nReverse reads: Observed frequency of each transition (e.g., T -&gt; G) as a function of the associated quality score.\n\n\n\nThe error rates for each possible transition (A to C, A to G, etc.) are shown. Points are the observed error rates for each consensus quality score. The black line shows the estimated error rates after convergence of the machine-learning algorithm. The red line shows the error rates expected under the nominal definition of the Q-score. Here the estimated error rates (black line) are a good fit to the observed rates (points), and the error rates drop with increased quality as expected.\n5. Dereplicate Reads\nNow we can use derepFastq to identify the unique sequences in the forward and reverse fastq files.\n\nsam.names &lt;- sapply(strsplit(basename(filtFs), \"_F_\"), `[`, 1)\nderepFs &lt;- derepFastq(filtFs)\nnames(derepFs) &lt;- sam.names\nderepRs &lt;- derepFastq(filtRs)\nnames(derepRs) &lt;- sam.names\n\n6. DADA2 & ASV Inference\nAt this point we are ready to apply the core sample inference algorithm (dada) to the filtered and trimmed sequence data. DADA2 offers three options for whether and how to pool samples for ASV inference.\nIf pool = TRUE, the algorithm will pool together all samples prior to sample inference.\nIf pool = FALSE, sample inference is performed on each sample individually.\nIf pool = \"pseudo\", the algorithm will perform pseudo-pooling between individually processed samples.\nFor our final analysis, we chose pool = pseudo for this data set.\n\ndadaFs &lt;- dada(derepFs, err = errF, pool = \"pseudo\", \n               multithread = TRUE)\ndadaRs &lt;- dada(derepRs, err = errR, pool = \"pseudo\",\n               multithread = TRUE)\n\n Results of dada on forward & reverse reads \nAs an example, we can inspect the returned dada-class object for the forward reads from the sample #1:\n\ndadaFs[[1]]\n\ndada-class: object describing DADA2 denoising results\n15 sequence variants were inferred from 813 input unique sequences.\nKey parameters: OMEGA_A = 1e-40, OMEGA_C = 1e-40, BAND_SIZE = 16\nAnd the corresponding reverse reads.\n\ndadaRs[[1]]\n\ndada-class: object describing DADA2 denoising results\n13 sequence variants were inferred from 683 input unique sequences.\nKey parameters: OMEGA_A = 1e-40, OMEGA_C = 1e-40, BAND_SIZE = 16\nThese outputs tell us how many true sequence variants the DADA2 algorithm inferred from the unique sequences, in this case the sample 1.\n7. Merge Paired Reads\nWe now merge the forward and reverse reads together to obtain the full denoised sequence dataset. Merging is performed by aligning the denoised forward reads with the reverse-complement of the corresponding denoised reverse reads, and then constructing the merged “contig” sequences. By default, merged sequences are only output if the forward and reverse reads overlap by at least 12 bases, and are identical to each other in the overlap region (but these conditions can be changed via function arguments).\n\nmergers &lt;- mergePairs(dadaFs, derepFs, dadaRs, derepRs)\n\nThe mergers objects are lists of data.frames from each sample. Each data.frame contains the merged sequence, its abundance, and the indices of the forward and reverse sequence variants that were merged. Paired reads that did not exactly overlap were removed by mergePairs, further reducing spurious output.\n8. Construct Sequence Table\nNow we construct an amplicon sequence variant (ASV) table.\n\nBCS_26 &lt;- makeSequenceTable(mergers)\ndim(BCS_26)\n\n\n\n[1] 29430\n\n\nLooks like we have 29430 sequence variants from 384 samples.\n\ntable(nchar(getSequences(BCS_26)))\n\n  220   221   222   223   224   225   226   227   228   229   230   231   234 \n   32    30     6    13     3     2     1     9     5     2     1     3     2 \n  235   236   237   238   239   240   241   242   243   244   245   246   247 \n    2   479    83     3     4    45    31   218    72    20     3     3     7 \n  248   249   250   251   252   253   254   255   256   257   258   259   260 \n    8    10    14    47   985 25810  1025    80    27    15     6     2     3 \n  261   262   263   268   270   272   273   275   276   278   279   280   286 \n   10     3     2     1     1     2     1     1     4     1     2     1     1 \n  289   293   294   295   296   298   300   311   316   318   319   321   323 \n    1     3     3     1     2     1     1     1     2     1     1     1     1 \n  324   333   334   335   337   338   339   340   341   342   343   344   345 \n    1     1     7     5     9     1    10     5     3     7     1     3     2 \n  346   347   348   349   350   351   352   355   356   357   359   360   361 \n    3     2     5     8     5     1     7     7     2     2     4     2    22 \n  362   363   364   365   366   367   368   369   370   372   373   374   377 \n   28     4    30    50     5     3     3     2     1     3    10     1     3 \n  378 \n    2 \nThe sequence table is a matrix with rows corresponding to (and named by) the samples, and columns corresponding to (and named by) the sequence variants. We have 29430 sequence variants but also a range of sequence lengths. Since many of these sequence variants are singletons or doubletons, we can just select a range that corresponds to the expected amplicon length and eliminate the spurious reads. But we will do this step after merging sequence tables from all 6 runs.\n\n\n\n\nDistribution of read length by total ASVs before removing extreme length variants.\n\n\n\n9. Tracking Reads\nFinally, we can look at how reads have changed so far in the pipeline\n\n\nTotal reads per sample from input to merging.\n\n\n\n\n Download read count changes \nWe retained 93.7% of the reads from this run.\nAnd save the sequence table to an RDS file.\n\nsaveRDS(BCS_26, \"BCS_26.rds\")\n\n\n\n\n1. Set Working Environment\n\n\n\n\n\n\nNote\n\n\n\nThe following plate were sequenced in this run: ISTHMO S3, S4. You can access the source code for this section here\n\n\nFirst, we setup the working environment by defining a path for the working directory.\n\npath &lt;- \"BCS_28/\"\nhead(list.files(path)) \n\n\n\n[1] \"Control_31_R1_001.trimmed.fastq\" \"Control_31_R2_001.trimmed.fastq\"\n[3] \"Control_32_R1_001.trimmed.fastq\" \"Control_32_R2_001.trimmed.fastq\"\n[5] \"Control_33_R1_001.trimmed.fastq\" \"Control_33_R2_001.trimmed.fastq\"\n\n\nThen, we generate matched lists of the forward and reverse read files. We also parse out the sample name.\n\nfnFs &lt;- sort(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\nfnRs &lt;- sort(list.files(path, pattern = \"_R2_001.trimmed.fastq\"))\nsample.names &lt;- sapply(strsplit(fnFs, \"_R1_\"), `[`, 1)\nfnFs &lt;- file.path(path, fnFs)\nfnRs &lt;- file.path(path, fnRs)\n\n2. Plot quality scores\nAnd next we plot a visual summary of the distribution of quality scores as a function of sequence position for the input fastq files. Here we set the number of records to sample from each fastq file (n) to 20000 reads.\n\nqprofile_fwd &lt;- plotQualityProfile(fnFs[1:x], \n                                   aggregate = TRUE, \n                                   n = 20000)\nqprofile_rev &lt;- plotQualityProfile(fnRs[1:x], \n                                   aggregate = TRUE, \n                                   n = 20000)\nqprofile &lt;- grid.arrange(qprofile_fwd, qprofile_rev, nrow = 1)\n\n\n\n\n\nAggregated quality score plots for forward (left) & reverse (right) reads.\n\n\n\n3. Filtering\nWe again make some path variables and setup a new directory of filtered reads.\n\nfiltFs &lt;- file.path(path, \"filtered\", \n                    paste0(sample.names, \"_F_filt.fastq\")\n                    )\nfiltRs &lt;- file.path(path, \"filtered\", \n                    paste0(sample.names, \"_R_filt.fastq\")\n                    )\nnames(filtFs) &lt;- sample.names\nnames(filtRs) &lt;- sample.names\n\n\nout &lt;- filterAndTrim(fnFs, filtFs, fnRs, filtRs, \n                     truncLen=c(220,180), \n                     maxN = 0, maxEE = 2, truncQ = 2, \n                     rm.phix = TRUE, compress = TRUE, \n                     multithread = 20)\n\n\n\n\n\n\n\nNote\n\n\n\nThese parameters should be set based on the anticipated length of the amplicon and the read quality.\n\n\nAnd here is a table of how the filtering step affected the number of reads in each sample.\n\n\nTotal reads per sample before and after filtering.\n\n\n\n\n Download filtered read count \n4. Learn Error Rates\nNow it is time to assess the error rate of the data. The DADA2 algorithm uses a parametric error model. Every amplicon data set has a different set of error rates and the learnErrors method learns this error model from the data. It does this by alternating estimation of the error rates and inference of sample composition until they converge on a jointly consistent solution. The algorithm begins with an initial guess, for which the maximum possible error rates in the data are used.\nForward Reads\n\nerrF &lt;- learnErrors(filtFs, multithread = TRUE)\n\n100307240 total bases in 455942 reads from 31 samples \nwill be used for learning the error rates.\nWe can then plot the error rates for the forward reads.\n\np3 &lt;- plotErrors(errF, nominalQ = TRUE)\nggsave(\"figures/BCS_28_plot_errorF_1.png\", p3, width = 7, height = 5)\nggsave(\"figures/BCS_28_plot_errorF_2.png\", p3)\n\n\n\n\n\nForward reads: Observed frequency of each transition (e.g., T -&gt; G) as a function of the associated quality score.\n\n\n\nReverse Reads\nAnd now we can process the reverse read error rates.\n\nerrR &lt;- learnErrors(filtRs, multithread = TRUE)\n\n103203360 total bases in 573352 reads from 36 samples \nwill be used for learning the error rates.\n\np4 &lt;- plotErrors(errR, nominalQ = TRUE)\nggsave(\"figures/BCS_28_plot_errorR_1.png\", p4, width = 7, height = 5)\nggsave(\"figures/BCS_28_plot_errorR_2.png\", p4)\n\n\n\n\n\nReverse reads: Observed frequency of each transition (e.g., T -&gt; G) as a function of the associated quality score.\n\n\n\nThe error rates for each possible transition (A to C, A to G, etc.) are shown. Points are the observed error rates for each consensus quality score. The black line shows the estimated error rates after convergence of the machine-learning algorithm. The red line shows the error rates expected under the nominal definition of the Q-score. Here the estimated error rates (black line) are a good fit to the observed rates (points), and the error rates drop with increased quality as expected.\n5. Dereplicate Reads\nNow we can use derepFastq to identify the unique sequences in the forward and reverse fastq files.\n\nsam.names &lt;- sapply(strsplit(basename(filtFs), \"_F_\"), `[`, 1)\nderepFs &lt;- derepFastq(filtFs)\nnames(derepFs) &lt;- sam.names\nderepRs &lt;- derepFastq(filtRs)\nnames(derepRs) &lt;- sam.names\n\n6. DADA2 & ASV Inference\nAt this point we are ready to apply the core sample inference algorithm (dada) to the filtered and trimmed sequence data. DADA2 offers three options for whether and how to pool samples for ASV inference.\nIf pool = TRUE, the algorithm will pool together all samples prior to sample inference.\nIf pool = FALSE, sample inference is performed on each sample individually.\nIf pool = \"pseudo\", the algorithm will perform pseudo-pooling between individually processed samples.\nFor our final analysis, we chose pool = pseudo for this data set.\n\ndadaFs &lt;- dada(derepFs, err = errF, pool = \"pseudo\", \n               multithread = TRUE)\ndadaRs &lt;- dada(derepRs, err = errR, pool = \"pseudo\",\n               multithread = TRUE)\n\n Results of dada on forward & reverse reads \nAs an example, we can inspect the returned dada-class object for the forward reads from the sample #1:\n\ndadaFs[[1]]\n\ndada-class: object describing DADA2 denoising results\n76 sequence variants were inferred from 8771 input unique sequences.\nKey parameters: OMEGA_A = 1e-40, OMEGA_C = 1e-40, BAND_SIZE = 16\nAnd the corresponding reverse reads.\n\ndadaRs[[1]]\n\ndada-class: object describing DADA2 denoising results\n74 sequence variants were inferred from 6979 input unique sequences.\nKey parameters: OMEGA_A = 1e-40, OMEGA_C = 1e-40, BAND_SIZE = 16\nThese outputs tell us how many true sequence variants the DADA2 algorithm inferred from the unique sequences, in this case the sample 1.\n7. Merge Paired Reads\nWe now merge the forward and reverse reads together to obtain the full denoised sequence dataset. Merging is performed by aligning the denoised forward reads with the reverse-complement of the corresponding denoised reverse reads, and then constructing the merged “contig” sequences. By default, merged sequences are only output if the forward and reverse reads overlap by at least 12 bases, and are identical to each other in the overlap region (but these conditions can be changed via function arguments).\n\nmergers &lt;- mergePairs(dadaFs, derepFs, dadaRs, derepRs)\n\nThe mergers objects are lists of data.frames from each sample. Each data.frame contains the merged sequence, its abundance, and the indices of the forward and reverse sequence variants that were merged. Paired reads that did not exactly overlap were removed by mergePairs, further reducing spurious output.\n8. Construct Sequence Table\nNow we construct an amplicon sequence variant (ASV) table.\n\nBCS_28 &lt;- makeSequenceTable(mergers)\ndim(BCS_28)\n\n\n\n[1] 9461\n\n\nLooks like we have 9461 sequence variants from 192 samples.\n\ntable(nchar(getSequences(BCS_28)))\n\n 220  221  222  223  224  227  230  231  236  237  238  239  241  242  243  244 \n   3    8    2    1    1    4    1    1    1    2    1    1    2    5    1    1 \n 245  246  247  248  249  250  251  252  253  254  255  256  257  258  259  268 \n   1    1    2    1    1    4    8  328 8699  252   21    9    6    1    1    1 \n 270  292  318  335  336  337  338  339  341  342  344  345  348  357  359  361 \n   1    1    1    1    2    2    1    1    2    3    1    1    3    1    1    5 \n 362  363  364  365  366  367  368  372  373  374  376  378  385  388 \n   8    3   22   13    1    1    8    1    1    1    1    1    1    1 \nThe sequence table is a matrix with rows corresponding to (and named by) the samples, and columns corresponding to (and named by) the sequence variants. We have 9461 sequence variants but also a range of sequence lengths. Since many of these sequence variants are singletons or doubletons, we can just select a range that corresponds to the expected amplicon length and eliminate the spurious reads. But we will do this step after merging sequence tables from all 6 runs.\n\n\n\n\nDistribution of read length by total ASVs before removing extreme length variants.\n\n\n\n9. Tracking Reads\nFinally, we can look at how reads have changed so far in the pipeline.\n\n\nTotal reads per sample from input to merging.\n\n\n\n\n Download read count changes \nWe retained 58.3% of the reads from this run.\nAnd save the sequence table to an RDS file.\n\nsaveRDS(BCS_28, \"BCS_28.rds\")\n\n\n\n\n1. Set Working Environment\n\n\n\n\n\n\nNote\n\n\n\nThe following plate were sequenced in this run: ISTHMO S13, S14, S15, S16. You can access the source code for this section here\n\n\nFirst, we setup the working environment by defining a path for the working directory.\n\npath &lt;- \"BCS_29/\"\nhead(list.files(path)) \n\n\n\n[1] \"Control_37_R1_001.trimmed.fastq\" \"Control_37_R2_001.trimmed.fastq\"\n[3] \"Control_38_R1_001.trimmed.fastq\" \"Control_38_R2_001.trimmed.fastq\"\n[5] \"Control_39_R1_001.trimmed.fastq\" \"Control_39_R2_001.trimmed.fastq\"\n\n\nThen, we generate matched lists of the forward and reverse read files. We also parse out the sample name.\n\nfnFs &lt;- sort(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\nfnRs &lt;- sort(list.files(path, pattern = \"_R2_001.trimmed.fastq\"))\nsample.names &lt;- sapply(strsplit(fnFs, \"_R1_\"), `[`, 1)\nfnFs &lt;- file.path(path, fnFs)\nfnRs &lt;- file.path(path, fnRs)\n\n2. Plot quality scores\nAnd next we plot a visual summary of the distribution of quality scores as a function of sequence position for the input fastq files. Here we set the number of records to sample from each fastq file (n) to 20000 reads.\n\nqprofile_fwd &lt;- plotQualityProfile(fnFs[1:x], \n                                   aggregate = TRUE, \n                                   n = 20000)\nqprofile_rev &lt;- plotQualityProfile(fnRs[1:x], \n                                   aggregate = TRUE, \n                                   n = 20000)\nqprofile &lt;- grid.arrange(qprofile_fwd, qprofile_rev, nrow = 1)\n\n\n\n\n\nAggregated quality score plots for forward (left) & reverse (right) reads.\n\n\n\n3. Filtering\nWe again make some path variables and setup a new directory of filtered reads.\n\nfiltFs &lt;- file.path(path, \"filtered\", \n                    paste0(sample.names, \"_F_filt.fastq\")\n                    )\nfiltRs &lt;- file.path(path, \"filtered\", \n                    paste0(sample.names, \"_R_filt.fastq\")\n                    )\nnames(filtFs) &lt;- sample.names\nnames(filtRs) &lt;- sample.names\n\n\nout &lt;- filterAndTrim(fnFs, filtFs, fnRs, filtRs, \n                     truncLen=c(220,180), \n                     maxN = 0, maxEE = 2, truncQ = 2, \n                     rm.phix = TRUE, compress = TRUE, \n                     multithread = 20)\n\n\n\n\n\n\n\nNote\n\n\n\nThese parameters should be set based on the anticipated length of the amplicon and the read quality.\n\n\nAnd here is a table of how the filtering step affected the number of reads in each sample.\n\n\nTotal reads per sample before and after filtering.\n\n\n\n\n Download filtered read count \n4. Learn Error Rates\nNow it is time to assess the error rate of the data. The DADA2 algorithm uses a parametric error model. Every amplicon data set has a different set of error rates and the learnErrors method learns this error model from the data. It does this by alternating estimation of the error rates and inference of sample composition until they converge on a jointly consistent solution. The algorithm begins with an initial guess, for which the maximum possible error rates in the data are used.\nForward Reads\n\nerrF &lt;- learnErrors(filtFs, multithread = TRUE)\n\n104777640 total bases in 476262 reads from 36 samples \nwill be used for learning the error rates.\nWe can then plot the error rates for the forward reads.\n\np3 &lt;- plotErrors(errF, nominalQ = TRUE)\nggsave(\"figures/BCS_29_plot_errorF_1.png\", p3, width = 7, height = 5)\nggsave(\"figures/BCS_29_plot_errorF_2.png\", p3)\n\n\n\n\n\nForward reads: Observed frequency of each transition (e.g., T -&gt; G) as a function of the associated quality score.\n\n\n\nReverse Reads\nAnd now we can process the reverse read error rates.\n\nerrR &lt;- learnErrors(filtRs, multithread = TRUE)\n\n103229460 total bases in 573497 reads from 43 samples \nwill be used for learning the error rates.\n\np4 &lt;- plotErrors(errR, nominalQ = TRUE)\nggsave(\"figures/BCS_29_plot_errorR_1.png\", p4, width = 7, height = 5)\nggsave(\"figures/BCS_29_plot_errorR_2.png\", p4)\n\n\n\n\n\nReverse reads: Observed frequency of each transition (e.g., T -&gt; G) as a function of the associated quality score.\n\n\n\nThe error rates for each possible transition (A to C, A to G, etc.) are shown. Points are the observed error rates for each consensus quality score. The black line shows the estimated error rates after convergence of the machine-learning algorithm. The red line shows the error rates expected under the nominal definition of the Q-score. Here the estimated error rates (black line) are a good fit to the observed rates (points), and the error rates drop with increased quality as expected.\n5. Dereplicate Reads\nNow we can use derepFastq to identify the unique sequences in the forward and reverse fastq files.\n\nsam.names &lt;- sapply(strsplit(basename(filtFs), \"_F_\"), `[`, 1)\nderepFs &lt;- derepFastq(filtFs)\nnames(derepFs) &lt;- sam.names\nderepRs &lt;- derepFastq(filtRs)\nnames(derepRs) &lt;- sam.names\n\n6. DADA2 & ASV Inference\nAt this point we are ready to apply the core sample inference algorithm (dada) to the filtered and trimmed sequence data. DADA2 offers three options for whether and how to pool samples for ASV inference.\nIf pool = TRUE, the algorithm will pool together all samples prior to sample inference.\nIf pool = FALSE, sample inference is performed on each sample individually.\nIf pool = \"pseudo\", the algorithm will perform pseudo-pooling between individually processed samples.\nFor our final analysis, we chose pool = pseudo for this data set.\n\ndadaFs &lt;- dada(derepFs, err = errF, pool = \"pseudo\", \n               multithread = TRUE)\ndadaRs &lt;- dada(derepRs, err = errR, pool = \"pseudo\",\n               multithread = TRUE)\n\n Results of dada on forward & reverse reads \nAs an example, we can inspect the returned dada-class object for the forward reads from the sample #1:\n\ndadaFs[[1]]\n\ndada-class: object describing DADA2 denoising results\n46 sequence variants were inferred from 3108 input unique sequences.\nKey parameters: OMEGA_A = 1e-40, OMEGA_C = 1e-40, BAND_SIZE = 16\nAnd the corresponding reverse reads.\n\ndadaRs[[1]]\n\ndada-class: object describing DADA2 denoising results\n44 sequence variants were inferred from 2744 input unique sequences.\nKey parameters: OMEGA_A = 1e-40, OMEGA_C = 1e-40, BAND_SIZE = 16\nThese outputs tell us how many true sequence variants the DADA2 algorithm inferred from the unique sequences, in this case the sample 1.\n7. Merge Paired Reads\nWe now merge the forward and reverse reads together to obtain the full denoised sequence dataset. Merging is performed by aligning the denoised forward reads with the reverse-complement of the corresponding denoised reverse reads, and then constructing the merged “contig” sequences. By default, merged sequences are only output if the forward and reverse reads overlap by at least 12 bases, and are identical to each other in the overlap region (but these conditions can be changed via function arguments).\n\nmergers &lt;- mergePairs(dadaFs, derepFs, dadaRs, derepRs)\n\nThe mergers objects are lists of data.frames from each sample. Each data.frame contains the merged sequence, its abundance, and the indices of the forward and reverse sequence variants that were merged. Paired reads that did not exactly overlap were removed by mergePairs, further reducing spurious output.\n8. Construct Sequence Table\nNow we construct an amplicon sequence variant (ASV) table.\n\nBCS_29 &lt;- makeSequenceTable(mergers)\ndim(BCS_29)\n\n\n\n[1] 21105\n\n\nLooks like we have 21105 sequence variants from 384 samples.\n\ntable(nchar(getSequences(BCS_29)))\n\n   220   221   222   223   224   225   226   227   228   229   231   234   236 \n   16    22     1    10     5     5     2    10     1     2    11     3   518 \n  237   238   240   241   242   243   244   245   246   247   248   249   250 \n    3     7     8   155    11    81     1     2     4     4     1     2     8 \n  251   252   253   254   255   256   257   258   260   262   269   270   272 \n   28   690 18300   857    42    17    18     1     1     2     1     1     1 \n  273   274   275   277   278   281   286   288   292   293   294   297   300 \n    1     1     1     1     3     1     1     1     1     2     2     1     1 \n  303   304   305   308   313   315   318   319   329   330   334   335   336 \n    2     1     3     1     2     1     1     1     3     2     3     1     1 \n  337   339   340   341   342   343   344   347   348   349   350   352   356 \n    2     4     5    21     7     1     1     1     2     3     1     1     2 \n  357   358   359   360   361   362   363   364   365   366   367   368   370 \n   14     1     2     6    14    65     4    32    13     4     2     2     2 \n  379   384 \n    1     1  \nThe sequence table is a matrix with rows corresponding to (and named by) the samples, and columns corresponding to (and named by) the sequence variants. We have 21105 sequence variants but also a range of sequence lengths. Since many of these sequence variants are singletons or doubletons, we can just select a range that corresponds to the expected amplicon length and eliminate the spurious reads. But we will do this step after merging sequence tables from all 6 runs.\n\n\n\n\nDistribution of read length by total ASVs before removing extreme length variants.\n\n\n\n9. Tracking Reads\nFinally, we can look at how reads have changed so far in the pipeline.\n\n\nTotal reads per sample from input to merging.\n\n\n\n\n Download read count changes \nWe retained 81.5% of the reads from this run.\nAnd save the sequence table to an RDS file.\n\nsaveRDS(BCS_29, \"BCS_29.rds\")\n\n\n\n\n1. Set Working Environment\n\n\n\n\n\n\nNote\n\n\n\nThe following plate were sequenced in this run: ISTHMO S17, S18, S19, S20. You can access the source code for this section here\n\n\nFirst, we setup the working environment by defining a path for the working directory.\n\npath &lt;- \"BCS_30/\"\nhead(list.files(path)) \n\n\n\n[1] \"Control_49_R1_001.trimmed.fastq\" \"Control_49_R2_001.trimmed.fastq\"\n[3] \"Control_50_R1_001.trimmed.fastq\" \"Control_50_R2_001.trimmed.fastq\"\n[5] \"Control_51_R1_001.trimmed.fastq\" \"Control_51_R2_001.trimmed.fastq\"\n\n\nThen, we generate matched lists of the forward and reverse read files. We also parse out the sample name.\n\nfnFs &lt;- sort(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\nfnRs &lt;- sort(list.files(path, pattern = \"_R2_001.trimmed.fastq\"))\nsample.names &lt;- sapply(strsplit(fnFs, \"_R1_\"), `[`, 1)\nfnFs &lt;- file.path(path, fnFs)\nfnRs &lt;- file.path(path, fnRs)\n\n2. Plot quality scores\nAnd next we plot a visual summary of the distribution of quality scores as a function of sequence position for the input fastq files. Here we set the number of records to sample from each fastq file (n) to 20000 reads.\n\nqprofile_fwd &lt;- plotQualityProfile(fnFs[1:x], \n                                   aggregate = TRUE, \n                                   n = 20000)\nqprofile_rev &lt;- plotQualityProfile(fnRs[1:x], \n                                   aggregate = TRUE, \n                                   n = 20000)\nqprofile &lt;- grid.arrange(qprofile_fwd, qprofile_rev, nrow = 1)\n\n\n\n\n\nAggregated quality score plots for forward (left) & reverse (right) reads.\n\n\n\n3. Filtering\nWe again make some path variables and setup a new directory of filtered reads.\n\nfiltFs &lt;- file.path(path, \"filtered\", \n                    paste0(sample.names, \"_F_filt.fastq\")\n                    )\nfiltRs &lt;- file.path(path, \"filtered\", \n                    paste0(sample.names, \"_R_filt.fastq\")\n                    )\nnames(filtFs) &lt;- sample.names\nnames(filtRs) &lt;- sample.names\n\n\nout &lt;- filterAndTrim(fnFs, filtFs, fnRs, filtRs, \n                     truncLen=c(220,180), \n                     maxN = 0, maxEE = 2, truncQ = 2, \n                     rm.phix = TRUE, compress = TRUE, \n                     multithread = 20)\n\n\n\n\n\n\n\nNote\n\n\n\nThese parameters should be set based on the anticipated length of the amplicon and the read quality.\n\n\nAnd here is a table of how the filtering step affected the number of reads in each sample.\n\n\nTotal reads per sample before and after filtering.\n\n\n\n\n Download filtered read count \n4. Learn Error Rates\nNow it is time to assess the error rate of the data. The DADA2 algorithm uses a parametric error model. Every amplicon data set has a different set of error rates and the learnErrors method learns this error model from the data. It does this by alternating estimation of the error rates and inference of sample composition until they converge on a jointly consistent solution. The algorithm begins with an initial guess, for which the maximum possible error rates in the data are used.\nForward Reads\n\nerrF &lt;- learnErrors(filtFs, multithread = TRUE)\n\n101887500 total bases in 463125 reads from 26 samples \nwill be used for learning the error rates.\nWe can then plot the error rates for the forward reads.\n\np3 &lt;- plotErrors(errF, nominalQ = TRUE)\nggsave(\"figures/BCS_30_plot_errorF_1.png\", p3, width = 7, height = 5)\nggsave(\"figures/BCS_30_plot_errorF_2.png\", p3)\n\n\n\n\n\nForward reads: Observed frequency of each transition (e.g., T -&gt; G) as a function of the associated quality score.\n\n\n\nReverse Reads\nAnd now we can process the reverse read error rates.\n\nerrR &lt;- learnErrors(filtRs, multithread = TRUE)\n\n105320340 total bases in 585113 reads from 30 samples \nwill be used for learning the error rates.\n\np4 &lt;- plotErrors(errR, nominalQ = TRUE)\nggsave(\"figures/BCS_30_plot_errorR_1.png\", p4, width = 7, height = 5)\nggsave(\"figures/BCS_30_plot_errorR_2.png\", p4)\n\n\n\n\n\nReverse reads: Observed frequency of each transition (e.g., T -&gt; G) as a function of the associated quality score.\n\n\n\nThe error rates for each possible transition (A to C, A to G, etc.) are shown. Points are the observed error rates for each consensus quality score. The black line shows the estimated error rates after convergence of the machine-learning algorithm. The red line shows the error rates expected under the nominal definition of the Q-score. Here the estimated error rates (black line) are a good fit to the observed rates (points), and the error rates drop with increased quality as expected.\n5. Dereplicate Reads\nNow we can use derepFastq to identify the unique sequences in the forward and reverse fastq files.\n\nsam.names &lt;- sapply(strsplit(basename(filtFs), \"_F_\"), `[`, 1)\nderepFs &lt;- derepFastq(filtFs)\nnames(derepFs) &lt;- sam.names\nderepRs &lt;- derepFastq(filtRs)\nnames(derepRs) &lt;- sam.names\n\n6. DADA2 & ASV Inference\nAt this point we are ready to apply the core sample inference algorithm (dada) to the filtered and trimmed sequence data. DADA2 offers three options for whether and how to pool samples for ASV inference.\nIf pool = TRUE, the algorithm will pool together all samples prior to sample inference.\nIf pool = FALSE, sample inference is performed on each sample individually.\nIf pool = \"pseudo\", the algorithm will perform pseudo-pooling between individually processed samples.\nFor our final analysis, we chose pool = pseudo for this data set.\n\ndadaFs &lt;- dada(derepFs, err = errF, pool = \"pseudo\", \n               multithread = TRUE)\ndadaRs &lt;- dada(derepRs, err = errR, pool = \"pseudo\",\n               multithread = TRUE)\n\n Results of dada on forward & reverse reads \nAs an example, we can inspect the returned dada-class object for the forward reads from the sample #1:\n\ndadaFs[[1]]\n\ndada-class: object describing DADA2 denoising results\n9 sequence variants were inferred from 25 input unique sequences.\nKey parameters: OMEGA_A = 1e-40, OMEGA_C = 1e-40, BAND_SIZE = 16\nAnd the corresponding reverse reads.\n\ndadaRs[[1]]\n\ndada-class: object describing DADA2 denoising results\n9 sequence variants were inferred from 21 input unique sequences.\nKey parameters: OMEGA_A = 1e-40, OMEGA_C = 1e-40, BAND_SIZE = 16\nThese outputs tell us how many true sequence variants the DADA2 algorithm inferred from the unique sequences, in this case the sample 1.\n7. Merge Paired Reads\nWe now merge the forward and reverse reads together to obtain the full denoised sequence dataset. Merging is performed by aligning the denoised forward reads with the reverse-complement of the corresponding denoised reverse reads, and then constructing the merged “contig” sequences. By default, merged sequences are only output if the forward and reverse reads overlap by at least 12 bases, and are identical to each other in the overlap region (but these conditions can be changed via function arguments).\n\nmergers &lt;- mergePairs(dadaFs, derepFs, dadaRs, derepRs)\n\nThe mergers objects are lists of data.frames from each sample. Each data.frame contains the merged sequence, its abundance, and the indices of the forward and reverse sequence variants that were merged. Paired reads that did not exactly overlap were removed by mergePairs, further reducing spurious output.\n8. Construct Sequence Table\nNow we construct an amplicon sequence variant (ASV) table.\n\nBCS_30 &lt;- makeSequenceTable(mergers)\ndim(BCS_30)\n\n\n\n[1] 36401\n\n\nLooks like we have 36401 sequence variants from 380 samples.\n\ntable(nchar(getSequences(BCS_30)))\n\n  220   221   222   223   224   225   226   227   228   229   231   232   234 \n   45    24     5    12    10     3     2    13     2     1    15     2     3 \n  235   236   237   238   240   241   242   243   244   245   246   247   248 \n    5   142     2    29    33   128    26   266     5     3     4    10     8 \n  249   250   251   252   253   254   255   256   257   258   259   260   261 \n    7    16    62  1119 31980  1708   149    56    47     7     6     2     4 \n  262   263   264   267   268   269   270   271   272   273   274   276   278 \n    3     1     2     1     1     1     4     7     1     2     2     1     5 \n  279   282   284   285   286   288   289   291   292   293   294   295   296 \n    1     1     1     3     1     1     2     1     1     5     1     1     1 \n  297   298   303   305   307   308   311   313   316   317   319   320   321 \n    1     1     1     2     1     1     2     2     2     1     2     1     1 \n  322   323   325   326   328   332   333   334   335   336   337   338   339 \n    2     1     2     1     2     1     1     2     1     6     3     3     5 \n  340   341   342   343   344   345   347   348   349   350   351   352   353 \n    2    21    23     4     2     4     8     6     6     3     1     2     1 \n  354   355   356   357   358   359   360   361   362   363   364   365   366 \n    1     1     3     6     2     8     9    76    58    18    33    22     5 \n  368   371   372   373   379   380   387 \n    8     1     1     1     2     1     2 \nThe sequence table is a matrix with rows corresponding to (and named by) the samples, and columns corresponding to (and named by) the sequence variants. We have 36401 sequence variants but also a range of sequence lengths. Since many of these sequence variants are singletons or doubletons, we can just select a range that corresponds to the expected amplicon length and eliminate the spurious reads. But we will do this step after merging sequence tables from all 6 runs.\n\n\n\n\nDistribution of read length by total ASVs before removing extreme length variants.\n\n\n\n9. Tracking Reads\nFinally, we can look at how reads have changed so far in the pipeline.\n\n\nTotal reads per sample from input to merging.\n\n\n\n\n Download read count changes \nWe retained 91% of the reads from this run.\nAnd save the sequence table to an RDS file.\n\nsaveRDS(BCS_30, \"BCS_30.rds\")\n\n\n\n\n1. Set Working Environment\n\n\n\n\n\n\nNote\n\n\n\nThe following plate were sequenced in this run: ISTHMO S01, S02. You can access the source code for this section here\n\n\nFirst, we setup the working environment by defining a path for the working directory.\n\npath &lt;- \"BCS_34/\"\nhead(list.files(path)) \n\n\n\n[1] \"Control_25_R1_001.trimmed.fastq\" \"Control_25_R2_001.trimmed.fastq\"\n[3] \"Control_26_R1_001.trimmed.fastq\" \"Control_26_R2_001.trimmed.fastq\"\n[5] \"Control_27_R1_001.trimmed.fastq\" \"Control_27_R2_001.trimmed.fastq\"\n\n\nThen, we generate matched lists of the forward and reverse read files. We also parse out the sample name.\n\nfnFs &lt;- sort(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\nfnRs &lt;- sort(list.files(path, pattern = \"_R2_001.trimmed.fastq\"))\nsample.names &lt;- sapply(strsplit(fnFs, \"_R1_\"), `[`, 1)\nfnFs &lt;- file.path(path, fnFs)\nfnRs &lt;- file.path(path, fnRs)\n\n2. Plot quality scores\nAnd next we plot a visual summary of the distribution of quality scores as a function of sequence position for the input fastq files. Here we set the number of records to sample from each fastq file (n) to 20000 reads.\n\nqprofile_fwd &lt;- plotQualityProfile(fnFs[1:x], \n                                   aggregate = TRUE, \n                                   n = 20000)\nqprofile_rev &lt;- plotQualityProfile(fnRs[1:x], \n                                   aggregate = TRUE, \n                                   n = 20000)\nqprofile &lt;- grid.arrange(qprofile_fwd, qprofile_rev, nrow = 1)\nqprofile &lt;- grid.arrange(qprofile_fwd, qprofile_rev, nrow = 1)\n\n\n\n\n\nAggregated quality score plots for forward (left) & reverse (right) reads.\n\n\n\n3. Filtering\nWe again make some path variables and setup a new directory of filtered reads.\n\nfiltFs &lt;- file.path(path, \"filtered\", \n                    paste0(sample.names, \"_F_filt.fastq\")\n                    )\nfiltRs &lt;- file.path(path, \"filtered\", \n                    paste0(sample.names, \"_R_filt.fastq\")\n                    )\nnames(filtFs) &lt;- sample.names\nnames(filtRs) &lt;- sample.names\n\n\nout &lt;- filterAndTrim(fnFs, filtFs, fnRs, filtRs, \n                     truncLen=c(220,180), \n                     maxN = 0, maxEE = 2, truncQ = 2, \n                     rm.phix = TRUE, compress = TRUE, \n                     multithread = 20)\n\n\n\n\n\n\n\nNote\n\n\n\nThese parameters should be set based on the anticipated length of the amplicon and the read quality.\n\n\nAnd here is a table of how the filtering step affected the number of reads in each sample.\n\n\nTotal reads per sample before and after filtering.\n\n\n\n\n Download filtered read count \n4. Learn Error Rates\nNow it is time to assess the error rate of the data. The DADA2 algorithm uses a parametric error model. Every amplicon data set has a different set of error rates and the learnErrors method learns this error model from the data. It does this by alternating estimation of the error rates and inference of sample composition until they converge on a jointly consistent solution. The algorithm begins with an initial guess, for which the maximum possible error rates in the data are used.\nForward Reads\n\nerrF &lt;- learnErrors(filtFs, multithread = TRUE)\n\n100938640 total bases in 458812 reads from 22 samples \nwill be used for learning the error rates.\nWe can then plot the error rates for the forward reads.\n\np3 &lt;- plotErrors(errF, nominalQ = TRUE)\nggsave(\"figures/BCS_34_plot_errorF_1.png\", p3, width = 7, height = 5)\nggsave(\"figures/BCS_34_plot_errorF_2.png\", p3)\n\n\n\n\n\nForward reads: Observed frequency of each transition (e.g., T -&gt; G) as a function of the associated quality score.\n\n\n\nReverse Reads\nAnd now we can process the reverse read error rates.\n\nerrR &lt;- learnErrors(filtRs, multithread = TRUE)\n\n104387760 total bases in 579932 reads from 25 samples \nwill be used for learning the error rates.\n\np4 &lt;- plotErrors(errR, nominalQ = TRUE)\nggsave(\"figures/BCS_34_plot_errorR_1.png\", p4, width = 7, height = 5)\nggsave(\"figures/BCS_34_plot_errorR_2.png\", p4)\n\n\n\n\n\nReverse reads: Observed frequency of each transition (e.g., T -&gt; G) as a function of the associated quality score.\n\n\n\nThe error rates for each possible transition (A to C, A to G, etc.) are shown. Points are the observed error rates for each consensus quality score. The black line shows the estimated error rates after convergence of the machine-learning algorithm. The red line shows the error rates expected under the nominal definition of the Q-score. Here the estimated error rates (black line) are a good fit to the observed rates (points), and the error rates drop with increased quality as expected.\n5. Dereplicate Reads\nNow we can use derepFastq to identify the unique sequences in the forward and reverse fastq files.\n\nsam.names &lt;- sapply(strsplit(basename(filtFs), \"_F_\"), `[`, 1)\nderepFs &lt;- derepFastq(filtFs)\nnames(derepFs) &lt;- sam.names\nderepRs &lt;- derepFastq(filtRs)\nnames(derepRs) &lt;- sam.names\n\n6. DADA2 & ASV Inference\nAt this point we are ready to apply the core sample inference algorithm (dada) to the filtered and trimmed sequence data. DADA2 offers three options for whether and how to pool samples for ASV inference.\nIf pool = TRUE, the algorithm will pool together all samples prior to sample inference.\nIf pool = FALSE, sample inference is performed on each sample individually.\nIf pool = \"pseudo\", the algorithm will perform pseudo-pooling between individually processed samples.\nFor our final analysis, we chose pool = pseudo for this data set.\n\ndadaFs &lt;- dada(derepFs, err = errF, pool = \"pseudo\", \n               multithread = TRUE)\ndadaRs &lt;- dada(derepRs, err = errR, pool = \"pseudo\",\n               multithread = TRUE)\n\n Results of dada on forward & reverse reads \nAs an example, we can inspect the returned dada-class object for the forward reads from the sample #1:\n\ndadaFs[[1]]\n\ndada-class: object describing DADA2 denoising results\n9 sequence variants were inferred from 327 input unique sequences.\nKey parameters: OMEGA_A = 1e-40, OMEGA_C = 1e-40, BAND_SIZE = 16\nAnd the corresponding reverse reads.\n\ndadaRs[[1]]\n\ndada-class: object describing DADA2 denoising results\n8 sequence variants were inferred from 255 input unique sequences.\nKey parameters: OMEGA_A = 1e-40, OMEGA_C = 1e-40, BAND_SIZE = 16\nThese outputs tell us how many true sequence variants the DADA2 algorithm inferred from the unique sequences, in this case the sample 1.\n7. Merge Paired Reads\nWe now merge the forward and reverse reads together to obtain the full denoised sequence dataset. Merging is performed by aligning the denoised forward reads with the reverse-complement of the corresponding denoised reverse reads, and then constructing the merged “contig” sequences. By default, merged sequences are only output if the forward and reverse reads overlap by at least 12 bases, and are identical to each other in the overlap region (but these conditions can be changed via function arguments).\n\nmergers &lt;- mergePairs(dadaFs, derepFs, dadaRs, derepRs)\n\nThe mergers objects are lists of data.frames from each sample. Each data.frame contains the merged sequence, its abundance, and the indices of the forward and reverse sequence variants that were merged. Paired reads that did not exactly overlap were removed by mergePairs, further reducing spurious output.\n8. Construct Sequence Table\nNow we construct an amplicon sequence variant (ASV) table.\n\nBCS_34 &lt;- makeSequenceTable(mergers)\ndim(BCS_34)\n\n\n\n[1] 18373\n\n\nLooks like we have 18373 sequence variants from 190 samples.\n\ntable(nchar(getSequences(BCS_34)))\n\n  220   221   222   223   224   225   226   227   229   230   231   234   236 \n   14    21     2     6     1     2     2     9     1     1     2     2   101 \n  237   238   239   241   242   243   244   245   246   247   248   249   250 \n   62     1     1    11    16     6     4     4     4     2     4     5     6 \n  251   252   253   254   255   256   257   258   259   261   265   270   271 \n   20   697 16533   584    52    15    11     5     1     2     1     2     1 \n  274   278   279   285   286   290   291   295   308   309   322   325   328 \n    1     1     1     1     1     1     1     1     1     1     1     1     1 \n  335   336   337   338   339   340   341   342   343   344   345   347   348 \n    2     1     4     1     5     1     4     8     2     3     2     1     5 \n  349   356   357   358   359   360   361   362   363   364   365   366   367 \n    1     1     1     3     2     1    14    15     8    25    16     2     1 \n  368   371   372   373   376   377   378   385   386 \n    3     2     2     4     2     1     1     1     1  \nThe sequence table is a matrix with rows corresponding to (and named by) the samples, and columns corresponding to (and named by) the sequence variants. We have 18373 sequence variants but also a range of sequence lengths. Since many of these sequence variants are singletons or doubletons, we can just select a range that corresponds to the expected amplicon length and eliminate the spurious reads. But we will do this step after merging sequence tables from all 6 runs.\n\n\n\n\nDistribution of read length by total ASVs before removing extreme length variants.\n\n\n\n9. Tracking Reads\nFinally, we can look at how reads have changed so far in the pipeline.\n\n\nTotal reads per sample from input to merging.\n\n\n\n\n Download read count changes \nWe retained 90.6% of the reads from this run.\nAnd save the sequence table to an RDS file.\n\nsaveRDS(BCS_34, \"BCS_34.rds\")\n\n\n\n\n1. Set Working Environment\n\n\n\n\n\n\nNote\n\n\n\nThe following plate were sequenced in this run: ISTHMO S9, S10, S11, S12. You can access the source code for this section here\n\n\nFirst, we setup the working environment by defining a path for the working directory.\n\npath &lt;- \"BCS_35/\"\nhead(list.files(path)) \n\n\n\n[1] \"Control_13_R1_001.trimmed.fastq\" \"Control_13_R2_001.trimmed.fastq\"\n[3] \"Control_14_R1_001.trimmed.fastq\" \"Control_14_R2_001.trimmed.fastq\"\n[5] \"Control_15_R1_001.trimmed.fastq\" \"Control_15_R2_001.trimmed.fastq\"\n\n\nThen, we generate matched lists of the forward and reverse read files. We also parse out the sample name.\n\nfnFs &lt;- sort(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\nfnRs &lt;- sort(list.files(path, pattern = \"_R2_001.trimmed.fastq\"))\nsample.names &lt;- sapply(strsplit(fnFs, \"_R1_\"), `[`, 1)\nfnFs &lt;- file.path(path, fnFs)\nfnRs &lt;- file.path(path, fnRs)\n\n2. Plot quality scores\nAnd next we plot a visual summary of the distribution of quality scores as a function of sequence position for the input fastq files. Here we set the number of records to sample from each fastq file (n) to 20000 reads.\n\nqprofile_fwd &lt;- plotQualityProfile(fnFs[1:x], \n                                   aggregate = TRUE, \n                                   n = 20000)\nqprofile_rev &lt;- plotQualityProfile(fnRs[1:x], \n                                   aggregate = TRUE, \n                                   n = 20000)\nqprofile &lt;- grid.arrange(qprofile_fwd, qprofile_rev, nrow = 1)\n\n\n\n\n\nAggregated quality score plots for forward (left) & reverse (right) reads.\n\n\n\n3. Filtering\nWe again make some path variables and setup a new directory of filtered reads.\n\nfiltFs &lt;- file.path(path, \"filtered\", \n                    paste0(sample.names, \"_F_filt.fastq\")\n                    )\nfiltRs &lt;- file.path(path, \"filtered\", \n                    paste0(sample.names, \"_R_filt.fastq\")\n                    )\nnames(filtFs) &lt;- sample.names\nnames(filtRs) &lt;- sample.names\n\n\nout &lt;- filterAndTrim(fnFs, filtFs, fnRs, filtRs, \n                     truncLen=c(220,180), \n                     maxN = 0, maxEE = 2, truncQ = 2, \n                     rm.phix = TRUE, compress = TRUE, \n                     multithread = 20)\n\n\n\n\n\n\n\nNote\n\n\n\nThese parameters should be set based on the anticipated length of the amplicon and the read quality.\n\n\nAnd here is a table of how the filtering step affected the number of reads in each sample.\n\n\nTotal reads per sample before and after filtering.\n\n\n\n\n Download filtered read count \n4. Learn Error Rates\nNow it is time to assess the error rate of the data. The DADA2 algorithm uses a parametric error model. Every amplicon data set has a different set of error rates and the learnErrors method learns this error model from the data. It does this by alternating estimation of the error rates and inference of sample composition until they converge on a jointly consistent solution. The algorithm begins with an initial guess, for which the maximum possible error rates in the data are used.\nForward Reads\n\nerrF &lt;- learnErrors(filtFs, multithread = TRUE)\n\n101017840 total bases in 459172 reads from 48 samples \nwill be used for learning the error rates.\nWe can then plot the error rates for the forward reads.\n\np3 &lt;- plotErrors(errF, nominalQ = TRUE)\nggsave(\"figures/BCS_35_plot_errorF_1.png\", p3, width = 7, height = 5)\nggsave(\"figures/BCS_35_plot_errorF_2.png\", p3)\n\n\n\n\n\nForward reads: Observed frequency of each transition (e.g., T -&gt; G) as a function of the associated quality score.\n\n\n\nReverse Reads\nAnd now we can process the reverse read error rates.\n\nerrR &lt;- learnErrors(filtRs, multithread = TRUE)\n\n100695240 total bases in 559418 reads from 59 samples \nwill be used for learning the error rates.\n\np4 &lt;- plotErrors(errR, nominalQ = TRUE)\nggsave(\"figures/BCS_35_plot_errorR_1.png\", p4, width = 7, height = 5)\nggsave(\"figures/BCS_35_plot_errorR_2.png\", p4)\n\n\n\n\n\nReverse reads: Observed frequency of each transition (e.g., T -&gt; G) as a function of the associated quality score.\n\n\n\nThe error rates for each possible transition (A to C, A to G, etc.) are shown. Points are the observed error rates for each consensus quality score. The black line shows the estimated error rates after convergence of the machine-learning algorithm. The red line shows the error rates expected under the nominal definition of the Q-score. Here the estimated error rates (black line) are a good fit to the observed rates (points), and the error rates drop with increased quality as expected.\n5. Dereplicate Reads\nNow we can use derepFastq to identify the unique sequences in the forward and reverse fastq files.\n\nsam.names &lt;- sapply(strsplit(basename(filtFs), \"_F_\"), `[`, 1)\nderepFs &lt;- derepFastq(filtFs)\nnames(derepFs) &lt;- sam.names\nderepRs &lt;- derepFastq(filtRs)\nnames(derepRs) &lt;- sam.names\n\n6. DADA2 & ASV Inference\nAt this point we are ready to apply the core sample inference algorithm (dada) to the filtered and trimmed sequence data. DADA2 offers three options for whether and how to pool samples for ASV inference.\nIf pool = TRUE, the algorithm will pool together all samples prior to sample inference.\nIf pool = FALSE, sample inference is performed on each sample individually.\nIf pool = \"pseudo\", the algorithm will perform pseudo-pooling between individually processed samples.\nFor our final analysis, we chose pool = pseudo for this data set.\n\ndadaFs &lt;- dada(derepFs, err = errF, pool = \"pseudo\", \n               multithread = TRUE)\ndadaRs &lt;- dada(derepRs, err = errR, pool = \"pseudo\",\n               multithread = TRUE)\n\n Results of dada on forward & reverse reads \nAs an example, we can inspect the returned dada-class object for the forward reads from the sample #1:\n\ndadaFs[[1]]\n\ndada-class: object describing DADA2 denoising results\n15 sequence variants were inferred from 432 input unique sequences.\nKey parameters: OMEGA_A = 1e-40, OMEGA_C = 1e-40, BAND_SIZE = 16\nAnd the corresponding reverse reads.\n\ndadaRs[[1]]\n\ndada-class: object describing DADA2 denoising results\n19 sequence variants were inferred from 465 input unique sequences.\nKey parameters: OMEGA_A = 1e-40, OMEGA_C = 1e-40, BAND_SIZE = 16\nThese outputs tell us how many true sequence variants the DADA2 algorithm inferred from the unique sequences, in this case the sample 1.\n7. Merge Paired Reads\nWe now merge the forward and reverse reads together to obtain the full denoised sequence dataset. Merging is performed by aligning the denoised forward reads with the reverse-complement of the corresponding denoised reverse reads, and then constructing the merged “contig” sequences. By default, merged sequences are only output if the forward and reverse reads overlap by at least 12 bases, and are identical to each other in the overlap region (but these conditions can be changed via function arguments).\n\nmergers &lt;- mergePairs(dadaFs, derepFs, dadaRs, derepRs)\n\nThe mergers objects are lists of data.frames from each sample. Each data.frame contains the merged sequence, its abundance, and the indices of the forward and reverse sequence variants that were merged. Paired reads that did not exactly overlap were removed by mergePairs, further reducing spurious output.\n8. Construct Sequence Table\nNow we construct an amplicon sequence variant (ASV) table.\n\nBCS_35 &lt;- makeSequenceTable(mergers)\ndim(BCS_35)\n\n\n\n[1] 23367\n\n\nLooks like we have 23367 sequence variants from 379 samples.\n\ntable(nchar(getSequences(BCS_35)))\n\n  220   221   222   223   224   225   226   227   228   229   230   234   235 \n   16    28     5     9     2     2     4     4     1     1     1     1     2 \n  236   237   240   241   242   243   245   246   247   248   249   250   251 \n  147     4    10    76    16    27     5     1     2     3     2     8    31 \n  252   253   254   255   256   257   258   260   262   263   266   268   273 \n 1004 20727   767    44     9    22     1     3     3     1     1     1     3 \n  282   292   293   294   295   303   307   309   310   312   333   334   335 \n    1     1     3     1     1     1     1     1     1     1     1     4     1 \n  336   337   339   340   341   342   344   346   347   348   349   350   352 \n    1     6     6     3    39    23     4     1    19     5     5     3     3 \n  357   359   360   361   362   363   364   365   366   367   368   369   372 \n   13     1     8    64    46    18    33    26     4     4    14     3     1 \n  376   377 \n    1     1  \nThe sequence table is a matrix with rows corresponding to (and named by) the samples, and columns corresponding to (and named by) the sequence variants. We have 23367 sequence variants but also a range of sequence lengths. Since many of these sequence variants are singletons or doubletons, we can just select a range that corresponds to the expected amplicon length and eliminate the spurious reads. But we will do this step after merging sequence tables from all 6 runs.\n\n\n\n\nDistribution of read length by total ASVs before removing extreme length variants.\n\n\n\n9. Tracking Reads\nFinally, we can look at how reads have changed so far in the pipeline.\n\n\nTotal reads per sample from input to merging.\n\n\n\n\n Download read count changes \nWe retained 74.3% of the reads from this run.\nAnd save the sequence table to an RDS file.\n\nsaveRDS(BCS_35, \"BCS_35.rds\")",
    "crumbs": [
      "16S rRNA Processing",
      "2. ASV Workflow"
    ]
  },
  {
    "objectID": "docs/ssu-workflows/asv/index.html#merged-runs-workflow",
    "href": "docs/ssu-workflows/asv/index.html#merged-runs-workflow",
    "title": "2. ASV Workflow",
    "section": "Merged Runs Workflow",
    "text": "Merged Runs Workflow\nNow it is time to combine the sequence tables from each run together into one merged sequences table.\nWe start by reading in each sequence table.\n\n\n\n\n\n\nNote\n\n\n\nYou can access the source code for this section here\n\n\n\n\nStep\nCommand\nWhat we’re doing\n\n\n\n10\nmergeSequenceTables()\nmerge seqtabs from all runs.\n\n\n11\nremoveBimeraDenovo()\nscreen for & remove chimeras\n\n\n12\n\ntrack reads through workflow\n\n\n13\nassignTaxonomy()\nassign taxonomy & finish workflow\n\n\n\n\n\n\n\n\nflowchart LR\n    D(BCS_26&lt;br/&gt;BCS_28&lt;br/&gt;BCS_29&lt;br/&gt;BCS_30&lt;br/&gt;BCS_34&lt;br/&gt;BCS_35&lt;br/&gt;) --&gt; M\n    D --&gt; M\n    D --&gt; M\n    D --&gt; M\n    D --&gt; M\n    D --&gt; M\n    M(mergeSequenceTables) --&gt; N(removeBimeraDenovo)\n    N --&gt; O(assignTaxonomy)\n\n\n\n\n\n\n\nBCS_26 &lt;- readRDS(\"`BCS_26.rds\")\nBCS_28 &lt;- readRDS(\"`BCS_28.rds\")\nBCS_29 &lt;- readRDS(\"`BCS_29.rds\")\nBCS_30 &lt;- readRDS(\"`BCS_30.rds\")\nBCS_34 &lt;- readRDS(\"`BCS_34.rds\")\nBCS_35 &lt;- readRDS(\"`BCS_35.rds\")\n\n10. Merge Sequencing Tables\n\nseqtab.merge &lt;- mergeSequenceTables(BCS_26, BCS_28, BCS_29, \n                                    BCS_30, BCS_34, BCS_35)\ndim(seqtab.merge)\n\n\n\n[1]  1909 96680\n\n\nSo our count is 96680 ASVs across 1909 samples.\n\ntable(nchar(getSequences(seqtab.merge)))\n\n  220   221   222   223   224   225   226   227   228   229   230   231   232 \n  124    67    14    36    20    13    10    25     8     6     4    30     2 \n  234   235   236   237   238   239   240   241   242   243   244   245   246 \n    9     8  1371   151    41     6    96   401   291   443    31    14    13 \n  247   248   249   250   251   252   253   254   255   256   257   258   259 \n   26    23    19    48   159  3606 83887  3756   315   121    95    20    10 \n  260   261   262   263   264   265   266   267   268   269   270   271   272 \n    8    16     9     4     2     1     1     1     4     2     9     8     4 \n  273   274   275   276   277   278   279   280   281   282   284   285   286 \n    7     3     2     5     1     7     4     1     1     2     1     4     4 \n  288   289   290   291   292   293   294   295   296   297   298   300   303 \n    1     3     1     2     4     8     7     2     3     2     2     2     3 \n  304   305   307   308   309   310   311   312   313   315   316   317   318 \n    1     5     2     3     2     1     3     1     3     1     4     1     3 \n  319   320   321   322   323   324   325   326   328   329   330   332   333 \n    3     1     2     3     2     1     3     1     3     3     2     1     3 \n  334   335   336   337   338   339   340   341   342   343   344   345   346 \n   13     6     7    17     5    25    16    70    51     8     7     8     4 \n  347   348   349   350   351   352   353   354   355   356   357   358   359 \n   28    17    21    10     2    11     1     1     7     6    31     6    15 \n  360   361   362   363   364   365   366   367   368   369   370   371   372 \n   21   161   186    43   135   107    19     9    26     5     3     3     8 \n  373   374   376   377   378   379   380   384   385   386   387   388 \n   11     2     3     5     2     3     1     1     1     1     2     1 \n\nread_length_all &lt;-  data.frame(nchar(getSequences(seqtab.merge)))\ncolnames(read_length_all) &lt;- \"length\"\nplot_all &lt;- qplot(length, data = read_length_all, geom = \"histogram\", \n                  binwidth = 1, xlab = \"read length\", \n                  ylab = \"total variants\", xlim = c(200,400)) \n\n\n\n\n\nDistribution of read length by total ASVs after merging & before removing extreme length variants.\n\n\n\nThen we remove length variants.\n\nseqtab.trim &lt;- seqtab.merge[,nchar(colnames(seqtab.merge)) %in% \n                              seq(252, 254)]\ndim(seqtab.trim)\n\n\n\n[1]  1909 91249\n\n\nAnd now our count is 91249 ASVs across 1909 samples.\n\ntable(nchar(getSequences(seqtab.trim)))\n\n 252   253   254 \n 3606 83887  3756 \n11. Remove Chimeras\nEven though the dada method corrects substitution and indel errors, chimeric sequences remain. According to the DADA2 documentation, the accuracy of sequence variants after denoising makes identifying chimeric ASVs simpler than when dealing with fuzzy OTUs. Chimeric sequences are identified if they can be exactly reconstructed by combining a left-segment and a right-segment from two more abundant parent sequences.\n\nseqtab.trim.nochim.consensus &lt;- \n  removeBimeraDenovo(seqtab.trim, \n                     method = \"consensus\", \n                     multithread = 20,  \n                     verbose = TRUE)\ndim(seqtab.trim.nochim.consensus)\n\nIdentified 18398 bimeras out of 91249 input sequences.\n\n\n[1]  1909 72851\n\n\n\nsum(seqtab.nochim)/sum(seqtab.2)\n\n[1] 0.9669996\nChimera checking removed an additional 18398 sequence variants however, when we account for the abundances of each variant, we see chimeras accounts for about 3.30004% of the merged sequence reads. Not bad.\n12. Track Reads through Workflow\nAt this point we can look at the number of reads that made it through each step of the workflow for every sample.\n\ngetN &lt;- function(x) sum(getUniques(x))\ntrack &lt;- cbind(rowSums(seqtab), \n               rowSums(seqtab.trim), \n               rowSums(seqtab.trim.nochim.pool), \n               rowSums(seqtab.trim.nochim.consensus))\n\ncolnames(track) &lt;- c(\"merged\", \"trim\", \n                     \"chimera_pool\", \n                     \"chimera_concensus\")\n\n\n\nTracking read changes at each step of the DADA2 workflow.\n\n\n\n\n Download read changes for the DADA2 pipeline \n13. Assign Taxonomy\nThe assignTaxonomy command implements the naive Bayesian classifier, so for reproducible results you need to set a random number seed (see issue #538). We did this at the beginning of the workflow. For taxonomic assignment, we are using the GSR database (Molano, Vega-Abellaneda, and Manichanh 2024). The developers of DADA2 maintain formatted versions of popular databases, however the GSR-DB has not been formatted by the developers yet.\n\n\n\n\n\n\nNote\n\n\n\nYou can download an appropriate version of the GSR database here.\n\n\nTo create a DADA2 formatted version GSR-DB1, we perform the following steps.\nDownload a data base\nHere we are using the GSR V4 database.\n\nwget https://manichanh.vhir.org/gsrdb/GSR-DB_V4_cluster-1.tar.gz\ntar -xvzf GSR-DB_V4_cluster-1.tar.gz\n\nOnce you uncompress the tar file you should see four files, two .qza files (which you can ignore), a _taxa.txt file and a _seqs.fasta file. We are interested in the latter two files. These are the files we need to format for DADA2. How about we have a look at each file?\nFirst the taxonomy file.\n\nhead GSR-DB_V4_cluster-1_taxa.txt\n\nFeature ID  Taxon\nAY999846    k__Bacteria; p__Actinobacteria; c__Actinomycetia; o__Actinomycetales-Streptomycetales-Unknown; f__Streptomycetaceae-Unknown; g__Streptomyces-Unknown; s__Unknown\nJN885187.1.1362 k__Bacteria; p__Actinobacteria; c__Actinomycetia; o__Actinomycetales-Streptomycetales-Unknown; f__Streptomycetaceae-Unknown; g__Kitasatospora-Streptomyces-Unknown; s__Unknown\nAnd next the fasta file.\n\nhead GSR-DB_V4_cluster-1_seqs.fasta\n\n&gt;AY999846\nTACGTAGGGCGCAAGCGTTGTCCGGAATTATTGGGCGTAAAGAGCTCGTAGGCGGCTTGT\nCACGTCGGTTGTGAAAGCCCGGGGCTTAACCCCGGGTCTGCAGTCGATACGGGCAGGCTA\nGAGTTCGGTAGGGGAGATCGGAATTCCTGGTGTAGCGGTGAAATGCGCAGATATCAGGAG\nGAACACCGGTGGCGAAGGCGGATCTCTGGGCCGATACTGACGCTGAGGAGCGAAAGCGTG\nGGGAGCGAACAGG\n&gt;JN885187.1.1362\nTACGTAGGGCGCGAGCGTTGTCCGGAATTATTGGGCGTAAAGAGCTCGTAGGCGGCTTGT\nCACGTCGGTTGTGAAAGCCCGGGGCTTAACCCCGGGTCTGCAGTCGATACGGGCAGGCTA\nGAGTTCGGTAGGGGAGATCGGAATTCCTGGTGTAGCGGTGAAATGCGCAGATATCAGGAG\nDADA2 requires a very specific format for classification. For the next few step we use a tool called SeqKit [Shen et al. (2016);shen2024seqkit2] for fasta defline manipulation.\nThis first step replaces the original fasta defline with the correspoding lineage information.\n\nconda activate seqkit\nseqkit replace -w 0  -p \"(.+)\" -r '{kv}' -k GSR-DB_V4_cluster-1_taxa.txt GSR-DB_V4_cluster-1_seqs.fasta &gt; tmp_1.fa\n\n[INFO] read key-value file: GSR-DB_V4_cluster-1_taxa.txt\n[INFO] 38802 pairs of key-value loaded\nHere is what the first few entries look like.\n&gt;k__Bacteria; p__Actinobacteria; c__Actinomycetia; o__Actinomycetales-Streptomycetales-Unknown; f__Streptomycetaceae-Unknown; g__Streptomyces-Unknown; s__Unknown\nTACGTAGGGCGCAAGCGTTGTCCGGAATTATTGGGCGTAAAGAGCTCGTAGGCGGCTTGTCACGTCGGTTGTGAAAGCCCGGGGCTTAACCCCGGGTCTGCAGTCGATACGGGCAGGCTAGAGTTCGGTAGGGGAGATCGGAATTCCTGGTGTAGCGGTGAAATGCGCAGATATCAGGAGGAACACCGGTGGCGAAGGCGGATCTCTGGGCCGATACTGACGCTGAGGAGCGAAAGCGTGGGGAGCGAACAGG\n&gt;k__Bacteria; p__Actinobacteria; c__Actinomycetia; o__Actinomycetales-Streptomycetales-Unknown; f__Streptomycetaceae-Unknown; g__Kitasatospora-Streptomyces-Unknown; s__Unknown\nTACGTAGGGCGCGAGCGTTGTCCGGAATTATTGGGCGTAAAGAGCTCGTAGGCGGCTTGTCACGTCGGTTGTGAAAGCCCGGGGCTTAACCCCGGGTCTGCAGTCGATACGGGCAGGCTAGAGTTCGGTAGGGGAGATCGGAATTCCTGGTGTAGCGGTGAAATGCGCAGATATCAGGAGGAACACCGGTGGCGAAGGCGGATCTCTGGGCCGATACTGACGCTGAGGAGCGAAAGCGTGGGGAGCGAACAGGATTAGATACCCTGGTAGTCCACGCCGTAAACGGTGGGCACTAGGTGTAGG\nNext we tidy up the deflines to remove spaces and leading taxonomic rank designations.\n\nseqkit replace -w 0  -p \" s__.*\" -r ''  tmp_1.fa &gt; tmp_2.fa\nseqkit replace -w 0  -p \"\\s\" -r ''  tmp_2.fa &gt; tmp_3.fa\nseqkit replace -w 0  -p \"\\w__\" -r ''  tmp_3.fa &gt; gsrdb_dada2.fa\nrm tmp_*\n\nAnd here are the first few lines of the final formatted GSR-DB fasta file.\n&gt;Bacteria;Actinobacteria;Actinomycetia;Actinomycetales-Streptomycetales-Unknown;Streptomycetaceae-Unknown;Streptomyces-Unknown;\nTACGTAGGGCGCAAGCGTTGTCCGGAATTATTGGGCGTAAAGAGCTCGTAGGCGGCTTGTCACGTCGGTTGTGAAAGCCCGGGGCTTAACCCCGGGTCTGCAGTCGATACGGGCAGGCTAGAGTTCGGTAGGGGAGATCGGAATTCCTGGTGTAGCGGTGAAATGCGCAGATATCAGGAGGAACACCGGTGGCGAAGGCGGATCTCTGGGCCGATACTGACGCTGAGGAGCGAAAGCGTGGGGAGCGAACAGG\n&gt;Bacteria;Actinobacteria;Actinomycetia;Actinomycetales-Streptomycetales-Unknown;Streptomycetaceae-Unknown;Kitasatospora-Streptomyces-Unknown;\nTACGTAGGGCGCGAGCGTTGTCCGGAATTATTGGGCGTAAAGAGCTCGTAGGCGGCTTGTCACGTCGGTTGTGAAAGCCCGGGGCTTAACCCCGGGTCTGCAGTCGATACGGGCAGGCTAGAGTTCGGTAGGGGAGATCGGAATTCCTGGTGTAGCGGTGAAATGCGCAGATATCAGGAGGAACACCGGTGGCGAAGGCGGATCTCTGGGCCGATACTGACGCTGAGGAGCGAAAGCGTGGGGAGCGAACAGGATTAGATACCCTGGTAGTCCACGCCGTAAACGGTGGGCACTAGGTGTAGG\nNow we can run the classification step.\n\nseqtab.consensus &lt;- seqtab.trim.nochim.consensus\ntax_gsrdb.consensus &lt;- \n  assignTaxonomy(seqtab.consensus, \n                 \"TAXONOMY_FILES/gsrdb_dada2.fa\",\n                 multithread = TRUE, \n                 verbose = TRUE)\nsaveRDS(tax_gsrdb.consensus, \"4.tax_gsrdb.consensus.rds\")",
    "crumbs": [
      "16S rRNA Processing",
      "2. ASV Workflow"
    ]
  },
  {
    "objectID": "docs/ssu-workflows/asv/index.html#r-session-information-code",
    "href": "docs/ssu-workflows/asv/index.html#r-session-information-code",
    "title": "2. ASV Workflow",
    "section": "R Session Information & Code",
    "text": "R Session Information & Code\nThis workflow was run on the Smithsonian High Performance Cluster (SI/HPC), Smithsonian Institution. Below are the specific packages and versions used in this workflow using both sessionInfo() and devtools::session_info(). Click the arrow to see the details.\n\nShow/hide R Session Info\nsessionInfo()\nR version 4.3.2 (2023-10-31)\nPlatform: x86_64-conda-linux-gnu (64-bit)\nRunning under: Rocky Linux 8.9 (Green Obsidian)\n\nMatrix products: default\nBLAS/LAPACK: /home/scottjj/miniconda3/envs/R/lib/libopenblasp-r0.3.25.so;  LAPACK version 3.11.0\n\nlocale:\n[1] C\n\ntime zone: America/New_York\ntzcode source: system (glibc)\n\nattached base packages:\n [1] parallel  stats4    grid      stats     graphics  grDevices utils    \n [8] datasets  methods   base     \n\nother attached packages:\n [1] DECIPHER_2.30.0             RSQLite_2.3.4              \n [3] ShortRead_1.60.0            GenomicAlignments_1.38.0   \n [5] SummarizedExperiment_1.32.0 Biobase_2.62.0             \n [7] MatrixGenerics_1.14.0       matrixStats_1.2.0          \n [9] Rsamtools_2.18.0            GenomicRanges_1.54.1       \n[11] Biostrings_2.70.1           GenomeInfoDb_1.38.2        \n[13] XVector_0.42.0              IRanges_2.36.0             \n[15] S4Vectors_0.40.2            BiocParallel_1.36.0        \n[17] BiocGenerics_0.48.1         decontam_1.22.0            \n[19] dplyr_1.1.4                 gridExtra_2.3              \n[21] phyloseq_1.46.0             ff_4.0.9                   \n[23] bit_4.0.5                   ggplot2_3.4.4              \n[25] dada2_1.30.0                Rcpp_1.0.11                \n\nloaded via a namespace (and not attached):\n [1] DBI_1.2.0               bitops_1.0-7            deldir_2.0-2           \n [4] permute_0.9-7           rlang_1.1.2             magrittr_2.0.3         \n [7] ade4_1.7-22             compiler_4.3.2          mgcv_1.9-1             \n[10] systemfonts_1.0.5       png_0.1-8               vctrs_0.6.5            \n[13] reshape2_1.4.4          stringr_1.5.1           pkgconfig_2.0.3        \n[16] crayon_1.5.2            fastmap_1.1.1           labeling_0.4.3         \n[19] utf8_1.2.4              ragg_1.2.7              zlibbioc_1.48.0        \n[22] cachem_1.0.8            jsonlite_1.8.8          biomformat_1.30.0      \n[25] blob_1.2.4              rhdf5filters_1.14.1     DelayedArray_0.28.0    \n[28] Rhdf5lib_1.24.1         jpeg_0.1-10             cluster_2.1.6          \n[31] R6_2.5.1                stringi_1.8.3           RColorBrewer_1.1-3     \n[34] iterators_1.0.14        Matrix_1.6-4            splines_4.3.2          \n[37] igraph_1.6.0            tidyselect_1.2.0        abind_1.4-5            \n[40] vegan_2.6-4             codetools_0.2-19        hwriter_1.3.2.1        \n[43] lattice_0.22-5          tibble_3.2.1            plyr_1.8.9             \n[46] withr_2.5.2             survival_3.5-7          RcppParallel_5.1.7     \n[49] pillar_1.9.0            foreach_1.5.2           generics_0.1.3         \n[52] RCurl_1.98-1.13         munsell_0.5.0           scales_1.3.0           \n[55] glue_1.6.2              tools_4.3.2             interp_1.1-5           \n[58] data.table_1.14.10      rhdf5_2.46.1            ape_5.7-1              \n[61] latticeExtra_0.6-30     colorspace_2.1-0        nlme_3.1-164           \n[64] GenomeInfoDbData_1.2.11 cli_3.6.2               textshaping_0.3.7      \n[67] fansi_1.0.6             S4Arrays_1.2.0          gtable_0.3.4           \n[70] digest_0.6.33           SparseArray_1.2.2       farver_2.1.1           \n[73] memoise_2.0.1           multtest_2.58.0         lifecycle_1.0.4        \n[76] bit64_4.0.5             MASS_7.3-60            \n\ndevtools::session_info()\n- Session info ---------------------------------------------------------------\n setting  value\n version  R version 4.3.2 (2023-10-31)\n os       Rocky Linux 8.9 (Green Obsidian)\n system   x86_64, linux-gnu\n ui       X11\n language (EN)\n collate  C\n ctype    C\n tz       America/New_York\n date     2024-08-15\n pandoc   3.1.3 @ /home/scottjj/miniconda3/envs/R/bin/pandoc\n\n- Packages -------------------------------------------------------------------\n package              * version   date (UTC) lib source\n abind                  1.4-5     2016-07-21 [1] CRAN (R 4.3.2)\n ade4                   1.7-22    2023-02-06 [1] CRAN (R 4.3.2)\n ape                    5.7-1     2023-03-13 [1] CRAN (R 4.3.2)\n Biobase              * 2.62.0    2023-10-24 [1] Bioconductor\n BiocGenerics         * 0.48.1    2023-11-01 [1] Bioconductor\n BiocParallel         * 1.36.0    2023-10-24 [1] Bioconductor\n biomformat             1.30.0    2023-10-24 [1] Bioconductor\n Biostrings           * 2.70.1    2023-10-25 [1] Bioconductor\n bit                  * 4.0.5     2022-11-15 [1] CRAN (R 4.3.0)\n bit64                  4.0.5     2020-08-30 [1] CRAN (R 4.3.0)\n bitops                 1.0-7     2021-04-24 [1] CRAN (R 4.3.2)\n blob                   1.2.4     2023-03-17 [1] CRAN (R 4.3.0)\n cachem                 1.0.8     2023-05-01 [1] CRAN (R 4.3.0)\n cli                    3.6.2     2023-12-11 [1] CRAN (R 4.3.2)\n cluster                2.1.6     2023-12-01 [1] CRAN (R 4.3.2)\n codetools              0.2-19    2023-02-01 [1] CRAN (R 4.3.0)\n colorspace             2.1-0     2023-01-23 [1] CRAN (R 4.3.0)\n crayon                 1.5.2     2022-09-29 [1] CRAN (R 4.3.0)\n dada2                * 1.30.0    2023-10-24 [1] Bioconductor\n data.table             1.14.10   2023-12-08 [1] CRAN (R 4.3.2)\n DBI                    1.2.0     2023-12-21 [1] CRAN (R 4.3.2)\n DECIPHER             * 2.30.0    2023-10-24 [1] Bioconductor\n decontam             * 1.22.0    2023-10-24 [1] Bioconductor\n DelayedArray           0.28.0    2023-10-24 [1] Bioconductor\n deldir                 2.0-2     2023-11-23 [1] CRAN (R 4.3.2)\n devtools               2.4.5     2022-10-11 [1] CRAN (R 4.3.2)\n digest                 0.6.33    2023-07-07 [1] CRAN (R 4.3.0)\n dplyr                * 1.1.4     2023-11-17 [1] CRAN (R 4.3.2)\n ellipsis               0.3.2     2021-04-29 [1] CRAN (R 4.3.0)\n fansi                  1.0.6     2023-12-08 [1] CRAN (R 4.3.2)\n farver                 2.1.1     2022-07-06 [1] CRAN (R 4.3.0)\n fastmap                1.1.1     2023-02-24 [1] CRAN (R 4.3.0)\n ff                   * 4.0.9     2023-01-25 [1] CRAN (R 4.3.2)\n foreach                1.5.2     2022-02-02 [1] CRAN (R 4.3.0)\n fs                     1.6.3     2023-07-20 [1] CRAN (R 4.3.1)\n generics               0.1.3     2022-07-05 [1] CRAN (R 4.3.0)\n GenomeInfoDb         * 1.38.2    2023-12-13 [1] Bioconductor 3.18 (R 4.3.2)\n GenomeInfoDbData       1.2.11    2023-12-26 [1] Bioconductor\n GenomicAlignments    * 1.38.0    2023-10-24 [1] Bioconductor\n GenomicRanges        * 1.54.1    2023-10-29 [1] Bioconductor\n ggplot2              * 3.4.4     2023-10-12 [1] CRAN (R 4.3.1)\n glue                   1.6.2     2022-02-24 [1] CRAN (R 4.3.0)\n gridExtra            * 2.3       2017-09-09 [1] CRAN (R 4.3.2)\n gtable                 0.3.4     2023-08-21 [1] CRAN (R 4.3.1)\n htmltools              0.5.7     2023-11-03 [1] CRAN (R 4.3.2)\n htmlwidgets            1.6.4     2023-12-06 [1] CRAN (R 4.3.2)\n httpuv                 1.6.13    2023-12-06 [1] CRAN (R 4.3.2)\n hwriter                1.3.2.1   2022-04-08 [1] CRAN (R 4.3.2)\n igraph                 1.6.0     2023-12-11 [1] CRAN (R 4.3.2)\n interp                 1.1-5     2023-11-27 [1] CRAN (R 4.3.2)\n IRanges              * 2.36.0    2023-10-24 [1] Bioconductor\n iterators              1.0.14    2022-02-05 [1] CRAN (R 4.3.0)\n jpeg                   0.1-10    2022-11-29 [1] CRAN (R 4.3.2)\n jsonlite               1.8.8     2023-12-04 [1] CRAN (R 4.3.2)\n labeling               0.4.3     2023-08-29 [1] CRAN (R 4.3.1)\n later                  1.3.2     2023-12-06 [1] CRAN (R 4.3.2)\n lattice                0.22-5    2023-10-24 [1] CRAN (R 4.3.1)\n latticeExtra           0.6-30    2022-07-04 [1] CRAN (R 4.3.2)\n lifecycle              1.0.4     2023-11-07 [1] CRAN (R 4.3.2)\n magrittr               2.0.3     2022-03-30 [1] CRAN (R 4.3.0)\n MASS                   7.3-60    2023-05-04 [1] CRAN (R 4.3.0)\n Matrix                 1.6-4     2023-11-30 [1] CRAN (R 4.3.2)\n MatrixGenerics       * 1.14.0    2023-10-24 [1] Bioconductor\n matrixStats          * 1.2.0     2023-12-11 [1] CRAN (R 4.3.2)\n memoise                2.0.1     2021-11-26 [1] CRAN (R 4.3.0)\n mgcv                   1.9-1     2023-12-21 [1] CRAN (R 4.3.2)\n mime                   0.12      2021-09-28 [1] CRAN (R 4.3.0)\n miniUI                 0.1.1.1   2018-05-18 [1] CRAN (R 4.3.2)\n multtest               2.58.0    2023-10-24 [1] Bioconductor\n munsell                0.5.0     2018-06-12 [1] CRAN (R 4.3.0)\n nlme                   3.1-164   2023-11-27 [1] CRAN (R 4.3.2)\n permute                0.9-7     2022-01-27 [1] CRAN (R 4.3.2)\n phyloseq             * 1.46.0    2023-10-24 [1] Bioconductor\n pillar                 1.9.0     2023-03-22 [1] CRAN (R 4.3.0)\n pkgbuild               1.4.3     2023-12-10 [1] CRAN (R 4.3.2)\n pkgconfig              2.0.3     2019-09-22 [1] CRAN (R 4.3.0)\n pkgload                1.3.3     2023-09-22 [1] CRAN (R 4.3.2)\n plyr                   1.8.9     2023-10-02 [1] CRAN (R 4.3.1)\n png                    0.1-8     2022-11-29 [1] CRAN (R 4.3.2)\n profvis                0.3.8     2023-05-02 [1] CRAN (R 4.3.2)\n promises               1.2.1     2023-08-10 [1] CRAN (R 4.3.1)\n purrr                  1.0.2     2023-08-10 [1] CRAN (R 4.3.1)\n R6                     2.5.1     2021-08-19 [1] CRAN (R 4.3.0)\n ragg                   1.2.7     2023-12-11 [1] CRAN (R 4.3.2)\n RColorBrewer           1.1-3     2022-04-03 [1] CRAN (R 4.3.0)\n Rcpp                 * 1.0.11    2023-07-06 [1] CRAN (R 4.3.0)\n RcppParallel           5.1.7     2023-02-27 [1] CRAN (R 4.3.2)\n RCurl                  1.98-1.13 2023-11-02 [1] CRAN (R 4.3.2)\n remotes                2.4.2.1   2023-07-18 [1] CRAN (R 4.3.2)\n reshape2               1.4.4     2020-04-09 [1] CRAN (R 4.3.0)\n rhdf5                  2.46.1    2023-11-29 [1] Bioconductor 3.18 (R 4.3.2)\n rhdf5filters           1.14.1    2023-11-06 [1] Bioconductor\n Rhdf5lib               1.24.1    2023-12-11 [1] Bioconductor 3.18 (R 4.3.2)\n rlang                  1.1.2     2023-11-04 [1] CRAN (R 4.3.2)\n Rsamtools            * 2.18.0    2023-10-24 [1] Bioconductor\n RSQLite              * 2.3.4     2023-12-08 [1] CRAN (R 4.3.2)\n S4Arrays               1.2.0     2023-10-24 [1] Bioconductor\n S4Vectors            * 0.40.2    2023-11-23 [1] Bioconductor 3.18 (R 4.3.2)\n scales                 1.3.0     2023-11-28 [1] CRAN (R 4.3.2)\n sessioninfo            1.2.2     2021-12-06 [1] CRAN (R 4.3.2)\n shiny                  1.8.0     2023-11-17 [1] CRAN (R 4.3.2)\n ShortRead            * 1.60.0    2023-10-24 [1] Bioconductor\n SparseArray            1.2.2     2023-11-07 [1] Bioconductor\n stringi                1.8.3     2023-12-11 [1] CRAN (R 4.3.2)\n stringr                1.5.1     2023-11-14 [1] CRAN (R 4.3.2)\n SummarizedExperiment * 1.32.0    2023-10-24 [1] Bioconductor\n survival               3.5-7     2023-08-14 [1] CRAN (R 4.3.1)\n systemfonts            1.0.5     2023-10-09 [1] CRAN (R 4.3.1)\n textshaping            0.3.7     2023-10-09 [1] CRAN (R 4.3.1)\n tibble                 3.2.1     2023-03-20 [1] CRAN (R 4.3.0)\n tidyselect             1.2.0     2022-10-10 [1] CRAN (R 4.3.0)\n urlchecker             1.0.1     2021-11-30 [1] CRAN (R 4.3.2)\n usethis                2.2.2     2023-07-06 [1] CRAN (R 4.3.0)\n utf8                   1.2.4     2023-10-22 [1] CRAN (R 4.3.1)\n vctrs                  0.6.5     2023-12-01 [1] CRAN (R 4.3.2)\n vegan                  2.6-4     2022-10-11 [1] CRAN (R 4.3.2)\n withr                  2.5.2     2023-10-30 [1] CRAN (R 4.3.1)\n xtable                 1.8-4     2019-04-21 [1] CRAN (R 4.3.0)\n XVector              * 0.42.0    2023-10-24 [1] Bioconductor\n zlibbioc               1.48.0    2023-10-24 [1] Bioconductor",
    "crumbs": [
      "16S rRNA Processing",
      "2. ASV Workflow"
    ]
  },
  {
    "objectID": "docs/ssu-workflows/asv/index.html#workflow-output",
    "href": "docs/ssu-workflows/asv/index.html#workflow-output",
    "title": "2. ASV Workflow",
    "section": "Workflow Output",
    "text": "Workflow Output\nData products generated in this workflow can be downloaded from figshare.\nPENDING\nThe end!",
    "crumbs": [
      "16S rRNA Processing",
      "2. ASV Workflow"
    ]
  },
  {
    "objectID": "docs/ssu-workflows/asv/index.html#read-counts-assessment",
    "href": "docs/ssu-workflows/asv/index.html#read-counts-assessment",
    "title": "2. ASV Workflow",
    "section": "Read Counts Assessment",
    "text": "Read Counts Assessment\nBefore we begin, let’s create a summary table containing some basic sample metadata and the read count data from the Sample Data section of the workflow. We want to inspect how total reads changed through the workflow. Table headers are as follows:\n\n\n\n\n\n\nHeader\nDescription\n\n\n\nSample ID\nNew sample ID based on Ocean, species, tissue, & unique ID\n\n\ninput rc\nNo. of raw reads\n\n\nfinal rc\nFinal read count after removing chimeras\n\n\nper reads retain\nPercent of reads remaining from input to final rc\n\n\n\ntotal ASVs\nNo. of ASVs\n\n\nOcean\nSampling ocean\n\n\nMorphospecies\nHost shrimp species\n\n\nTissue\nShrimp tissue type\n\n\nHabitat\nSampling habitat\n\n\nSite\nSampling site\n\n\nTaxon\nShrimp, environmental samples, Controls\n\n\nSpecies_Pair\nASK MATT\n\n\nSpecies_group\nASK MATT\n\n\nSpecies_complex\nASK MATT\n\n\nRun\nSequencing Run\n\n\nPlate\nPlate name\n\n\n\n\n\n\nSample metadata including read changes at start and end of DADA2 workflow.\n\n\n\n\n Download sample metadata",
    "crumbs": [
      "16S rRNA Processing",
      "2. ASV Workflow"
    ]
  },
  {
    "objectID": "docs/ssu-workflows/asv/index.html#prep-data-for-microeco",
    "href": "docs/ssu-workflows/asv/index.html#prep-data-for-microeco",
    "title": "2. ASV Workflow",
    "section": "Prep Data for microeco\n",
    "text": "Prep Data for microeco\n\nLike any tool, the microeco package needs the data in a specific form. I formatted our data to match the mock data in the microeco tutorial, specifically this section.\nA. Taxonomy Table\nHere is what the taxonomy table looks like in the mock data.\n\ntaxonomy_table_16S[1:6, 1:4]\n\n         Kingdom      Phylum            Class                 Order\nOTU_4272 k__Bacteria  p__Firmicutes     c__Bacilli            o__Bacillales\nOTU_236  k__Bacteria  p__Chloroflexi    c__                   o__\nOTU_399  k__Bacteria  p__Proteobacteria c__Betaproteobacteria o__Nitrosomonadales\nOTU_1556 k__Bacteria  p__Acidobacteria  c__Acidobacteria      o__Subgroup 17\nOTU_32   k__Archaea   p__Miscellaneous  c__                   o__\nOTU_706  k__Bacteria  p__Actinobacteria c__Actinobacteria     o__Frankiales\nThe first step is to rename the amplicon sequence variants so the designations are a bit more user friendly. By default, DADA2 names each ASV by its unique sequence so that data can be directly compared across studies (which is great). But this convention can get cumbersome downstream, so we rename the ASVs using a simpler convention—ASV1, ASV2, ASV3, and so on.\n\ntmp_tax &lt;- data.frame(tax_gsrdb)\n# adding unique ASV names\nrow.names(tmp_tax) &lt;- paste0(\"ASV\", seq(nrow(tmp_tax)))\n\nAnd this is how the taxonomy table looks after assigning new names.\n\n\n      Kingdom         Phylum               Class       Order\nASV1 Bacteria Proteobacteria Gammaproteobacteria        &lt;NA&gt;\nASV2 Bacteria Proteobacteria Gammaproteobacteria Vibrionales\nASV3 Bacteria           &lt;NA&gt;                &lt;NA&gt;        &lt;NA&gt;\nASV4 Bacteria           &lt;NA&gt;                &lt;NA&gt;        &lt;NA&gt;\nASV5 Bacteria           &lt;NA&gt;                &lt;NA&gt;        &lt;NA&gt;\nASV6 Bacteria           &lt;NA&gt;                &lt;NA&gt;        &lt;NA&gt;\n\n\nNext we need to add rank definitions to each classification.\n\ntmp_tax[is.na(tmp_tax)] &lt;- \"\"\ntmp_tax$Kingdom &lt;- paste(\"k\", tmp_tax$Kingdom, sep = \"__\")\ntmp_tax$Phylum &lt;- paste(\"p\", tmp_tax$Phylum, sep = \"__\")\ntmp_tax$Class &lt;- paste(\"c\", tmp_tax$Class, sep = \"__\")\ntmp_tax$Order &lt;- paste(\"o\", tmp_tax$Order, sep = \"__\")\ntmp_tax$Family &lt;- paste(\"f\", tmp_tax$Family, sep = \"__\")\ntmp_tax$Genus &lt;- paste(\"g\", tmp_tax$Genus, sep = \"__\")\n\ntmp_tax %&lt;&gt;% tidy_taxonomy\n\nAnd now the final, modified taxonomy table.\n\n\n         Kingdom            Phylum                  Class          Order\nASV1 k__Bacteria p__Proteobacteria c__Gammaproteobacteria            o__\nASV2 k__Bacteria p__Proteobacteria c__Gammaproteobacteria o__Vibrionales\nASV3 k__Bacteria               p__                    c__            o__\nASV4 k__Bacteria               p__                    c__            o__\nASV5 k__Bacteria               p__                    c__            o__\nASV6 k__Bacteria               p__                    c__            o__\n\n\nB. Sequence Table\nHere is what the sequence table looks like in the mock data.\n\notu_table_16S[1:6, 1:11]\n\n         S1 S2 S3 S4  S5  S6  S7 S9 S10 S11 S12\nOTU_4272  1  0  1  1   0   0   1  1   0   1   1\nOTU_236   1  4  0  2  35   5  94  0 177  14  27\nOTU_399   9  2  2  4   4   0   3  6   0   1   2\nOTU_1556  5 18  7  3   2   9   2  6   1   2   1\nOTU_32   83  9 19  8 102 300 158 55 321  16  13\nOTU_706   0  1  0  1   0   0   1  0   0   0   1\n\n\n\ntmp_st &lt;- data.frame(seqtab)\nidentical(colnames(tmp_st), row.names(tax_gsrdb))\nnames(tmp_st) &lt;- row.names(tmp_tax)\n\ntmp_st &lt;-  tmp_st %&gt;% tibble::rownames_to_column()\n\ntmp_st &lt;- tmp_st %&gt;%\n  tidyr::pivot_longer(cols = c(-1), names_to = \"tmp\") %&gt;%\n  tidyr::pivot_wider(names_from = c(1))\ntmp_st &lt;- tibble::column_to_rownames(tmp_st, \"tmp\")\n\nAnd now the final, modified sequence table.\n\n\n     Control_10 Control_11 Control_12 Control_1 Control_2 Control_3\nASV1          0          0          0         0         0         0\nASV2          0          0          0         0         0         0\nASV3          0          0          0         0         0         0\nASV4          0          0          0         0         2         1\nASV5          0          0          0         0         0         0\nASV6          0          0          2         0         0         0\n\n\nC. Sample Table\nHere is what the sample table looks like in the mock data.\n\nhead(sample_info_16S)\n\n   SampleID Group Type          Saline\nS1       S1    IW   NE Non-saline soil\nS2       S2    IW   NE Non-saline soil\nS3       S3    IW   NE Non-saline soil\nS4       S4    IW   NE Non-saline soil\nS5       S5    IW   NE Non-saline soil\nS6       S6    IW   NE Non-saline soil\n\n\n\nsamdf &lt;- readRDS(\"../sampledata/files/tables/samdf.rds\")\n\nsamdf &lt;- samdf %&gt;% tibble::column_to_rownames(\"SampleID\")\nsamdf$SampleID &lt;- rownames(samdf)\nsamdf &lt;- samdf %&gt;% relocate(SampleID)\n\nAnd now the final, modified sample table.\n\n\n             SampleID OCEAN SPECIES TISSUE ID    SITE\nControl_1   Control_1   CON     CON    CON  1 Control\nControl_10 Control_10   CON     CON    CON 10 Control\nControl_11 Control_11   CON     CON    CON 11 Control\nControl_12 Control_12   CON     CON    CON 12 Control\nControl_13 Control_13   CON     CON    CON 13 Control\nControl_14 Control_14   CON     CON    CON 14 Control",
    "crumbs": [
      "16S rRNA Processing",
      "2. ASV Workflow"
    ]
  },
  {
    "objectID": "docs/ssu-workflows/asv/index.html#create-a-microtable-object",
    "href": "docs/ssu-workflows/asv/index.html#create-a-microtable-object",
    "title": "2. ASV Workflow",
    "section": "Create a Microtable Object",
    "text": "Create a Microtable Object\nWith these three files in hand we are now ready to create a microtable object.\n\n\n\n\n\n\nNote\n\n\n\nA microtable object contains an ASV table (taxa abundances), sample metadata, and taxonomy table (mapping between ASVs and higher-level taxonomic classifications).\n\n\n\nsample_info &lt;- samdf\ntax_tab &lt;- tmp_tax\notu_tab &lt;- tmp_st\n\n\ntmp_me &lt;- microtable$new(sample_table = sample_info, \n                         otu_table = otu_tab, \n                         tax_table = tax_tab)\ntmp_me\n\nmicrotable-class object:\nsample_table have 1909 rows and 13 columns\notu_table have 72851 rows and 1909 columns\ntax_table have 72851 rows and 6 columns\nAdd Representative Sequence\nWe can also add representative sequences for each OTU/ASV. For this step, we can simply grab the sequences from the row names of the DADA2 taxonomy object loaded above.\n\ntmp_seq &lt;- data.frame(row.names(data.frame(tax_gsrdb)) )\ntmp_names &lt;- data.frame(row.names(tax_tab))\ntmp_fa &lt;- cbind(tmp_names, tmp_seq)\ncolnames(tmp_fa) &lt;- c(\"ASV_ID\", \"ASV_SEQ\")\ntmp_fa$ASV_ID &lt;- sub(\"^\", \"&gt;\", tmp_fa$ASV_ID)\n\nwrite.table(tmp_fa, \"files/tables/rep_seq.fasta\",\n            sep = \"\\n\", col.names = FALSE, row.names = FALSE,\n            quote = FALSE, fileEncoding = \"UTF-8\")       \nrep_fasta &lt;- Biostrings::readDNAStringSet(\"files/tables/rep_seq.fasta\")\ntmp_me$rep_fasta &lt;- rep_fasta\ntmp_me$tidy_dataset()\nhead(tmp_me$rep_fasta)\ntmp_me",
    "crumbs": [
      "16S rRNA Processing",
      "2. ASV Workflow"
    ]
  },
  {
    "objectID": "docs/ssu-workflows/asv/index.html#curate-the-data-set",
    "href": "docs/ssu-workflows/asv/index.html#curate-the-data-set",
    "title": "2. ASV Workflow",
    "section": "Curate the Data Set",
    "text": "Curate the Data Set\nPretty much the last thing to do is remove unwanted taxa, negative controls, and low-count samples.\nRemove any Kingdom NAs\nHere we can just use the straight up subset command since we do not need to worry about any ranks above Kingdom also being removed.\n\ntmp_no_na &lt;- microeco::clone(tmp_me)\ntmp_no_na$tax_table %&lt;&gt;% base::subset(Kingdom == \"k__Archaea\" | Kingdom == \"k__Bacteria\")\ntmp_no_na$tidy_dataset()\ntmp_no_na\n\n\n\nmicrotable-class object:\nsample_table have 1909 rows and 13 columns\notu_table have 72827 rows and 1909 columns\ntax_table have 72827 rows and 6 columns\nrep_fasta have 72827 sequences\n\n\nRemove Contaminants\nNow we can remove any potential contaminants like mitochondria or chloroplasts.\n\ntmp_no_cont &lt;- microeco::clone(tmp_no_na)\ntmp_no_cont$filter_pollution(taxa = c(\"mitochondria\", \"chloroplast\"))\ntmp_no_cont$tidy_dataset()\ntmp_no_cont\n\nTotal 0 features are removed from tax_table ...\n\n\nmicrotable-class object:\nsample_table have 1909 rows and 13 columns\notu_table have 72827 rows and 1909 columns\ntax_table have 72827 rows and 6 columns\nrep_fasta have 72827 sequences\n\n\nRemove Negative Controls (NC)\nNow we need to remove the NC samples and ASVs found in those sample. We first identified all ASVs that were present in at least one NC sample represented by at least 1 read. We did this by subsetting the NC samples from the new microtable object.\n\ntmp_nc &lt;- microeco::clone(tmp_no_cont)\ntmp_nc$sample_table &lt;- subset(tmp_nc$sample_table, TAXON == \"Control\")\ntmp_nc$tidy_dataset()\ntmp_nc\n\n\n\nmicrotable-class object:\nsample_table have 60 rows and 13 columns\notu_table have 596 rows and 60 columns\ntax_table have 596 rows and 6 columns\nrep_fasta have 596 sequences\n\n\nLooks like there are 596 ASVs in the NC samples from a total of 205834 reads.\n\nnc_asvs &lt;- row.names(tmp_nc$tax_table)\n\n\n\n [1] \"ASV1\"  \"ASV2\"  \"ASV4\"  \"ASV5\"  \"ASV6\"  \"ASV7\"  \"ASV10\" \"ASV11\" \"ASV13\"\n[10] \"ASV14\" \"ASV15\" \"ASV16\" \"ASV17\" \"ASV18\" \"ASV20\" \"ASV22\" \"ASV23\" \"ASV26\"\n[19] \"ASV28\" \"ASV31\"\n\n\nHum. ASVs are numbered in order by total abundance in the data set so we know that some of the ASVs in the NC samples are abundant in the dataset. We can look at the abundance of these ASVs across all samples and compare it to the NC. This takes a bit of wrangling.\nEssentially, for each ASV, the code below calculates:\n\nThe total number of NC samples containing at least 1 read.\n\nThe total number of reads in NC samples.\n\nThe total number of non-NC samples containing at least 1 read.\n\nThe total number of reads in non-NC samples.\n\nThe percent of reads in the NC samples and the percent of NC samples containing reads.\n\n\ntmp_rem_nc &lt;- microeco::clone(tmp_no_cont)\ntmp_rem_nc_df &lt;- tmp_rem_nc$otu_table\ntmp_rem_nc_df &lt;- tmp_rem_nc_df %&gt;% filter(row.names(tmp_rem_nc_df) %in% nc_asvs)\ntmp_rem_nc_df &lt;- tmp_rem_nc_df %&gt;% tibble::rownames_to_column(\"ASV_ID\")\n\n\ntmp_rem_nc_df &lt;- tmp_rem_nc_df  %&gt;% \n  mutate(total_reads_NC = rowSums(select(., contains(\"Control\"))), \n         .after = \"ASV_ID\")\ntmp_rem_nc_df &lt;- dplyr::select(tmp_rem_nc_df, -contains(\"Control\"))\ntmp_rem_nc_df &lt;- tmp_rem_nc_df %&gt;%\n  dplyr::mutate(total_reads_samps = rowSums(.[3:ncol(tmp_rem_nc_df)]), \n                .after = \"total_reads_NC\")\ntmp_rem_nc_df[, 4:ncol(tmp_rem_nc_df)] &lt;- list(NULL)\ntmp_rem_nc_df &lt;- tmp_rem_nc_df %&gt;%\n  dplyr::mutate(perc_in_neg = 100*(\n    total_reads_NC / (total_reads_NC + total_reads_samps)),\n                .after = \"total_reads_samps\")\n\n\ntmp_rem_nc_df$perc_in_neg &lt;- round(tmp_rem_nc_df$perc_in_neg, digits = 6)\n\ntmp_1 &lt;- data.frame(rowSums(tmp_rem_nc$otu_table != 0))\ntmp_1 &lt;- tmp_1 %&gt;% tibble::rownames_to_column(\"ASV_ID\")\ntmp_1 &lt;- tmp_1 %&gt;% dplyr::rename(\"total_samples\" = 2)  \n\ntmp_2 &lt;- dplyr::select(tmp_rem_nc$otu_table, contains(\"Control\"))\ntmp_2$num_samp_nc &lt;- rowSums(tmp_2 != 0)\ntmp_2 &lt;- dplyr::select(tmp_2, contains(\"num_samp_nc\"))\ntmp_2 &lt;- tmp_2 %&gt;% tibble::rownames_to_column(\"ASV_ID\")\n\ntmp_3 &lt;- dplyr::select(tmp_rem_nc$otu_table, -contains(\"Control\"))\ntmp_3$num_samp_no_nc &lt;- rowSums(tmp_3 != 0)\ntmp_3 &lt;- dplyr::select(tmp_3, contains(\"num_samp_no_nc\"))\ntmp_3 &lt;- tmp_3 %&gt;% tibble::rownames_to_column(\"ASV_ID\")\n\ntmp_rem_nc_df &lt;- dplyr::left_join(tmp_rem_nc_df, tmp_1) %&gt;%\n                 dplyr::left_join(., tmp_2) %&gt;%\n                 dplyr::left_join(., tmp_3)\n\ntmp_rem_nc_df &lt;- tmp_rem_nc_df %&gt;%\n  dplyr::mutate(perc_in_neg_samp = 100*( num_samp_nc / (num_samp_nc + num_samp_no_nc)),\n                .after = \"num_samp_no_nc\")\n\n\n\nSummary of ASVs detected in Negative Control (NC) samples.\n\n\n\n\n Download summary of ASVs detected in Negative Control (NC) samples \nLooking at these data we can see that ASVs like ASV1, ASV2, and ASV4 are only represented by a really small number of NC reads and samples. On the other hand, ASVs such as ASV91, ASV336, and ASV299 are very abundant in NC samples. We decided to remove ASVs if:\n\nThe number of reads found in NC samples accounted for more than 10% of total reads OR\nThe percent of NC samples containing the ASV was greater than 10% of total samples.\n\n\nnc_remove &lt;- nc_check %&gt;% \n  filter(perc_in_neg &gt; 10 | perc_in_neg_samp &gt; 10)\ntmp_rem_asv &lt;- nc_remove$ASV_ID %&gt;% \n  unlist(strsplit(., split = \", \")) \n\n\n\n\n\n\n\n\n\n\n\nTotal ASVs\nNC reads\nnon NC reads\n% NC reads\n\n\n\nRemoved\n371\n170482\n233876\n42.161\n\n\nRetained\n225\n35352\n12394240\n0.284\n\n\n\nWe identified a total of 596 ASVs that were present in at least 1 NC sample by at least 1 read. We removed any ASV where more than 10% of total reads were found in NC samples OR any ASV found in more than 10% of NC samples. Based on these criteria we removed 371 ASVs from the data set, which represented 170482 total reads in NC samples and 233876 total reads in non-NC samples. Of the total reads removed 42.161% came from NC samples. Of all ASVs identified in NC samples,225 were retained because the fell below the threshhold criteria. These ASVs accounted for 35352 reads in NC samples and 12394240 reads in non-NC samples. NC samples accounted for 0.284% of these reads.\nOK, now we can remove the NC samples and any ASVs that met our criteria described above.\n\ntmp_no_nc &lt;- microeco::clone(tmp_no_cont)\n\ntmp_rem_asv &lt;- as.factor(nc_remove$ASV_ID)\ntmp_no_nc$otu_table &lt;- tmp_rem_nc$otu_table %&gt;% \n  filter(!row.names(tmp_no_nc$otu_table) %in% tmp_rem_asv)\ntmp_no_nc$tidy_dataset()\n\ntmp_no_nc$sample_table &lt;- subset(tmp_no_nc$sample_table, \n                                 TAXON != \"Control\")\ntmp_no_nc$tidy_dataset()\ntmp_no_nc\n\n9 samples with 0 abundance are removed from the otu_table ...\n\n\nmicrotable-class object:\nsample_table have 1849 rows and 13 columns\notu_table have 72456 rows and 1849 columns\ntax_table have 72456 rows and 6 columns\nrep_fasta have 72456 sequences\n\n\nRemove Low-Count Samples\nNext, we can remove samples with really low read counts—here we set the threshold to 1000 reads.\n\ntmp_no_low &lt;- microeco::clone(tmp_no_nc)\ntmp_no_low$otu_table &lt;- tmp_no_nc$otu_table %&gt;%\n          dplyr::select(where(~ is.numeric(.) && sum(.) &gt;= 1000))\ntmp_no_low$tidy_dataset()\ntmp_no_low\n\n26 taxa with 0 abundance are removed from the otu_table ...\n\n\nmicrotable-class object:\nsample_table have 1838 rows and 13 columns\notu_table have 72430 rows and 1838 columns\ntax_table have 72430 rows and 6 columns\nrep_fasta have 72430 sequences\n\n\nGiving us the final microtable object.\n\nme_asv &lt;- microeco::clone(tmp_no_low)",
    "crumbs": [
      "16S rRNA Processing",
      "2. ASV Workflow"
    ]
  },
  {
    "objectID": "docs/ssu-workflows/asv/index.html#summary",
    "href": "docs/ssu-workflows/asv/index.html#summary",
    "title": "2. ASV Workflow",
    "section": "Summary",
    "text": "Summary\nNow time to summarize the data. For this we use the R package miaverse (Ernst et al. 2024).\nFirst we do a little formatting to get our data compatible with mia.\n\n# https://github.com/microbiome/OMA/issues/202\ntmp_counts &lt;- as.matrix(me_asv_raw$otu_table)\ntmp_assays &lt;-  SimpleList(counts = tmp_counts)\nmia_me_asv_raw &lt;- TreeSummarizedExperiment(assays = tmp_assays,\n                                colData = DataFrame(me_asv_raw$sample_table),\n                                rowData = DataFrame(me_asv_raw$tax_table))\nrm(list = ls(pattern = \"tmp_\"))\ntmp_counts &lt;- as.matrix(me_asv$otu_table)\ntmp_assays &lt;-  SimpleList(counts = tmp_counts)\nmia_me_asv &lt;- TreeSummarizedExperiment(assays = tmp_assays,\n                                colData = DataFrame(me_asv$sample_table),\n                                rowData = DataFrame(me_asv$tax_table))\nmia_me_asv_raw_summ &lt;- summary(mia_me_asv_raw, assay.type = \"counts\")\nmia_me_asv_summ &lt;- summary(mia_me_asv, assay.type = \"counts\")\nrm(list = ls(pattern = \"tmp_\"))\n\n\n\n\n\n\n\n\nMetric\nStart\nEnd\n\n\n\nMin. number reads\n49\n1202\n\n\nMax. number reads\n251022\n249532\n\n\nTotal number reads\n35764704\n35316367\n\n\nAvg number reads\n18735\n19215\n\n\nMedian number reads\n14827\n15253.5\n\n\nTotal ASVs\n72851\n72430\n\n\nNumber singleton ASVs\n102\n102\n\n\nAvg ASVs per sample.\n280\n285\n\n\n\nWe started off with 72851 ASVs and 1909 samples. Screening for NA kingdom assignment removed an additional 24 ASVs. Screening for Mitochondria and Chloroplast removed 0 ASVs. After removing the negative controls there were 72456 ASVs and 1849. After removing low-count samples, there were 72430 ASVs and 1838 samples remaining.",
    "crumbs": [
      "16S rRNA Processing",
      "2. ASV Workflow"
    ]
  },
  {
    "objectID": "docs/ssu-workflows/asv/index.html#footnotes",
    "href": "docs/ssu-workflows/asv/index.html#footnotes",
    "title": "2. ASV Workflow",
    "section": "Footnotes",
    "text": "Footnotes\n\nFrom the developers: GSR database (Greengenes, SILVA, and RDP database) is an integrated and manually curated database for bacterial and archaeal 16S amplicon taxonomy analysis. Unlike previous integration approaches, this database creation pipeline includes a taxonomy unification step to ensure consistency in taxonomical annotations. The database was validated with three mock communities and two real datasets and compared with existing 16S databases such as Greengenes, GTDB, ITGDB, SILVA, RDP, and MetaSquare. Results showed that the GSR database enhances taxonomical annotations of 16S sequences, outperforming current 16S databases at the species level. The GSR database is available for full-length 16S sequences and the most commonly used hypervariable regions: V4, V1-V3, V3-V4, and V3-V5.↩︎",
    "crumbs": [
      "16S rRNA Processing",
      "2. ASV Workflow"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Website Build",
    "section": "",
    "text": "This is how I created this website a) Quarto to build the site and b) GitHub Pages to host the site.\n\n\n\n\n\n\nCaution\n\n\n\nThis tutorial assumes you have a) git installed, b) a GitHub account, c) RStudio installed, and d) Quarto for R installed.\n\n\nYou will need to jump back and forth between the RStudio IDE and a terminal. You can use the terminal in RStudio if you wish, it is up to you.\nThere are two main options for a GitHub Pages website.\n\nA user or organization site.\n\nA project site.\n\nI suggest you have a look at the GitHub Pages for more information but basically the difference between the two is that a user/organization site is a single site, like:\nhttps://microbes.github.io/\nand individual pages on this site are indexed like so:\nhttps://microbes.github.io/about.html \nA project site on the other hand is like a collection of sites within a main site. A project site would have the same root URL, for example…\nhttps://microbes.github.io/\nbut each project would have it’s own unique extension, like these examples:\nhttps://microbes.github.io/deep-sea/   \nhttps://microbes.github.io/forest-soils/\nIndividual pages within a project site would be indexed like this:\nhttps://microbes.github.io/deep-sea/about.html\nThe instructions here are for a GitHub Pages project site.\nFor more information see the GitHub Pages documentation."
  },
  {
    "objectID": "about.html#overview",
    "href": "about.html#overview",
    "title": "Website Build",
    "section": "",
    "text": "This is how I created this website a) Quarto to build the site and b) GitHub Pages to host the site.\n\n\n\n\n\n\nCaution\n\n\n\nThis tutorial assumes you have a) git installed, b) a GitHub account, c) RStudio installed, and d) Quarto for R installed.\n\n\nYou will need to jump back and forth between the RStudio IDE and a terminal. You can use the terminal in RStudio if you wish, it is up to you.\nThere are two main options for a GitHub Pages website.\n\nA user or organization site.\n\nA project site.\n\nI suggest you have a look at the GitHub Pages for more information but basically the difference between the two is that a user/organization site is a single site, like:\nhttps://microbes.github.io/\nand individual pages on this site are indexed like so:\nhttps://microbes.github.io/about.html \nA project site on the other hand is like a collection of sites within a main site. A project site would have the same root URL, for example…\nhttps://microbes.github.io/\nbut each project would have it’s own unique extension, like these examples:\nhttps://microbes.github.io/deep-sea/   \nhttps://microbes.github.io/forest-soils/\nIndividual pages within a project site would be indexed like this:\nhttps://microbes.github.io/deep-sea/about.html\nThe instructions here are for a GitHub Pages project site.\nFor more information see the GitHub Pages documentation."
  },
  {
    "objectID": "about.html#key-steps",
    "href": "about.html#key-steps",
    "title": "Website Build",
    "section": "Key Steps",
    "text": "Key Steps\nHere is an overview of what we will be doing in this tutorial:\n\nCreate an organization on GitHub.\nCreate two GitHub repos for the site\n\none to host the organization URL\nanother to host the raw code plus the gh-pages branch.\n\n\nClone the code repo to your computer.\nCreate a gh-pages branch.\nBuild the initial site with Quarto for R.\nCustomize the default _quarto.yml file.\nRebuild the site.\nPush changes to GitHub."
  },
  {
    "objectID": "about.html#github-setup",
    "href": "about.html#github-setup",
    "title": "Website Build",
    "section": "1. GitHub Setup",
    "text": "1. GitHub Setup\nCreate organization on GitHub\n\n\n\n\n\n\nTip\n\n\n\nIf you already have an organization skip to the next step.\n\n\na) In the upper right-hand corner of your GitHub page click on your avatar and select Your organizations.\n\n\n\n\n\n\n\n\n\nb) On the page that opens, hit New organization.\n\n\n\n\n\n\n\n\n\nc) Next, give your organization a name, provide an email address, and indicate whether this is for a personal or institutional account. Then hit Next.\n\n\n\n\n\n\nTip\n\n\n\nThe name of the organization does not need to be the same as the website you create later.\n\n\n\n\n\n\n\n\n\n\n\nd) Here you can add additional community members such as collaborators. For now, just hit Complete setup.\n\n\n\n\n\n\n\n\nCreate repos for the site\nIn this section, we need to create two repositories for the site: i) one to host the organization URL (i.e., https://istmobiome.github.io/) and ii) another to host the raw code (e.g. .qmd files) plus the gh-pages branch. More on that in a minute. I am including instructions for things you MUST do in order to get the rest of the tutorial to work properly.\na) Hit the Create a new repository button.\n\n\n\n\n\n\n\n\n\nb) First, we create the github.io repo to host the main site.\nThere are a few things to do here:\n\nName the repo, Here we call it istmobiome.github.io. it doesn’t matter what you call this but it must be unique and have the .github.io extension.\nMake sure repo is public.\n\nYou must Initialize this repository with a README File.\nHit the Create repository button. This creates the main branch of the repo.\n\n\n\n\n\n\n\nTip\n\n\n\nNote: you can host a site on GitHub without the github.io extension but that is a more advanced skill and beyond the scope of this tutorial.\n\n\n\n\n\n\n\n\n\n\n\nc) Next, we create the build repo to host the project source code. These steps are similar to those described above.\n\nName the repo, here we call it trans-shrimp. This will be the extension of the project site. So this project’s URL will be https://istmobiome.github.io/trans-shrimp/.\nMake sure the repo is public.\n\nYou must Initialize this repository with a README File.\nHit the Create repository button. This creates the main branch of the repo.\n\n\nNote: you can make these repos private, however that is a bit more advanced and beyond the scope of the tutorial provided here :)\n\n\n\n\n\n\n\n\n\nReview\nHere is what we have so far:\nAn organization at https://github.com/istmobiome/ that contains 2 repos, one for the GitHub Pages root URL, https://github.com/istmobiome/istmobiome.github.io and another for the source code, https://github.com/istmobiome/trans-shrimp. At this point both repos should only contain README files.\nWe also have two URLs, one for the organiztion site (https://istmobiome.github.io/) and another for the project site (https://istmobiome.github.io/trans-shrimp/)."
  },
  {
    "objectID": "about.html#local-setup",
    "href": "about.html#local-setup",
    "title": "Website Build",
    "section": "2. Local Setup",
    "text": "2. Local Setup\nNow that we have the main pieces in place of GitHub, it is time to setup everything on our local machine.\nClone the Code Repo\nLet’s take a look at the trans-shrimp repo page. There are two important things to point out. First, in the upper left corner is a box that says main. This is the primary branch for the repo and contains the source code for the site build (e.g., the .qmd files). This will become important in a moment when we create a branch called gh-pages that will link the build files (e.g., .html files) to the organization URL. More on that in a minute.\nFirst, we need to clone the trans-shrimp repo from GitHub to our local machine. There are several ways to do this, but I will show you how I do it. Navigate to the repo page. At the time of creation, it looked like this:\n\n\n\n\n\n\n\n\n\nSee the green button that says Code. Click on that button and copy the URL.\n\n\n\n\n\n\n\n\n\nOpen a terminal window and navigate to a place where you want the repo to live on your computer. We will use the command git clone with the URL you just copied. Run the clone command and then cd into the directory.\n\ngit clone https://github.com/istmobiome/trans-shrimp.git\ncd trans-shrimp/\n\nNow have a look at the contents of the directory using the ls command. We want to append the command with the -al flag. The l option provides extended details about each file/directory and the a option lists hidden files—files/directories that begin with a period and are hidden by default.\n\nls -al\n\nSo we have the README.md file that was generated when we created the repo and a hidden directory called .git. The .git directory is super, super important because this is how the remote repo (on GitHub) and the local repo (on your computer) keep track of changes.\ntotal 8\ndrwxr-xr-x@  4 scottjj  923590601  128 May 13 08:25 .\ndrwxr-xr-x@  5 scottjj  923590601  160 May 13 08:25 ..\ndrwxr-xr-x@ 12 scottjj  923590601  384 May 13 08:25 .git\n-rw-r--r--@  1 scottjj  923590601   11 May 13 08:25 README.md\nCreate a gh-pages branch\nGreat. We have the trans-shrimp repo cloned on our local machine. For historical reason, a GitHub Pages site needs to be built to a directory called public/. But we do not need to upload the public/ build directory to the main branch on our repo. So, we are going to create a .gitignore file and add public/ to that file. The .gitignore tells git to ignore whatever is listed in that file when it pushes local changes to the remote repository. As you build your site, you will need to add additional items to the .gitignore file.\n\necho public/ &gt;&gt; .gitignore # Add items as needed.\n\nIf you run ls -al again you should see the .gitignore file is now listed in the directory. If you type nano .gitignore you can see the contents of the file.\nOK, now let’s push the changes we made to the local branch (origin) to the remote branch (main). First, run:\n\ngit status\n\nHopefully you see the .gitignore file listed in red along with some other details. This means there are untracked files in your repo. Go ahead and run:\n\ngit add --all\ngit status\n\nNow you should see the same files in green. This means the files are staged and ready to commit. Now run:\n\ngit commit -a -m \"initial commit\"\ngit push origin main\n\nIf everything worked OK, your files have been pushed to the GitHub repo. Have a look at the code repo on GitHub to confirm your changes have been pushed.\n\n\n\n\n\n\n\n\n\nNow, it is important that there is no public/ directory yet. Just to make sure there is no public/ directory, we can run the remove command in our terminal.\n\nrm -rf public \n\nNow we go through the steps to create the the gh-pages branch.\n\ngit checkout --orphan gh-pages\n\nSwitched to a new branch 'gh-pages'\n\ngit reset --hard\ngit commit --allow-empty -m \"Initializing gh-pages branch\"\n\n[gh-pages (root-commit) fd6028a] Initializing gh-pages branch\n\ngit push origin gh-pages\n\nEnumerating objects: 2, done.\nCounting objects: 100% (2/2), done.\nWriting objects: 100% (2/2), 180 bytes | 180.00 KiB/s, done.\nTotal 2 (delta 0), reused 0 (delta 0), pack-reused 0\nremote:\nremote: Create a pull request for 'gh-pages' on GitHub by visiting:\nremote:      https://github.com/istmobiome/trans-shrimp/pull/new/gh-pages\nremote:\nTo https://github.com/istmobiome/trans-shrimp.git\n * [new branch]      gh-pages -&gt; gh-pages\n\ngit checkout main\n\nEnumerating objects: 2, done.\nCounting objects: 100% (2/2), done.\nWriting objects: 100% (2/2), 180 bytes | 180.00 KiB/s, done.\nTotal 2 (delta 0), reused 0 (delta 0), pack-reused 0\nremote:\nremote: Create a pull request for 'gh-pages' on GitHub by visiting:\nremote:      https://github.com/istmobiome/trans-shrimp/pull/new/gh-pages\nremote:\nTo https://github.com/istmobiome/trans-shrimp.git\n * [new branch]      gh-pages -&gt; gh-pages\n(base) trans-shrimp: scottjj$ git checkout main\nSwitched to branch 'main'\nYour branch is up to date with 'origin/main'.\n\ngit worktree add -B gh-pages public origin/gh-pages\n\nPreparing worktree (resetting branch 'gh-pages'; was at fd6028a)\nBranch 'gh-pages' set up to track remote branch 'gh-pages' from 'origin'.\nHEAD is now at fd6028a Initializing gh-pages branch\nNow run the following to see what is in the repo directory.\n\nls -al\n\ndrwxr-xr-x@  6 scottjj  923590601  192 May 13 08:41 .\ndrwxr-xr-x@  5 scottjj  923590601  160 May 13 08:25 ..\ndrwxr-xr-x@ 14 scottjj  923590601  448 May 13 08:41 .git\n-rw-r--r--@  1 scottjj  923590601    8 May 13 08:40 .gitignore\n-rw-r--r--@  1 scottjj  923590601   11 May 13 08:40 README.md\ndrwxr-xr-x@  3 scottjj  923590601   96 May 13 08:41 public\nThere should now be a public/ directory. Run the following to make sure this is a .git directory.\n\nls -al public/\n\ntotal 8\ndrwxr-xr-x@ 3 scottjj  923590601   96 May 13 08:41 .\ndrwxr-xr-x@ 6 scottjj  923590601  192 May 13 08:41 ..\n-rw-r--r--@ 1 scottjj  923590601  104 May 13 08:41 .git\nReview\nAt this point all the pieces are in place to build your site and host it on GitHub. In addition to the repos you have on GitHub, you should have a local directory of the code repo, in our case it is called trans-shrimp. In this directory there should be a README.md file, a .gitignore file, a .git directory, and the public/ directory."
  },
  {
    "objectID": "about.html#site-build",
    "href": "about.html#site-build",
    "title": "Website Build",
    "section": "3. Site Build",
    "text": "3. Site Build\nTime to build the site. As mentioned in the intro, we will use Quarto to create the website. To keep things simple, we will generate the initial site in a temporary directory and copy the files to the git formatted directory at the end.\nOpen RStudio and hit File &gt;&gt; New Project. In the pop-up window hit New Project.\n\n\n\n\n\n\n\n\n\nThen in the New Project Wizard select the New Directory option.\n\n\n\n\n\n\n\n\n\nUnder Project Type, scroll down until you see Quarto Website and select that option.\n\n\n\n\n\n\n\n\n\nGive the Directory a name and hit Create Project. Consider this an intermediate step in the process. We need to create a project directory to generate the initial site. Eventually we will copy all of these file to the directory of the repo we cloned earlier. You can call this something like web if you wish, it doesn’t really matter since this is only a temporary directory.\n\n\n\n\n\n\n\n\n\nNow, if everything worked OK, you should see that five files were created and two of the files are open in your RStudio IDE.\n– index.qmd. This is the landing page of your site. This file is the first thing people see when they navigate to your site.\n– _quarto.yml. This is a YAML file and is used to configure your site. DO NOT erase this file.\n\n\n\n\n\n\n\n\n\nIn fact, we need to edit the _quarto.yml file to configure it for our site. As you move further into building your site, you will need to make many modifications to this file. For now however, we will just focus on the minimum changes you need to make to get your site online. Go ahead and click on that file. This is what the default file looks like.\n\n\n\n\n\n\n\n\n\nFirst, below the title line (nested under website) add the following lines.\nsite-url: https://istmobiome.github.io/trans-shrimp/\nrepo-url: https://github.com/istmobiome/trans-shrimp/\nThe site-url is the .github.io URL we created way back when we created the site repo. The repo-url is the repo we created to host the project source code.\nWhen you build or render your site, RStudio will dump all the site files (e.g., .html files) in the output_dir. The default output directory is _site, which RStudio generates when you render the site for the first time. I see no reason why you cannot leave the build directory as is but I prefer to call mine public_build/. Go ahead and add a line of the _quarto.yml file specifying the output-dir. This should be nested under project like so:\nproject:\n  type: website\n  output-dir: public_build\n\nGreat. One last thing to do. When you render your site, RStudio will try to add everything in the main directory to the output directory, in this case public_build/. There are many cases where this is not desirable. For example, let’s say we have you have a directory called sequence_data that contains a bunch of fastq files. We do not need these files to build the site (and we probably do not want these on GitHub) so we need to tell RStudio to ignore these files when it builds the site. For that, we add the line render nested under project. Here we can specify what is, and is not, rendered when the site is build. As you get deeper into building your site, you will likely have many files/directories that you want excluded.\n\nDon’t get confused here. The render line in the _quarto.yml file is different than the .gitignore file discussed earlier. They do completely different things. If this is unclear, go back and review the section on the .gitignore file.\n\nFor now, we will focus on the necessary things to include and exclude from the build. Items must be listed on separate lines, in double quotes. So let’s say we want to render all .qmd files but ignore a few directories. The way to code this is that anything we want to exclude should begin with an exclamation point (!). Go ahead and add these lines.\nproject:\n  type: website\n  output-dir: public_build\n  render:\n    - \"*.qmd\"\n    - \"!public/\"\n    - \"!public_build/\"\nOK, a little explanation. The first thing we need to exclude in the actual build directory public_build/ (or _site if you kept the default). If we do not include this, RStudio will get stuck in a loop where it continues to add public_build/ to the public_build/ directory. Next, remember that to host a gh-pages site, the build files (e.g., the .html files) must be in a directory called public/. We created this directory when we created the gh-pages branch.\n\nSo why don’t we just make the buld directory public/ instead of public_build/ or _site/?\n\nGood question. The reason is that public/ is a special directory that contains git related files (hidden by default). When we render a site, RStudio will first overwrite the output directory, thereby deleting the .git files, and then recreate the output directory add the new build files. This is not cool. To avoid this, we use an intermediate output directory, public_build/ (or whatever you call it), to generate the site. Than we can copy all the files from the output directory to the public/ directory without losing the git info. This is a little cumbersome but I have not yet found a workaround. That said, there may be an easier way to do this following the GitHub Pages tutorial on Quarto. I will leave that up to you since I just found this tutorial and I am already too deep to change now.\nIf you changed the name of the default build directory, go ahead and remove the default directory now. In your terminal type:\n\nrm -r _site\n\nNow, save all of your changes to the _quarto.yml file and in the RStudio Console run the following:\n\nquarto::quarto_render()\n\nYou should see a new directory called public_build/. Open that folder and double-click on index.html. The homepage of your new site should open in your default browser.\nReview\nSweet. Let’s review what we have so far. From the\n\n1. GitHub Setup we created two repos, one to host the .github.io organization and another to host the raw code (e.g. .qmd files) plus the gh-pages branch.\n2. Local Setup we cloned the code repo and create a gh-pages branch in a directory called public/.\n3. Site Build we generated an initial site within a dummy directory, modified the _quarto.yml file, and re-rendered the site to an output directory called public_build/"
  },
  {
    "objectID": "about.html#integration",
    "href": "about.html#integration",
    "title": "Website Build",
    "section": "Integration",
    "text": "Integration\nTime to push our changes to GitHub. The first step is to copy all of the files in your dummy site directory to the directory of the cloned repo (trans-shrimp).\n\n\n\n\n\n\n\n\n\nFirst, in your terminal, navigate to the cloned repo directory. Use the pwd command to confirm your location.\n\npwd\n\n/Users/scottjj/Dropbox (Smithsonian)/GITHUB_PROJECTS/TRANS-ISHMIAN/trans-shrimp\nAssuming your directory structure setup is similar to the image above, next run the following to copy all of the files.\n\ncp -r ../dummy/* .\n\nNow take a look at the contents of the current working directory.\n\nls -al\n\nIt should look something like this:\ndrwxr-xr-x  14 scottjj  923590601   448 May 13 09:18 .\ndrwxr-xr-x@  7 scottjj  923590601   224 May 13 09:15 ..\n-rw-r--r--@  1 scottjj  923590601  6148 May 13 09:18 .DS_Store\n-rw-r--r--   1 scottjj  923590601    50 May 13 09:18 .Rhistory\ndrwxr-xr-x   4 scottjj  923590601   128 May 13 09:18 .Rproj.user\ndrwxr-xr-x@ 14 scottjj  923590601   448 May 13 08:41 .git\n-rw-r--r--@  1 scottjj  923590601     8 May 13 08:40 .gitignore\n-rw-r--r--@  1 scottjj  923590601    11 May 13 08:40 README.md\n-rw-r--r--   1 scottjj  923590601   202 May 13 09:18 _quarto.yml\n-rw-r--r--   1 scottjj  923590601    58 May 13 09:18 about.qmd\n-rw-r--r--   1 scottjj  923590601   145 May 13 09:18 index.qmd\n-rw-r--r--   1 scottjj  923590601    17 May 13 09:18 styles.css\ndrwxr-xr-x@  3 scottjj  923590601    96 May 13 08:41 public\ndrwxr-xr-x   7 scottjj  923590601   224 May 13 09:18 public_build\n-rw-r--r--   1 scottjj  923590601   225 May 13 09:18 web.Rproj\nAt this point we need to do a few final housekeeping tasks. First, let’s add a few additional files to the .gitignore file.\nRun this command in your terminal. You should only see the public/ directory listed.\n\nnano .gitignore\n\nAdd the following lines, save the file, and exit nano.\n.Rproj.user\n.Rhistory\n.RData\n.Ruserdata\n.Rapp.history\n.DS_Store\nweb.Rproj\n\npublic_build/\n\nIf you using macOS you may want to remove the .DS_Store directories before pushihng your build to GitHub.\n\n\nfind . -name \".DS_Store\" -print -delete\n\nNow, remove everything from public and copy over all of the files from public_build/.\n\nrm -r public/*\ncp -r public_build/* public/\n\nThen move into the public/ directory and run git status.\n\ncd public\ngit status\n\nAll the untracked or changed files should be in red.\nOn branch gh-pages\nYour branch is up to date with 'origin/gh-pages'.\n\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n    about.html\n    index.html\n    search.json\n    site_libs/\n    sitemap.xml\n\nnothing added to commit but untracked files present (use \"git add\" to track)\nNow add the files to be committed and check the status again.\n\ngit add --all\ngit status\n\nAll the untracked or changed files should now be green.\n\n\n\n\n\n\nExpand to see the output of git status\n\n\n\n\n\nOn branch gh-pages\nYour branch is up to date with 'origin/gh-pages'.\n\nChanges to be committed:\n  (use \"git restore --staged &lt;file&gt;...\" to unstage)\n    new file:   about.html\n    new file:   index.html\n    new file:   search.json\n    new file:   site_libs/anchor-4.2.2/anchor.min.js\n    new file:   site_libs/autocomplete-0.37.1/autocomplete.min.js\n    new file:   site_libs/bowser-1.9.3/bowser.min.js\n    new file:   site_libs/distill-2.2.21/template.v2.js\n    new file:   site_libs/font-awesome-5.1.0/css/all.css\n    new file:   site_libs/font-awesome-5.1.0/css/v4-shims.css\n    new file:   site_libs/font-awesome-5.1.0/webfonts/fa-brands-400.eot\n    new file:   site_libs/font-awesome-5.1.0/webfonts/fa-brands-400.svg\n    new file:   site_libs/font-awesome-5.1.0/webfonts/fa-brands-400.ttf\n    new file:   site_libs/font-awesome-5.1.0/webfonts/fa-brands-400.woff\n    new file:   site_libs/font-awesome-5.1.0/webfonts/fa-brands-400.woff2\n    new file:   site_libs/font-awesome-5.1.0/webfonts/fa-regular-400.eot\n    new file:   site_libs/font-awesome-5.1.0/webfonts/fa-regular-400.svg\n    new file:   site_libs/font-awesome-5.1.0/webfonts/fa-regular-400.ttf\n    new file:   site_libs/font-awesome-5.1.0/webfonts/fa-regular-400.woff\n    new file:   site_libs/font-awesome-5.1.0/webfonts/fa-regular-400.woff2\n    new file:   site_libs/font-awesome-5.1.0/webfonts/fa-solid-900.eot\n    new file:   site_libs/font-awesome-5.1.0/webfonts/fa-solid-900.svg\n    new file:   site_libs/font-awesome-5.1.0/webfonts/fa-solid-900.ttf\n    new file:   site_libs/font-awesome-5.1.0/webfonts/fa-solid-900.woff\n    new file:   site_libs/font-awesome-5.1.0/webfonts/fa-solid-900.woff2\n    new file:   site_libs/fuse-6.4.1/fuse.min.js\n    new file:   site_libs/header-attrs-2.11/header-attrs.js\n    new file:   site_libs/headroom-0.9.4/headroom.min.js\n    new file:   site_libs/jquery-3.6.0/jquery-3.6.0.js\n    new file:   site_libs/jquery-3.6.0/jquery-3.6.0.min.js\n    new file:   site_libs/jquery-3.6.0/jquery-3.6.0.min.map\n    new file:   site_libs/popper-2.6.0/popper.min.js\n    new file:   site_libs/tippy-6.2.7/tippy-bundle.umd.min.js\n    new file:   site_libs/tippy-6.2.7/tippy-light-border.css\n    new file:   site_libs/tippy-6.2.7/tippy.css\n    new file:   site_libs/tippy-6.2.7/tippy.umd.min.js\n    new file:   site_libs/webcomponents-2.0.0/webcomponents.js\n    new file:   sitemap.xml\n\n\n\nCommit the files.\n\ngit commit -m \"Publishing to gh-pages initial commit\"\n\n\n\n\n\n\n\nExpand to see the output of git commit\n\n\n\n\n\n[gh-pages f30f1ff] Publishing to gh-pages initial commit\n 37 files changed, 22182 insertions(+)\n create mode 100644 about.html\n create mode 100644 index.html\n create mode 100644 search.json\n create mode 100644 site_libs/anchor-4.2.2/anchor.min.js\n create mode 100644 site_libs/autocomplete-0.37.1/autocomplete.min.js\n create mode 100644 site_libs/bowser-1.9.3/bowser.min.js\n create mode 100644 site_libs/distill-2.2.21/template.v2.js\n create mode 100644 site_libs/font-awesome-5.1.0/css/all.css\n create mode 100644 site_libs/font-awesome-5.1.0/css/v4-shims.css\n create mode 100644 site_libs/font-awesome-5.1.0/webfonts/fa-brands-400.eot\n create mode 100644 site_libs/font-awesome-5.1.0/webfonts/fa-brands-400.svg\n create mode 100644 site_libs/font-awesome-5.1.0/webfonts/fa-brands-400.ttf\n create mode 100644 site_libs/font-awesome-5.1.0/webfonts/fa-brands-400.woff\n create mode 100644 site_libs/font-awesome-5.1.0/webfonts/fa-brands-400.woff2\n create mode 100644 site_libs/font-awesome-5.1.0/webfonts/fa-regular-400.eot\n create mode 100644 site_libs/font-awesome-5.1.0/webfonts/fa-regular-400.svg\n create mode 100644 site_libs/font-awesome-5.1.0/webfonts/fa-regular-400.ttf\n create mode 100644 site_libs/font-awesome-5.1.0/webfonts/fa-regular-400.woff\n create mode 100644 site_libs/font-awesome-5.1.0/webfonts/fa-regular-400.woff2\n create mode 100644 site_libs/font-awesome-5.1.0/webfonts/fa-solid-900.eot\n create mode 100644 site_libs/font-awesome-5.1.0/webfonts/fa-solid-900.svg\n create mode 100644 site_libs/font-awesome-5.1.0/webfonts/fa-solid-900.ttf\n create mode 100644 site_libs/font-awesome-5.1.0/webfonts/fa-solid-900.woff\n create mode 100644 site_libs/font-awesome-5.1.0/webfonts/fa-solid-900.woff2\n create mode 100644 site_libs/fuse-6.4.1/fuse.min.js\n create mode 100644 site_libs/header-attrs-2.11/header-attrs.js\n create mode 100644 site_libs/headroom-0.9.4/headroom.min.js\n create mode 100644 site_libs/jquery-3.6.0/jquery-3.6.0.js\n create mode 100644 site_libs/jquery-3.6.0/jquery-3.6.0.min.js\n create mode 100644 site_libs/jquery-3.6.0/jquery-3.6.0.min.map\n create mode 100644 site_libs/popper-2.6.0/popper.min.js\n create mode 100644 site_libs/tippy-6.2.7/tippy-bundle.umd.min.js\n create mode 100644 site_libs/tippy-6.2.7/tippy-light-border.css\n create mode 100644 site_libs/tippy-6.2.7/tippy.css\n create mode 100644 site_libs/tippy-6.2.7/tippy.umd.min.js\n create mode 100644 site_libs/webcomponents-2.0.0/webcomponents.js\n create mode 100644 sitemap.xml\n\n\n\nAnd finally push the files to the gh-pages branch.\n\ngit push origin gh-pages\n\nEnumerating objects: 55, done.\nCounting objects: 100% (55/55), done.\nDelta compression using up to 8 threads\nCompressing objects: 100% (45/45), done.\nWriting objects: 100% (54/54), 1.37 MiB | 348.00 KiB/s, done.\nTotal 54 (delta 1), reused 0 (delta 0), pack-reused 0\nremote: Resolving deltas: 100% (1/1), done.\nTo https://github.com/istmobiome/trans-shrimp.git\n   fd6028a..f30f1ff  gh-pages -&gt; gh-pages\nDo the same for the build files. Make sure you back out of the public directory with the cd ../ command. This time you are pushing to the main branch, not the gh-pages branch.\n\ncd ../\ngit status\ngit add --all\ngit status\ngit commit -m \"Publishing to gh-pages initial commit\"\ngit push origin main\n\nNow head over to your GitHub code repo and look at the two branches to make sure everything looks good. In this case, https://github.com/istmobiome/trans-shrimp. You should see the main branch of this repo—this contains all of the .qmd files.\n\n\n\n\n\n\n\n\n\nFind the button in the upper left that says main. Click on that and select gh-pages.\n\n\n\n\n\n\n\n\n\nThe gh-pages branch contains all of the .html files.\n\n\n\n\n\n\n\n\n\nWait a few minutes and navigate to your project webpage. In our case, https://istmobiome.github.io/trans-shrimp/.\nYou site should be LIVE."
  },
  {
    "objectID": "about.html#adding-content",
    "href": "about.html#adding-content",
    "title": "Website Build",
    "section": "Adding content",
    "text": "Adding content\nMost of the steps up to this point should only need to be done once. Now it is time to add content to your site, customize the look, etc. From this point forward, whenever you make changes that you want to make public, you need to do the following:\n\nRender the site.\n\n\nquarto::quarto_render()\n\nOr to build individual pages run:\n\nquarto::quarto_render(\"new_page.qmd\")\n\n\nCopy files from public_build/ to public/\n\n\n\ncp -r public_build/* public/\n\n\nCommit and push the changes to the gh-pages branch.\n\n\ncd public/\ngit status\ngit add --all\ngit status\ngit commit -m \"added new page\"\ngit push origin gh-pages\n\n\nCommit and push the changes to the main branch.\n\n\ncd ../\ngit status\ngit add --all\ngit status\ngit commit -m \"added new page\"\ngit push origin main"
  },
  {
    "objectID": "docs/ssu-workflows/otu/index.html",
    "href": "docs/ssu-workflows/otu/index.html",
    "title": "3. OTU Workflow",
    "section": "",
    "text": "Click here for page build libraries and setup information.knitr::opts_chunk$set(echo = TRUE, eval = FALSE)\nset.seed(919191)\nlibrary(dada2); packageVersion(\"dada2\")\nlibrary(ShortRead); packageVersion(\"ShortRead\")\nlibrary(ggplot2); packageVersion(\"ggplot2\")\nlibrary(Biostrings); packageVersion(\"Biostrings\")\npacman::p_load(tidyverse, gridExtra, grid, miaViz,\n               formatR, reactable, gdata, patchwork,\n               kableExtra, microeco, magrittr, \n               rprojroot,\n               tidySummarizedExperiment, scater,\n               install = FALSE, update = FALSE)\n\noptions(scipen = 999)\nknitr::opts_current$get(c(\n  \"cache\",\n  \"cache.path\",\n  \"cache.rebuild\",\n  \"dependson\",\n  \"autodep\"\n))\n#root &lt;- find_root(has_file(\"_quarto.yml\"))\n#source(file.path(root, \"assets\", \"functions.R\"))\nflowchart LR\n  A(make.file) --&gt; B(make.contigs)\n  B --&gt; C(screen.seqs&lt;br/&gt;unique.seqs)\n  C --&gt; E(align.seqs)\n  F(pcr.seqs) --&gt; E\n  E --&gt; I(screen.seqs&lt;br/&gt;filter.seqs&lt;br/&gt;unique.seqs)\n  I --&gt; J(pre.cluster)\nflowchart LR\ntitle[Remove Negative Controls]\nstyle title fill:#ffffff,stroke:#333,stroke-width:0px,mermaid-font-size:2.8rem\n\n  L(\"get.groups &lt;br/&gt; (NC Samples)\")\n  L --&gt; M(remove.seqs)\n  L --&gt; N(remove.groups)\n  M --&gt; O(chimera.vsearch)\n  N --&gt; O\n  O --&gt; P(classify.seqs)\n  P --&gt; Q(remove.lineage)\n  Q --&gt; R(cluster.split)\n  Q --&gt; S(dist.seqs)",
    "crumbs": [
      "16S rRNA Processing",
      "3. OTU Workflow"
    ]
  },
  {
    "objectID": "docs/ssu-workflows/otu/index.html#getting-started",
    "href": "docs/ssu-workflows/otu/index.html#getting-started",
    "title": "3. OTU Workflow",
    "section": "Getting Started",
    "text": "Getting Started\n\n\n\n\n\n\nExpand for the MOTHUR batchfile\n\n\n\n\n\n{{}}\n\n\n\n\n\n\n\n\nCode chunk coloring by language\n\nColor\nLanguage\n\n\n\nblue\nR\n\n\nvermillion\nbash\n\n\nbluish green\nshell\n\n\nreddish purple\nmothur\n\n\n\n\n\nWe make use of many different coding languages in these workflows. Code chucks are colored by language.\nset.dir(output=pipelineFiles/)\nMothur's directories:\noutputDir=pipelineFiles/\nmake.file(inputdir=$DATA, type=$TYPE, prefix=shrimp)\nSetting input directories to: \n    01_TRIMMED_DATA/\n\nOutput File Names: \nshrimp.files",
    "crumbs": [
      "16S rRNA Processing",
      "3. OTU Workflow"
    ]
  },
  {
    "objectID": "docs/ssu-workflows/otu/index.html#reducing-sequencing-pcr-errors",
    "href": "docs/ssu-workflows/otu/index.html#reducing-sequencing-pcr-errors",
    "title": "3. OTU Workflow",
    "section": "Reducing Sequencing & PCR Errors",
    "text": "Reducing Sequencing & PCR Errors\nmake.contigs(file=current, processors=$PROC)\nWe will get the following message if sample names contain dash (-) characters. Mothur will change this for us.\n[WARNING]: group Control-10 contains illegal characters in the name. \nGroup names should not include :, -, or / characters.  The ':' character \nis a special character used in trees. Using ':' will result in your tree \nbeing unreadable by tree reading software.  The '-' character is a special \ncharacter used by mothur to parse group names.  Using the '-' character \nwill prevent you from selecting groups. The '/' character will created \nunreadable filenames when mothur includes the group in an output filename.\n\n[NOTE] Updating Control-10 to Control_10 to avoid downstream issues.\n\n...\n\nTotal of all groups is 44710450\n\nIt took 1257 secs to process 44710450 sequences.\n\nOutput File Names: \nshrimp.trim.contigs.fasta\nshrimp.scrap.contigs.fasta\nshrimp.contigs_report\nshrimp.contigs.count_table\nsummary.seqs(fasta=current, count=current, processors=$PROC)\n\n\n\n\n\n\nExpand to see data set summary\n\n\n\n\n\n\n\n\nStart\nEnd\nNBases\nAmbigs\nPolymer\nNumSeqs\n\n\n\nMinimum\n1\n226\n226\n0\n2\n1\n\n\n2.5%-tile:\n1\n252\n252\n0\n3\n1117762\n\n\n25%-tile:\n1\n253\n253\n0\n4\n11177613\n\n\nMedian:\n1\n253\n253\n0\n4\n22355226\n\n\n75%-tile:\n1\n253\n253\n0\n5\n33532838\n\n\n97.5%-tile:\n1\n254\n254\n6\n6\n43592689\n\n\nMaximum:\n1\n480\n480\n95\n233\n44710450\n\n\nMean:\n1\n254\n254\n0\n4\n\n\n\n\n# of unique seqs:   44710450\ntotal # of seqs:    44710450\n\nIt took 782 secs to summarize 44710450 sequences.\n\nOutput File Names:\nshrimp.trim.contigs.summary\n\n\n\ncount.groups(count=current)\nSize of smallest group: 58.\n\nTotal seqs: 44710450.\nscreen.seqs(fasta=current, count=current, maxambig=0, minlength=252, maxlength=254, maxhomop=6, processors=$PROC)\nUsing 30 processors.\n\nIt took 107 secs to screen 44710450 sequences, removed 8308318.\n\nOutput File Names:\nshrimp.contigs.pick.count_table\n\n/******************************************/\n\nOutput File Names:\nshrimp.trim.contigs.good.fasta\nshrimp.trim.contigs.bad.accnos\nshrimp.contigs.good.count_table\n\nIt took 557 secs to screen 44710450 sequences.\nsummary.seqs(fasta=current, count=current, processors=$PROC)\n\n\n\n\n\n\nExpand to see data set summary\n\n\n\n\n\n\n\n\nStart\nEnd\nNBases\nAmbigs\nPolymer\nNumSeqs\n\n\n\nMinimum\n1\n252\n252\n0\n3\n1\n\n\n2.5%-tile:\n1\n252\n252\n0\n3\n910054\n\n\n25%-tile:\n1\n253\n253\n0\n4\n9100534\n\n\nMedian:\n1\n253\n253\n0\n4\n18201067\n\n\n75%-tile:\n1\n253\n253\n0\n5\n27301600\n\n\n97.5%-tile:\n1\n254\n254\n6\n6\n35492079\n\n\nMaximum:\n1\n254\n254\n0\n6\n36402132\n\n\nMean:\n1\n252\n252\n0\n4\n\n\n\n\n# of unique seqs:   36402132\ntotal # of seqs:    36402132\n\nIt took 610 secs to summarize 36402132 sequences.\n\nOutput File Names:\nshrimp.trim.contigs.good.summary\n\n\n\ncount.groups(count=current)\nSize of smallest group: 57.\n\nTotal seqs: 36402132.\nOutput File Names: \nshrimp.contigs.good.count.summary",
    "crumbs": [
      "16S rRNA Processing",
      "3. OTU Workflow"
    ]
  },
  {
    "objectID": "docs/ssu-workflows/otu/index.html#processing-improved-reads",
    "href": "docs/ssu-workflows/otu/index.html#processing-improved-reads",
    "title": "3. OTU Workflow",
    "section": "Processing Improved Reads",
    "text": "Processing Improved Reads\nunique.seqs(fasta=current, count=current)\n36402132    4224192\n\nOutput File Names: \nshrimp.trim.contigs.good.unique.fasta\nshrimp.trim.contigs.good.count_table\nsummary.seqs(count=current, processors=$PROC)\n\n\n\n\n\n\nExpand to see data set summary\n\n\n\n\n\n\n\n\nStart\nEnd\nNBases\nAmbigs\nPolymer\nNumSeqs\n\n\n\nMinimum\n1\n252\n252\n0\n3\n1\n\n\n2.5%-tile:\n1\n252\n252\n0\n3\n910054\n\n\n25%-tile:\n1\n253\n253\n0\n4\n9100534\n\n\nMedian:\n1\n253\n253\n0\n4\n18201067\n\n\n75%-tile:\n1\n253\n253\n0\n5\n27301600\n\n\n97.5%-tile:\n1\n254\n254\n6\n6\n35492079\n\n\nMaximum:\n1\n254\n254\n0\n6\n36402132\n\n\nMean:\n1\n252\n252\n0\n4\n\n\n\n\n# of unique seqs:   4224192\ntotal # of seqs:    36402132\n\nIt took 78 secs to summarize 36402132 sequences.\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.summary",
    "crumbs": [
      "16S rRNA Processing",
      "3. OTU Workflow"
    ]
  },
  {
    "objectID": "docs/ssu-workflows/otu/index.html#aligning-reads",
    "href": "docs/ssu-workflows/otu/index.html#aligning-reads",
    "title": "3. OTU Workflow",
    "section": "Aligning Reads",
    "text": "Aligning Reads\nSince we are using the silva.nr_v132.align to align sequences, we can check where our reads start and end with the ARB-SILVA web aligner. After uploading a few sequences we find they start at postion 13862 and end at position 23445. Neat. We will pad these numbers to make sure we do not miss anything.\npcr.seqs(fasta=$REF_LOC/silva.nr_v132.align, start=11895, end=25318, keepdots=F, processors=$PROC)\nUsing 30 processors.\n[NOTE]: no sequences were bad, removing silva.nr_v132.bad.accnos\n\nIt took 10 secs to screen 213119 sequences.\n\nOutput File Names: \nsilva.nr_v132.pcr.align\nrename.file(input=$REF_LOC/silva.nr_v132.pcr.align, new=$REF_LOC/$ALIGNREF)\nsummary.seqs(fasta=$REF_LOC/$ALIGNREF, processors=$PROC)\n\n\n\n\n\n\nExpand to see data set summary\n\n\n\n\n\n\n\n\nStart\nEnd\nNBases\nAmbigs\nPolymer\nNumSeqs\n\n\n\nMinimum\n1\n9876\n218\n0\n3\n1\n\n\n2.5%-tile:\n1\n13424\n291\n0\n4\n5328\n\n\n25%-tile:\n1\n13424\n292\n0\n4\n53280\n\n\nMedian:\n1\n13424\n292\n0\n5\n106560\n\n\n75%-tile:\n1\n13424\n292\n0\n5\n159840\n\n\n97.5%-tile:\n1\n13424\n458\n1\n6\n207792\n\n\nMaximum:\n4225\n13424\n1520\n5\n16\n213119\n\n\nMean:\n1\n13423\n307\n0\n4\n\n\n\n\n# of Seqs:  213119\n\nIt took 5 secs to summarize 213119 sequences.\n\nOutput File Names:\nsilva.v4.summary\n\n\n\nalign.seqs(fasta=current, reference=$REF_LOC/$ALIGNREF, processors=$PROC)\nUsing 30 processors.\n\nReading in the silva.v4.fasta template sequences... DONE.\nIt took 34 to read  213119 sequences.\n\nAligning sequences from shrimp.trim.contigs.good.unique.fasta ...\nIt took 780 secs to align 4224192 sequences.\n\n[WARNING]: 217 of your sequences generated alignments that \neliminated too many bases, a list is provided in \nshrimp.trim.contigs.good.unique.flip.accnos.\n[NOTE]: 136 of your sequences were reversed to produce a better alignment.\n\nIt took 780 seconds to align 4224192 sequences.\n\nOutput File Names: \nshrimp.trim.contigs.good.unique.align\nshrimp.trim.contigs.good.unique.align_report\nshrimp.trim.contigs.good.unique.flip.accnos\nsummary.seqs(fasta=current, count=current, processors=$PROC)\n\n\n\n\n\n\nExpand to see data set summary\n\n\n\n\n\n\n\n\nStart\nEnd\nNBases\nAmbigs\nPolymer\nNumSeqs\n\n\n\nMinimum\n1\n1235\n2\n0\n1\n1\n\n\n2.5%-tile:\n1968\n11550\n252\n0\n3\n910054\n\n\n25%-tile:\n1968\n11550\n253\n0\n4\n9100534\n\n\nMedian:\n1968\n11550\n253\n0\n4\n18201067\n\n\n75%-tile:\n1968\n11550\n253\n0\n5\n27301600\n\n\n97.5%-tile:\n1968\n11550\n253\n0\n6\n35492079\n\n\nMaximum:\n13422\n13424\n254\n0\n6\n36402132\n\n\nMean:\n1968\n11549\n252\n0\n4\n\n\n\n\n# of unique seqs:   4224192\ntotal # of seqs:    36402132\n\nIt took 141 secs to summarize 36402132 sequences.\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.summary\n\n\n\nscreen.seqs(fasta=current, count=current, start=1968, end=11550, processors=$PROC)\nUsing 30 processors.\n\nIt took 66 secs to screen 4224192 sequences, removed 7623.\n\n/******************************************/\nRunning command: \nremove.seqs(accnos=shrimp.trim.contigs.good.unique.bad.accnos.temp, \ncount=shrimp.trim.contigs.good.count_table)\nRemoved 25154 sequences from shrimp.trim.contigs.good.count_table.\n\nOutput File Names:\nshrimp.trim.contigs.good.pick.count_table\n\n/******************************************/\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.good.align\nshrimp.trim.contigs.good.unique.bad.accnos\nshrimp.trim.contigs.good.good.count_table\nsummary.seqs(fasta=current, count=current, processors=$PROC)\n\n\n\n\n\n\nExpand to see data set summary\n\n\n\n\n\n\n\n\nStart\nEnd\nNBases\nAmbigs\nPolymer\nNumSeqs\n\n\n\nMinimum\n1\n11550\n240\n0\n3\n1\n\n\n2.5%-tile:\n1968\n11550\n252\n0\n3\n909425\n\n\n25%-tile:\n1968\n11550\n253\n0\n4\n9094245\n\n\nMedian:\n1968\n11550\n253\n0\n4\n18188490\n\n\n75%-tile:\n1968\n11550\n253\n0\n5\n27282734\n\n\n97.5%-tile:\n1968\n11550\n253\n0\n6\n35467554\n\n\nMaximum:\n1968\n13424\n254\n0\n6\n36376978\n\n\nMean:\n1967\n11550\n252\n0\n4\n\n\n\n\n# of unique seqs:   4216569\ntotal # of seqs:    36376978\n\nIt took 152 secs to summarize 36376978 sequences.\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.good.summary\n\n\n\ncount.groups(count=current)\nSize of smallest group: 57.\n\nTotal seqs: 36376978.\n\nOutput File Names: \nshrimp.trim.contigs.good.good.count.summary\nfilter.seqs(fasta=current, vertical=T, trump=., processors=$PROC)\nUsing 30 processors.\nCreating Filter...\nIt took 31 secs to create filter for 4216569 sequences.\n\n\nRunning Filter...\nIt took 26 secs to filter 4216569 sequences.\n\n\n\nLength of filtered alignment: 736\nNumber of columns removed: 12688\nLength of the original alignment: 13424\nNumber of sequences used to construct filter: 4216569\n\nOutput File Names: \nshrimp.filter\nshrimp.trim.contigs.good.unique.good.filter.fasta\nunique.seqs(fasta=current, count=current)\n4216569 4192289\n\nOutput File Names: \nshrimp.trim.contigs.good.unique.good.filter.unique.fasta\nshrimp.trim.contigs.good.unique.good.filter.count_table\ncount.groups(count=current)\nSize of smallest group: 57.\n\nTotal seqs: 36376978.\n\nOutput File Names: \nshrimp.trim.contigs.good.unique.good.filter.count.summary\nsummary.seqs(fasta=current, count=current, processors=$PROC)\n\n\n\n\n\n\nExpand to see data set summary\n\n\n\n\n\n\n\n\nStart\nEnd\nNBases\nAmbigs\nPolymer\nNumSeqs\n\n\n\nMinimum\n1\n735\n208\n0\n3\n1\n\n\n2.5%-tile:\n1\n736\n252\n0\n3\n909425\n\n\n25%-tile:\n1\n736\n253\n0\n4\n9094245\n\n\nMedian:\n1\n736\n253\n0\n4\n18188490\n\n\n75%-tile:\n1\n736\n253\n0\n5\n27282734\n\n\n97.5%-tile:\n1\n736\n253\n0\n6\n35467554\n\n\nMaximum:\n4\n736\n254\n0\n6\n36376978\n\n\nMean:\n1\n735\n252\n0\n4\n\n\n\n\n# of unique seqs:   4192289\ntotal # of seqs:    36376978\n\nIt took 88 secs to summarize 36376978 sequences.\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.good.filter.unique.summary",
    "crumbs": [
      "16S rRNA Processing",
      "3. OTU Workflow"
    ]
  },
  {
    "objectID": "docs/ssu-workflows/otu/index.html#precluster",
    "href": "docs/ssu-workflows/otu/index.html#precluster",
    "title": "3. OTU Workflow",
    "section": "Precluster",
    "text": "Precluster\npre.cluster(fasta=current, count=current, diffs=2, processors=$PROC)\n\n\n\n\n\n\nExpand to see partial output of pre.cluster\n\n\n\n\n\nUsing 30 processors.\n\n/******************************************/\nSplitting by sample: \n\nUsing 30 processors.\n\nSelecting sequences for groups Control_1-Control_10-Control_11-Control_12-\nControl_13-Control_14-Control_15-Control_16-Control_17-Control_18\n\n\nSelecting sequences for groups EP_A_PANA_EG_9332-EP_A_PANA_EG_9338-\nEP_A_PANA_EG_9339-EP_A_PANA_GL_7322-EP_A_PANA_GL_7326-\nEP_A_PANA_GL_7327-EP_A_PANA_GL_7329-EP_A_PANA_GL_7330-\nEP_A_PANA_GL_7331-EP_A_PANA_GL_7332\n....\n\nSelected 1054 sequences from Control_1.\nSelected 695 sequences from Control_10.\nSelected 902 sequences from Control_11.\nSelected 216 sequences from Control_12.\nSelected 212 sequences from Control_13.\nSelected 75 sequences from Control_14.\nSelected 488 sequences from Control_15.\nSelected 246 sequences from Control_16.\nSelected 47 sequences from Control_17.\nSelected 171 sequences from Control_18.\n\nSelecting sequences for groups Control_19-Control_2-\nControl_20-Control_21-Control_22-Control_23-Control_24-\nControl_25-Control_26-Control_27\n\nSelected 4720 sequences from EP_A_PANA_EG_9332.\nSelected 4278 sequences from EP_A_PANA_EG_9338.\nSelected 689 sequences from EP_A_PANA_EG_9339.\nSelected 3148 sequences from EP_A_PANA_GL_7322.\nSelected 3989 sequences from EP_A_PANA_GL_7326.\nSelected 3552 sequences from EP_A_PANA_GL_7327.\nSelected 3621 sequences from EP_A_PANA_GL_7329.\nSelected 2322 sequences from EP_A_PANA_GL_7330.\nSelected 6021 sequences from EP_A_PANA_GL_7331.\nSelected 4267 sequences from EP_A_PANA_GL_7332.\n\nSelecting sequences for groups EP_A_PANA_GL_8353-\nEP_A_PANA_GL_8792-EP_A_PANA_GL_8948-EP_A_PANA_GL_8951-\nEP_A_PANA_GL_8952-EP_A_PANA_GL_9021-EP_A_PANA_GL_9022-\nEP_A_PANA_GL_9131-EP_A_PANA_GL_9264-EP_A_PANA_GL_9332\n\n\n\nsummary.seqs(fasta=current, count=current, processors=$PROC)\n\n\n\n\n\n\nExpand to see data set summary\n\n\n\n\n\n\n\n\nStart\nEnd\nNBases\nAmbigs\nPolymer\nNumSeqs\n\n\n\nMinimum\n1\n735\n208\n0\n3\n1\n\n\n2.5%-tile:\n1\n736\n252\n0\n3\n909425\n\n\n25%-tile:\n1\n736\n253\n0\n4\n9094245\n\n\nMedian:\n1\n736\n253\n0\n4\n18188490\n\n\n75%-tile:\n1\n736\n253\n0\n5\n27282734\n\n\n97.5%-tile:\n1\n736\n253\n0\n6\n35467554\n\n\nMaximum:\n4\n736\n254\n0\n6\n36376978\n\n\nMean:\n1\n735\n252\n0\n4\n\n\n\n\n# of unique seqs:   1649819\ntotal # of seqs:    36376978\n\nIt took 35 secs to summarize 36376978 sequences.\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.summary\n\n\n\ncount.groups(count=current)\nSize of smallest group: 57.\n\nTotal seqs: 36376978.\n\nOutput File Names: \nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.count.summary",
    "crumbs": [
      "16S rRNA Processing",
      "3. OTU Workflow"
    ]
  },
  {
    "objectID": "docs/ssu-workflows/otu/index.html#remove-negative-controls",
    "href": "docs/ssu-workflows/otu/index.html#remove-negative-controls",
    "title": "3. OTU Workflow",
    "section": "Remove Negative Controls",
    "text": "Remove Negative Controls\nNow we need to remove the NC samples and reads found in those sample. We first identified all reads that were present in at least one NC sample represented by at least 1 read. We did this by subsetting the NC samples from dataset.\nget.groups(fasta=current, count=current, groups=Control_49-Control_23-Control_17-Control_24-Control_14-Control_44-Control_20-Control_33-Control_41-Control_29-Control_50-Control_22-Control_19-Control_18-Control_48-Control_13-Control_21-Control_16-Control_30-Control_5-Control_42-Control_25-Control_51-Control_40-Control_15-Control_36-Control_47-Control_27-Control_32-Control_8-Control_3-Control_4-Control_6-Control_45-Control_26-Control_46-Control_53-Control_7-Control_12-Control_10-Control_9-Control_35-Control_54-Control_2-Control_43-Control_1-Control_11-Control_52-Control_38-Control_34-Control_56-Control_37-Control_28-Control_57-Control_31-Control_39-Control_59-Control_55-Control_60-Control_58)\nSelected 193014 sequences from your count file.\nSelected 4133 sequences from your fasta file.\n\nOutput File names:\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.count_table\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.fasta\nrename.file(input=current, new=neg_control.fasta)\nrename.file(input=current, new=neg_control.count_table)\nsummary.seqs(fasta=neg_control.fasta, count=neg_control.count_table, processors=$PROC)\n\n\n\n\n\n\nExpand to see negative control summary\n\n\n\n\n\n\n\n\nStart\nEnd\nNBases\nAmbigs\nPolymer\nNumSeqs\n\n\n\nMinimum\n1\n735\n251\n0\n3\n1\n\n\n2.5%-tile:\n1\n736\n252\n0\n4\n4826\n\n\n25%-tile:\n1\n736\n253\n0\n4\n48254\n\n\nMedian:\n1\n736\n253\n0\n5\n96508\n\n\n75%-tile:\n1\n736\n253\n0\n5\n144761\n\n\n97.5%-tile:\n1\n736\n254\n0\n6\n188189\n\n\nMaximum:\n4\n736\n254\n0\n6\n193014\n\n\nMean:\n1\n735\n253\n0\n4\n\n\n\n\n# of unique seqs:   4133\ntotal # of seqs:    193014\n\nIt took 0 secs to summarize 193014 sequences.\n\nOutput File Names:\nneg_control.summary\n\n\n\nlist.seqs(count=neg_control.count_table)\nOutput File Names: \nneg_control.accnos\nThe next step is to use the *.accnos file from the previous step to remove any reads found in negative control (NC) samples. This seems reasonable enough except mothur will remove any read found in a NC sample. For example, let’s say we have two reads:\nread01 is found in most NC samples and not found in any remaining samples.read02 on the other hand is represented by one read in a single NC sample but very abundant in remaining samples.\nIt makes a lot of sense to remove read01 but not so for read02. So we need to make a custom *.accnos that only contains reads that are abundant in NC samples. To do this we will do a little data wrangling in R. For this we need two *.count_table files–one from the precluster step and the other from the step above. We got the idea on how best to do this from the mothur forum.\nEssentially, for each repseq, the code below calculates:\n\nThe total number of NC samples containing at least 1 read.\n\nThe total number of reads in NC samples.\n\nThe total number of non-NC samples containing at least 1 read.\n\nThe total number of reads in non-NC samples.\n\nThe percent of reads in the NC samples and the percent of NC samples containing reads.\n\nThe first command parses out the necessary data from the .count_table files.\n\ntmp_nc_reads &lt;- read_tsv(\n  \"files/tables/neg_control.count_table\",\n  col_names = TRUE,\n  col_types = NULL,\n  skip = 2,\n  col_select = c(\"Representative_Sequence\", \"total\")\n)\ntmp_non_nc_reads &lt;- read_tsv(\n  \"files/tables/shrimp.trim.contigs.good.unique.good.filter.unique.precluster.count_table\",\n  col_names = TRUE,\n  col_types = NULL,\n  skip = 2,\n  col_select = c(\"Representative_Sequence\", \"total\"),\n)\ntmp_reads &lt;- dplyr::left_join(tmp_nc_reads, tmp_non_nc_reads,\n                              by = \"Representative_Sequence\")\ntmp_reads &lt;- tmp_reads %&gt;% dplyr::rename(c(\n                            \"total_reads_NC\" = \"total.x\", \n                            \"total_reads_samps\" = \"total.y\")\n                            ) \n\nAnd here is what the new dataframe looks like. Three columns where the first is the repseq name, the second the total number of reads in NC samples, and the third the total number of reads in the entire dataset.\n\n\n# A tibble: 6 × 3\n  Representative_Sequence                      total_reads_NC total_reads_samps\n  &lt;chr&gt;                                                 &lt;dbl&gt;             &lt;dbl&gt;\n1 M06508_19_000000000-CR8CR_1_1109_12464_17636            740             12199\n2 M06508_8_000000000-JTFYW_1_2110_2674_14071                1                 1\n3 M06508_16_000000000-CMN9T_1_1113_26829_19406              1                 1\n4 M06508_16_000000000-CMN9T_1_2107_24314_16865              1                 1\n5 M06508_19_000000000-CR8CR_1_2113_26270_16687              1                 1\n6 M06508_8_000000000-JTFYW_1_1108_20403_19558               1                 1\n\n\nWe identified 4,133 reads that were potential contaminants.\nNow we add in a column that calculates the percent of reads in the NC samples.\n\ntmp_nc_check &lt;- tmp_reads %&gt;%\n  dplyr::mutate(perc_in_neg = 100*(\n    total_reads_NC / (total_reads_NC + total_reads_samps)),\n                .after = \"total_reads_samps\")\ntmp_nc_check$perc_in_neg &lt;- round(tmp_nc_check$perc_in_neg, digits = 6)\n\nNow we use the count.seqs command in mothur to generate a file describing the distribution of NC reads across all samples. This information will tell us how many samples contain these reads.\ncount.seqs(count=neg_control.count_table, compress=f)\ncount.seqs(count=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.count_table, compress=f)\n\ntmp_nc_dist &lt;- read_tsv(\"files/tables/neg_control.full.count_table\")\n\ntmp_all_dist &lt;- read_tsv(\"files/tables/shrimp.trim.contigs.good.unique.good.filter.unique.precluster.full.count_table\")\ntmp_all_dist &lt;- tmp_all_dist %&gt;% select(-starts_with(\"Control_\"))\n\ntmp_nc_dist$total &lt;- NULL\ntmp_all_dist$total &lt;- NULL\n\ntmp_nc_read_dist &lt;- dplyr::left_join(\n                    tmp_nc_dist, \n                    tmp_all_dist, \n                    by = \"Representative_Sequence\")\n\nAfter a little wrangling, the dataframe looks like this:\n\n\n# A tibble: 6 × 5\n  Representative_Sequence             Control_1 Control_10 Control_11 Control_12\n  &lt;chr&gt;                                   &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 M06508_19_000000000-CR8CR_1_1109_1…        21          0          0          0\n2 M06508_8_000000000-JTFYW_1_2110_26…         0          0          0          0\n3 M06508_16_000000000-CMN9T_1_1113_2…         0          0          0          0\n4 M06508_16_000000000-CMN9T_1_2107_2…         0          0          0          0\n5 M06508_19_000000000-CR8CR_1_2113_2…         0          0          0          0\n6 M06508_8_000000000-JTFYW_1_1108_20…         0          0          0          0\n\n\nAnd then we calculate row sums to get the number of NC and non-NC samples containing these reads.\n\ntmp_1 &lt;- data.frame(rowSums(tmp_nc_read_dist != 0))\ntmp_2 &lt;- dplyr::select(tmp_nc_read_dist, contains(\"Control\"))\ntmp_2$num_samp_nc &lt;- rowSums(tmp_2 != 0)\ntmp_2 &lt;- dplyr::select(tmp_2, -contains(\"Control\"))\ntmp_3 &lt;- dplyr::select(tmp_nc_read_dist, -contains(\"Control\"))\ntmp_3$num_samp_no_nc &lt;- rowSums(tmp_3 != 0)\ntmp_3 &lt;- dplyr::select(tmp_3, contains(\"num_samp_no_nc\"))\n\nAnd again calculate the percent of NC samples containing these reads.\n\ntmp_nc_check &lt;- cbind(tmp_nc_check, tmp_1, tmp_2, tmp_3)\ntmp_nc_check &lt;- tmp_nc_check %&gt;% dplyr::rename(\"total_samples\" = 5)  \ncolnames(tmp_nc_check)\ntmp_nc_check &lt;- tmp_nc_check %&gt;%\n  dplyr::mutate(perc_in_neg_samp = \n                  100*( num_samp_nc / (num_samp_nc + num_samp_no_nc)),\n                  .after = \"num_samp_no_nc\")\n\n\n\nSummary of reads detected in Negative Control (NC) samples.\n\n\n\n\n Download summary of reads detected in Negative Control (NC) samples \nNow we remove any repseqs where:\n\nThe number of reads found in NC samples accounted for more than 10% of total reads OR\nThe percent of NC samples containing the ASV was greater than 10% of total samples.\n\n\nnc_remove &lt;- tmp_nc_check %&gt;% \n  filter(perc_in_neg &gt; 10 | perc_in_neg_samp &gt; 10)\ntmp_rem_otu &lt;- nc_remove$Representative_Sequence %&gt;% \n  unlist(strsplit(., split = \", \")) \n\n\n\n\n\n\n\n\n\n\n\nTotal OTUs\nNC reads\nnon NC reads\n% NC reads\n\n\n\nRemoved\n3853\n156847\n370213\n29.759\n\n\nRetained\n280\n36167\n14783395\n0.244\n\n\n\nWe identified a total of 4133 OTUs that were present in at least 1 NC sample by at least 1 read. We removed any OTUs where more than 10% of total reads were found in NC samples OR any OTU found in more than 10% of NC samples. Based on these criteria we removed 3853 OTUs from the data set, which represented 156847 total reads in NC samples and 370213 total reads in non-NC samples. Of the total reads removed 29.759% came from NC samples. Of all OTUs identified in NC samples,280 were retained because they fell below the threshhold criteria. These OTUs accounted for 36167 reads in NC samples and 14783395 reads in non-NC samples. NC samples accounted for 0.244% of these reads.\nOK, now we can create a new neg_control.accnos containing only repseqs abundant in NC samples.\n\nwrite_delim(\n  data.frame(nc_remove$Representative_Sequence), \n  \"files/tables/neg_control_subset.accnos\", \n  col_names = FALSE)\n\nAnd then use this file in conjunction with the mothur command remove.seqs.\nremove.seqs(accnos=neg_control_subset.accnos, fasta=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.fasta, count=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.count_table)\nRemoved 3831 sequences from shrimp.trim.contigs.good.unique.good.filter.unique.precluster.fasta.\nRemoved 332770 sequences from shrimp.trim.contigs.good.unique.good.filter.unique.precluster.count_table.\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.fasta\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.count_table\ncount.groups(count=current)\nSize of smallest group: 1.\n\nTotal seqs: 36044208.\n\nOutput File Names: \nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.count.summary\nBefore we remove the NC samples we need to check whether some NC samples were already removed. When mothur runs the remove.seqs command it will automatically remove any samples where the read count has fallen to zero. If mothur did remove samples and we try to remove all NC samples, we will get an error. To check we can compare the count.summary files before and after the previous remove.seqs command.\n\ntmp_before &lt;- read_tsv(\n  \"files/tables/shrimp.trim.contigs.good.unique.good.filter.unique.precluster.count.summary\",\n  col_names = FALSE,\n  col_select = 1\n)\ntmp_after &lt;- read_tsv(\n  \"files/tables/shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.count.summary\",\n  col_names = FALSE,\n  col_select = 1\n)\ntmp_nc_lost &lt;- anti_join(tmp_before, tmp_after)\ntmp_nc_lost$X1\n\nThese are the samples that were already removed when remove.seqs was run above. We need to remove these from our list of NC samples.\n[1] \"Control_15\" \"Control_18\" \"Control_21\" \"Control_29\" \"Control_5\" \n\nnc_to_remove &lt;- semi_join(tmp_before, tmp_after)\nnc_to_remove &lt;- nc_to_remove %&gt;%\n  dplyr::filter(\n    stringr::str_starts(X1, \"Control\")\n    )\nnc_to_remove &lt;- paste0(nc_to_remove$X1, collapse=\"-\")\n\n\nnc_to_remove\n\n[1] \"Control_1-Control_10-Control_11-Control_12-Control_13-Control_14-Control_16-Control_17-Control_19-Control_2-Control_20-Control_22-Control_23-Control_24-Control_25-Control_26-Control_27-Control_28-Control_3-Control_30-Control_31-Control_32-Control_33-Control_34-Control_35-Control_36-Control_37-Control_38-Control_39-Control_4-Control_40-Control_41-Control_42-Control_43-Control_44-Control_45-Control_46-Control_47-Control_48-Control_49-Control_50-Control_51-Control_52-Control_53-Control_54-Control_55-Control_56-Control_57-Control_58-Control_59-Control_6-Control_60-Control_7-Control_8-Control_9\"\n\n\nremove.groups(count=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.count_table, fasta=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.fasta, groups=Control_1-Control_10-Control_11-Control_12-Control_13-Control_14-Control_16-Control_17-Control_19-Control_2-Control_20-Control_22-Control_23-Control_24-Control_25-Control_26-Control_27-Control_28-Control_3-Control_30-Control_31-Control_32-Control_33-Control_34-Control_35-Control_36-Control_37-Control_38-Control_39-Control_4-Control_40-Control_41-Control_42-Control_43-Control_44-Control_45-Control_46-Control_47-Control_48-Control_49-Control_50-Control_51-Control_52-Control_53-Control_54-Control_55-Control_56-Control_57-Control_58-Control_59-Control_6-Control_60-Control_7-Control_8-Control_9)\nRemoved 35932 sequences from your count file.\nRemoved 0 sequences from your fasta file.\n\nOutput File names: \nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.count_table\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.fasta\nsummary.seqs(fasta=current, count=current, processors=$PROC)\n\n\n\n\n\n\nExpand to see data set summary\n\n\n\n\n\n\n\n\nStart\nEnd\nNBases\nAmbigs\nPolymer\nNumSeqs\n\n\n\nMinimum\n1\n735\n208\n0\n3\n1\n\n\n2.5%-tile:\n1\n736\n252\n0\n3\n900207\n\n\n25%-tile:\n1\n736\n253\n0\n4\n9002070\n\n\nMedian:\n1\n736\n253\n0\n4\n18004139\n\n\n75%-tile:\n1\n736\n253\n0\n5\n27006208\n\n\n97.5%-tile:\n1\n736\n253\n0\n6\n35108070\n\n\nMaximum:\n4\n736\n254\n0\n6\n36008276\n\n\nMean:\n1\n735\n252\n0\n4\n\n\n\n\n# of unique seqs:   1645988\ntotal # of seqs:    36008276\n\nIt took 28 secs to summarize 36008276 sequences.\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.summary\n\n\n\ncount.groups(count=current)\nSize of smallest group: 14.\n\nTotal seqs: 36008276.\n\nOutput File Names: \nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.count.summary",
    "crumbs": [
      "16S rRNA Processing",
      "3. OTU Workflow"
    ]
  },
  {
    "objectID": "docs/ssu-workflows/otu/index.html#remove-chimeras",
    "href": "docs/ssu-workflows/otu/index.html#remove-chimeras",
    "title": "3. OTU Workflow",
    "section": "Remove Chimeras",
    "text": "Remove Chimeras\nchimera.vsearch(fasta=current, count=current, dereplicate=t, processors=$PROC)\nsummary.seqs(fasta=current, count=current, processors=$PROC)\ncount.groups(count=current)\nUsing vsearch version v2.15.2.\nChecking sequences from shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.fasta ...\n\n/******************************************/\nSplitting by sample: \n\n...\n\nRemoving chimeras from your input files:\n/******************************************/\nRunning command: \nremove.seqs(fasta=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.fasta,\naccnos=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.accnos)\nRemoved 622662 sequences from shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.fasta.\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.pick.fasta\n\n/******************************************/\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.count_table\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.chimeras\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.accnos\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.fasta\nsummary.seqs(fasta=current, count=current, processors=30)\n\n\n\n\n\n\nExpand to see data set summary\n\n\n\n\n\n\n\n\nStart\nEnd\nNBases\nAmbigs\nPolymer\nNumSeqs\n\n\n\nMinimum\n1\n735\n208\n0\n3\n1\n\n\n2.5%-tile:\n1\n736\n252\n0\n3\n865457\n\n\n25%-tile:\n1\n736\n253\n0\n4\n8654567\n\n\nMedian:\n1\n736\n253\n0\n4\n17309134\n\n\n75%-tile:\n1\n736\n253\n0\n5\n25963700\n\n\n97.5%-tile:\n1\n736\n253\n0\n6\n33752810\n\n\nMaximum:\n4\n736\n254\n0\n6\n34618266\n\n\nMean:\n1\n735\n252\n0\n4\n\n\n\n\n# of unique seqs:   1023326\ntotal # of seqs:    34618266\n\nIt took 22 secs to summarize 34618266 sequences.\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.summary\n\n\n\ncount.groups(count=current)\nSize of smallest group: 14.\n\nTotal seqs: 34618266.\n\nOutput File Names: \nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.count.summary",
    "crumbs": [
      "16S rRNA Processing",
      "3. OTU Workflow"
    ]
  },
  {
    "objectID": "docs/ssu-workflows/otu/index.html#assign-taxonomy",
    "href": "docs/ssu-workflows/otu/index.html#assign-taxonomy",
    "title": "3. OTU Workflow",
    "section": "Assign Taxonomy",
    "text": "Assign Taxonomy\nThe classify.seqs command requires properly formatted reference and taxonomy databases. For taxonomic assignment, we are using the GSR database (Molano, Vega-Abellaneda, and Manichanh 2024). The developers of mothur maintain formatted versions of popular databases, however the GSR-DB has not been formatted by the developers yet.\n\n\n\n\n\n\nNote\n\n\n\nYou can download an appropriate version of the GSR database here.\n\n\nTo create a mothur formatted version GSR-DB1, we perform the following steps.\nDownload a data base\nHere we are using the GSR V4 database.\n\nwget https://manichanh.vhir.org/gsrdb/GSR-DB_V4_cluster-1.tar.gz\ntar -xvzf GSR-DB_V4_cluster-1.tar.gz\n\nFirst (in the command line) we remove first line of the taxonomy file.\n\ncp GSR-DB_V4_cluster-1_taxa.txt tmp0.txt\nsed '1d' tmp0.txt &gt; tmp1.txt\n\nNext, delete species and remove leading [a-z]__ from taxa names\n\nsed -E 's/s__.*//g' tmp1.txt &gt; tmp2.txt\nsed -E 's/[a-zA-Z]__//g' tmp2.txt &gt; gsrdb.tax\ncp GSR-DB_V4_cluster-1_seqs.fasta gsrdb.fasta\n\nclassify.seqs(fasta=current, count=current, reference=$REF_LOC/$TAXREF_FASTA, taxonomy=$REF_LOC/$TAXREF_TAX, processors=$PROC)\nUsing 30 processors.\nGenerating search database...    DONE.\nIt took 2 seconds generate search database.\n\nReading in the reference_dbs/gsrdb.txt taxonomy...  DONE.\nCalculating template taxonomy tree...     DONE.\nCalculating template probabilities...     DONE.\nIt took 6 seconds get probabilities.\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.fasta ...\n[WARNING]: M06508_9_000000000-JTBW3_1_1102_26159_16839 could not be classified. \nYou can use the remove.lineage command with taxon=unknown; \nto remove such sequences.\n...\n\nIt took 348 secs to classify 1023326 sequences.\n\nIt took 503 secs to create the summary file for 1023326 sequences.\n\nOutput File Names: \nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.gsrdb.wang.taxonomy\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.gsrdb.wang.tax.summary",
    "crumbs": [
      "16S rRNA Processing",
      "3. OTU Workflow"
    ]
  },
  {
    "objectID": "docs/ssu-workflows/otu/index.html#remove-contaminants",
    "href": "docs/ssu-workflows/otu/index.html#remove-contaminants",
    "title": "3. OTU Workflow",
    "section": "Remove Contaminants",
    "text": "Remove Contaminants\nremove.lineage(fasta=current, count=current, taxonomy=current, taxon=$CONTAMINENTS)\nRunning command: \nremove.seqs(accnos=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.gsrdb.wang.accnos, \ncount=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.count_table, \nfasta=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.fasta)\n\nRemoved 560 sequences from shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.fasta.\nRemoved 6712 sequences from shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.count_table.\n\n/******************************************/\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.gsrdb.wang.pick.taxonomy\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.gsrdb.wang.accnos\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.pick.count_table\nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.pick.fasta\nsummary.tax(taxonomy=current, count=current)\nUsing shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.pick.count_table \nas input file for the count parameter.\nUsing shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.gsrdb.wang.pick.taxonomy \nas input file for the taxonomy parameter.\n\nIt took 489 secs to create the summary file for 34611554 sequences.\n\nOutput File Names: \nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.gsrdb.wang.pick.tax.summary\ncount.groups(count=current)\nSize of smallest group: 14.\n\nTotal seqs: 34611554.\n\nOutput File Names: \nshrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.pick.count.summary",
    "crumbs": [
      "16S rRNA Processing",
      "3. OTU Workflow"
    ]
  },
  {
    "objectID": "docs/ssu-workflows/otu/index.html#track-reads-through-workflow",
    "href": "docs/ssu-workflows/otu/index.html#track-reads-through-workflow",
    "title": "3. OTU Workflow",
    "section": "Track Reads through Workflow",
    "text": "Track Reads through Workflow\nAt this point we can look at the number of reads that made it through each step of the workflow for every sample.\n\nread_change &lt;- read_tsv(\n  \"include/tables/all_sample_otu_read_changes.txt\",\n  col_names = TRUE\n)\n\n\n\nTracking read changes at each step of the mothur workflow.\n\n\n\n\n Download read changes for mothur pipeline",
    "crumbs": [
      "16S rRNA Processing",
      "3. OTU Workflow"
    ]
  },
  {
    "objectID": "docs/ssu-workflows/otu/index.html#preparing-for-analysis",
    "href": "docs/ssu-workflows/otu/index.html#preparing-for-analysis",
    "title": "3. OTU Workflow",
    "section": "Preparing for analysis",
    "text": "Preparing for analysis\nrename.file(fasta=current, count=current, taxonomy=current, prefix=final)\nCurrent files saved by mothur:\nfasta=final.fasta\ntaxonomy=final.taxonomy\ncount=final.count_table",
    "crumbs": [
      "16S rRNA Processing",
      "3. OTU Workflow"
    ]
  },
  {
    "objectID": "docs/ssu-workflows/otu/index.html#clustering",
    "href": "docs/ssu-workflows/otu/index.html#clustering",
    "title": "3. OTU Workflow",
    "section": "Clustering",
    "text": "Clustering\ncluster.split(fasta=final.fasta, count=final.count_table, taxonomy=final.taxonomy, taxlevel=4, cluster=f, processors=$PROC)\ncluster.split(file=final.file, count=final.count_table, processors=$PROC)\nUsing 30 processors.\nSplitting the file...\n/******************************************/\nSelecting sequences for group Vibrionales (1 of 364)\nNumber of unique sequences: 92783\n\nSelected 5390956 sequences from final.count_table.\n\nCalculating distances for group Vibrionales (1 of 364):\n\nSequence    Time    Num_Dists_Below_Cutoff\n\nIt took 902 secs to find distances for 92783 sequences. \n477552179 distances below cutoff 0.03.\n\nOutput File Names:\nfinal.0.dist\n\n...\n\nIt took 8671 seconds to cluster\nMerging the clustered files...\nIt took 14 seconds to merge.\n[WARNING]: Cannot run sens.spec analysis without a column file, \nskipping.\nOutput File Names: \nfinal.opti_mcc.list\nsystem(mkdir cluster.split.gsrdb) \nsystem(mv final.opti_mcc.list cluster.split.gsrdb/) \nsystem(mv final.file cluster.split.gsrdb/) \nsystem(mv final.dist cluster.split.gsrdb/)\ndist.seqs(fasta=final.fasta, cutoff=0.03, processors=\\$PROC) cluster(column=final.dist, count=final.count_table)\nSequence    Time    Num_Dists_Below_Cutoff\n\nIt took 91935 secs to find distances for 1022766 sequences. \n1096480673 distances below cutoff 0.03.\n\nOutput File Names: \nfinal.dist\n\nYou did not set a cutoff, using 0.03.\n\nClustering final.dist\n\niter    time    label   num_otus    cutoff  tp  tn  fp  fn  sensitivity specificity ppv npv fdr accuracy    mcc f1score\n\n0.03\n0   0   0.03    1022766 0.03    0   5.21928e+11 0   1.09648e+09 0   1   0   0.997904    1   0.997904    0   0   \n1   3187    0.03    130371  0.03    7.80436e+08 5.21829e+11 9.9517e+07  3.16045e+08 0.711764    0.999809    0.886906    0.999395    0.886906    0.999205    0.794146    0.789741    \n2   3706    0.03    119919  0.03    7.82225e+08 5.21828e+11 9.99504e+07 3.14256e+08 0.713396    0.999808    0.8867  0.999398    0.8867  0.999208    0.794965    0.790663    \n3   3712    0.03    119453  0.03    7.82257e+08 5.21828e+11 9.99331e+07 3.14224e+08 0.713425    0.999809    0.886722    0.999398    0.886722    0.999208    0.794991    0.790689    \n\nIt took 21013 seconds to cluster\n\nOutput File Names: \nfinal.opti_mcc.list\nfinal.opti_mcc.steps\nfinal.opti_mcc.sensspec",
    "crumbs": [
      "16S rRNA Processing",
      "3. OTU Workflow"
    ]
  },
  {
    "objectID": "docs/ssu-workflows/otu/index.html#getting-files-from-mothur",
    "href": "docs/ssu-workflows/otu/index.html#getting-files-from-mothur",
    "title": "3. OTU Workflow",
    "section": "Getting Files from Mothur",
    "text": "Getting Files from Mothur\nTo create a microtable object we need a a sequence table, taxonomy table, and a sample data table. To generate the sequence table we need a shared file from mothur, which we can generate using the command make.shared. The data in a shared file represent the number of times that an OTU is observed in multiple samples.\nmake.shared(list=final.opti_mcc.list, count=final.count_table, label=0.03)\n0.03\n\nOutput File Names:\nfinal.opti_mcc.shared\nNext we use classify.otu to get the OTU taxonomy table.\nclassify.otu(list=final.opti_mcc.list, count=final.count_table, taxonomy=final.taxonomy, label=0.03)\n0.03\n\nOutput File Names: \nfinal.opti_mcc.0.03.cons.taxonomy\nfinal.opti_mcc.0.03.cons.tax.summary\ncount.groups(shared=final.opti_mcc.shared)\nSize of smallest group: 14.\n\nTotal seqs: 34611554.\n\nOutput File Names: \nfinal.opti_mcc.count.summary",
    "crumbs": [
      "16S rRNA Processing",
      "3. OTU Workflow"
    ]
  },
  {
    "objectID": "docs/ssu-workflows/otu/index.html#prep-data-for-microeco",
    "href": "docs/ssu-workflows/otu/index.html#prep-data-for-microeco",
    "title": "3. OTU Workflow",
    "section": "Prep Data for microeco\n",
    "text": "Prep Data for microeco\n\nLike any tool, the microeco package needs the data in a specific form. I formatted our data to match the mock data in the microeco tutorial, specifically this section.\nA. Taxonomy Table\nHere is what the taxonomy table looks like in the mock data.\n\ntaxonomy_table_16S[1:6, 1:4]\n\n         Kingdom      Phylum            Class                 Order\nOTU_4272 k__Bacteria  p__Firmicutes     c__Bacilli            o__Bacillales\nOTU_236  k__Bacteria  p__Chloroflexi    c__                   o__\nOTU_399  k__Bacteria  p__Proteobacteria c__Betaproteobacteria o__Nitrosomonadales\nOTU_1556 k__Bacteria  p__Acidobacteria  c__Acidobacteria      o__Subgroup 17\nOTU_32   k__Archaea   p__Miscellaneous  c__                   o__\nOTU_706  k__Bacteria  p__Actinobacteria c__Actinobacteria     o__Frankiales\nOur taxonomy file (below) needs a little wrangling to be properly formatted.\n\ntmp_tax &lt;- read_delim(\"final.opti_mcc.0.03.cons.taxonomy\", \n                      delim = \"\\t\")\nhead(tmp_tax)\n\nOTU       Size    Taxonomy\nOtu000001 2777831 Bacteria(100);Proteobacteria(100);Gammaproteobacteria(100);Vibrionales(100);Vibrionaceae(100);Vibrio(97);\nOtu000002 1587321 Bacteria(100);Bacteria_unclassified(100);Bacteria_unclassified(100);Bacteria_unclassified(100);Bacteria_unclassified(100);Bacteria_unclassified(100);\nOtu000003 1228070 BBacteria(100);Proteobacteria(100);Gammaproteobacteria(100);Gammaproteobacteria_unclassified(100);Gammaproteobacteria_unclassified(100);Gammaproteoba\nOtu000004 1199712 Bacteria(100);Bacteroidetes(100);Bacteroidia(100);Marinilabiliales(100);Marinilabiliaceae(100);Saccharicrinis(100);\nOtu000005  877883 Bacteria(100);Proteobacteria(100);Alphaproteobacteria(100);Rhodobacterales(100);Rhodobacterales_unclassified(100);Rhodobacterales_unclassified(100);\nOtu000006  660495 Bacteria(100);Bacteria_unclassified(100);Bacteria_unclassified(100);Bacteria_unclassified(100);Bacteria_unclassified(100);Bacteria_unclassified(100);\nSome fancy string manipulation…\n\ntmp_tax &lt;- data.frame(sapply(tmp_tax, \n                             gsub, \n                             pattern = \"\\\\(\\\\d+\\\\)\", \n                             replacement = \"\"))\ntmp_tax &lt;- data.frame(sapply(tmp_tax, \n                             gsub, \n                             pattern = \";$\", \n                             replacement = \"\"))\ntmp_tax &lt;- separate_wider_delim(tmp_tax, \n                              cols = Taxonomy, \n                              delim = \";\", names = c(\n                                \"Kingdom\", \"Phylum\", \n                                \"Class\", \"Order\", \n                                \"Family\", \"Genus\" \n                                )\n                              )\ntmp_tax &lt;- data.frame(sapply(tmp_tax, gsub, \n                           pattern = \"^.*_unclassified$\", \n                           replacement = \"\"))\ntmp_tax$Size &lt;- NULL\ntmp_tax &lt;- tibble::column_to_rownames(tmp_tax, \"OTU\")\nhead(tmp_tax)\n\nAnd we get this …\n           Kingdom    Phylum        Class              Order\nOtu000001  Bacteria   Proteobacteria  Gammaproteobacteria  Vibrionales  \nOtu000002  Bacteria\nOtu000003  Bacteria   Proteobacteria  Gammaproteobacteria    \nOtu000004  Bacteria   Bacteroidetes   Bacteroidia          Marinilabiliales  \nOtu000005  Bacteria   Proteobacteria  Alphaproteobacteria  Rhodobacterales  \nOtu000006  Bacteria\n\ntmp_tax[is.na(tmp_tax)] &lt;- \"\"\ntmp_tax$Kingdom &lt;- paste(\"k\", tmp_tax$Kingdom, sep = \"__\")\ntmp_tax$Phylum &lt;- paste(\"p\", tmp_tax$Phylum, sep = \"__\")\ntmp_tax$Class &lt;- paste(\"c\", tmp_tax$Class, sep = \"__\")\ntmp_tax$Order &lt;- paste(\"o\", tmp_tax$Order, sep = \"__\")\ntmp_tax$Family &lt;- paste(\"f\", tmp_tax$Family, sep = \"__\")\ntmp_tax$Genus &lt;- paste(\"g\", tmp_tax$Genus, sep = \"__\")\n\ntmp_tax %&lt;&gt;% tidy_taxonomy\nhead(tmp_tax)\n\nAnd then this. Excatly like the mock data.\n            Kingdom      Phylum             Class                   Order               Family\nOtu000001  k__Bacteria   p__Proteobacteria  c__Gammaproteobacteria  o__Vibrionales      f__Vibrionaceae  \nOtu000002  k__Bacteria   p__                c__                     o__                 f__\nOtu000003  k__Bacteria   p__Proteobacteria  c__Gammaproteobacteria  o__                 f__\nOtu000004  k__Bacteria   p__Bacteroidetes   c__Bacteroidia          o__Marinilabiliales f__Marinilabiliaceae  \nOtu000005  k__Bacteria   p__Proteobacteria  c__Alphaproteobacteria  o__Rhodobacterales  f__\nOtu000006  k__Bacteria   p__                c__                     o__                 f__\nB. Sequence Table\nHere is what the sequence table looks like in the mock data.\n\notu_table_16S[1:6, 1:11]\n\n         S1 S2 S3 S4  S5  S6  S7 S9 S10 S11 S12\nOTU_4272  1  0  1  1   0   0   1  1   0   1   1\nOTU_236   1  4  0  2  35   5  94  0 177  14  27\nOTU_399   9  2  2  4   4   0   3  6   0   1   2\nOTU_1556  5 18  7  3   2   9   2  6   1   2   1\nOTU_32   83  9 19  8 102 300 158 55 321  16  13\nOTU_706   0  1  0  1   0   0   1  0   0   0   1\n\n\nThese code block will return a properly formatted sequence table.\n\ntmp_st &lt;- readr::read_delim(\"final.opti_mcc.shared\",  delim = \"\\t\")\n\n\ntmp_st$numOtus &lt;- NULL\ntmp_st$label &lt;- NULL\ntmp_st &lt;- tmp_st %&gt;%\n  tidyr::pivot_longer(cols = c(-1), names_to = \"tmp\") %&gt;%\n  tidyr::pivot_wider(names_from = c(1))\n\ntmp_st &lt;- tibble::column_to_rownames(tmp_st, \"tmp\")\n\n\n# only need to run this if reading in processed files\n# code adds a tab to the beginning of first line\nsed '1s/^/\\t/' tmp_final.opti_mcc.fixed.shared &gt; final.opti_mcc.fixed.shared\n\nC. Sample Table\nHere is what the sample table looks like in the mock data.\n\nhead(sample_info_16S)\n\n   SampleID Group Type          Saline\nS1       S1    IW   NE Non-saline soil\nS2       S2    IW   NE Non-saline soil\nS3       S3    IW   NE Non-saline soil\nS4       S4    IW   NE Non-saline soil\nS5       S5    IW   NE Non-saline soil\nS6       S6    IW   NE Non-saline soil\n\n\nNo problem.\n\nsamdf &lt;- readRDS(\"../sampledata/files/tables/samdf.rds\")\n\nsamdf &lt;- samdf %&gt;% tibble::column_to_rownames(\"SampleID\")\nsamdf$SampleID &lt;- rownames(samdf)\nsamdf &lt;- samdf %&gt;% dplyr::relocate(SampleID)\n\nsamdf &lt;- samdf %&gt;%\n  dplyr::filter(\n    stringr::str_starts(SampleID, \"Control\", negate = TRUE)\n    )",
    "crumbs": [
      "16S rRNA Processing",
      "3. OTU Workflow"
    ]
  },
  {
    "objectID": "docs/ssu-workflows/otu/index.html#create-a-microtable-object",
    "href": "docs/ssu-workflows/otu/index.html#create-a-microtable-object",
    "title": "3. OTU Workflow",
    "section": "Create a Microtable Object",
    "text": "Create a Microtable Object\nWith these three files in hand we are now ready to create a microtable object.\n\n\n\n\n\n\nNote\n\n\n\nA microtable object contains an OTU table (taxa abundances), sample metadata, and taxonomy table (mapping between OTUs and higher-level taxonomic classifications).\n\n\n\nsample_info &lt;- samdf\ntax_tab &lt;- tmp_tax\notu_tab &lt;- tmp_st\n\n\ntmp_me &lt;- microtable$new(sample_table = sample_info, \n                         otu_table = otu_tab, \n                         tax_table = tax_tab)\ntmp_me\n\nmicrotable-class object:\nsample_table have 1849 rows and 13 columns\notu_table have 119453 rows and 1849 columns\ntax_table have 119453 rows and 6 columns\nAdd Representative Sequence\nWe can also add representative sequences for each OTU/ASV. For this step, we use the mothur command get.oturep.\nget.oturep(column=final.dist, list=final.opti_mcc.list, count=final.count_table, fasta=final.fasta)\nThe fasta file it returns needs a little T.L.C.\nYou did not provide a label, using 0.03.\n0.03    119453\n\nOutput File Names: \nfinal.opti_mcc.0.03.rep.count_table\nfinal.opti_mcc.0.03.rep.fasta\nFor that we use a tool called SeqKit [Shen et al. (2016);shen2024seqkit2] for fasta defline manipulation.\n\nseqkit replace -p \"\\|.*\" -r '' final.opti_mcc.0.03.rep.fasta &gt; tmp2.fa\nseqkit replace -p \".*\\\\t\" -r '' tmp2.fa &gt; tmp3.fa\nseqkit replace -p \"-\" -r '$1' -s -w 0 tmp3.fa &gt; otu_reps.fasta\nrm tmp*\n\n\nrep_fasta &lt;- Biostrings::readDNAStringSet(\"files_for_microeco/otu_reps.fasta\")\ntmp_me$rep_fasta &lt;- rep_fasta\ntmp_me$tidy_dataset()\ntmp_me",
    "crumbs": [
      "16S rRNA Processing",
      "3. OTU Workflow"
    ]
  },
  {
    "objectID": "docs/ssu-workflows/otu/index.html#curate-the-data-set",
    "href": "docs/ssu-workflows/otu/index.html#curate-the-data-set",
    "title": "3. OTU Workflow",
    "section": "Curate the Data Set",
    "text": "Curate the Data Set\nPretty much the last thing to do is remove low-count samples.\nRemove Low-Count Samples\n\ntmp_no_low &lt;- microeco::clone(me_otu_raw)\ntmp_no_low$otu_table &lt;- me_otu_raw$otu_table %&gt;%\n          dplyr::select(where(~ is.numeric(.) && sum(.) &gt;= 1000))\ntmp_no_low$tidy_dataset()\ntmp_no_low\n\n41 taxa with 0 abundance are removed from the otu_table ...\n\n\nmicrotable-class object:\nsample_table have 1838 rows and 13 columns\notu_table have 119412 rows and 1838 columns\ntax_table have 119412 rows and 6 columns\nrep_fasta have 119412 sequences\n\n\n\nme_otu &lt;- microeco::clone(tmp_no_low)",
    "crumbs": [
      "16S rRNA Processing",
      "3. OTU Workflow"
    ]
  },
  {
    "objectID": "docs/ssu-workflows/otu/index.html#summary",
    "href": "docs/ssu-workflows/otu/index.html#summary",
    "title": "3. OTU Workflow",
    "section": "Summary",
    "text": "Summary\nNow time to summarize the data. For this we use the R package miaverse (Ernst et al. 2024).\nFirst we do a little formatting to get our data compatible with mia.\n\n# https://github.com/microbiome/OMA/issues/202\ntmp_counts &lt;- as.matrix(me_otu_raw$otu_table)\ntmp_assays &lt;-  SimpleList(counts = tmp_counts)\nmia_me_otu_raw &lt;- TreeSummarizedExperiment(assays = tmp_assays,\n                                colData = DataFrame(me_otu_raw$sample_table),\n                                rowData = DataFrame(me_otu_raw$tax_table))\nrm(list = ls(pattern = \"tmp_\"))\ntmp_counts &lt;- as.matrix(me_otu$otu_table)\ntmp_assays &lt;-  SimpleList(counts = tmp_counts)\nmia_me_otu &lt;- TreeSummarizedExperiment(assays = tmp_assays,\n                                colData = DataFrame(me_otu$sample_table),\n                                rowData = DataFrame(me_otu$tax_table))\nmia_me_otu_raw_summ &lt;- summary(mia_me_otu_raw, assay.type = \"counts\")\nmia_me_otu_summ &lt;- summary(mia_me_otu, assay.type = \"counts\")\nrm(list = ls(pattern = \"tmp_\"))\nobjects()\n\n\n\n\n\n\n\n\nMetric\nStart\nEnd\n\n\n\nMin. number reads\n14\n1160\n\n\nMax. number reads\n244661\n244661\n\n\nTotal number reads\n34611554\n34606505\n\n\nAvg number reads\n18719\n18828\n\n\nMedian number reads\n14834\n14885\n\n\nTotal OTUs\n119453\n119412\n\n\nNumber singleton OTUs\n58112\n58086\n\n\nAvg OTUs per sample.\n444\n446\n\n\n\nWe started off with 119453 OTUs and 1849 samples. After removing low-count samples, there were 119412 OTUs and 1838 samples remaining.",
    "crumbs": [
      "16S rRNA Processing",
      "3. OTU Workflow"
    ]
  },
  {
    "objectID": "docs/ssu-workflows/otu/index.html#footnotes",
    "href": "docs/ssu-workflows/otu/index.html#footnotes",
    "title": "3. OTU Workflow",
    "section": "Footnotes",
    "text": "Footnotes\n\nFrom the developers: GSR database (Greengenes, SILVA, and RDP database) is an integrated and manually curated database for bacterial and archaeal 16S amplicon taxonomy analysis. Unlike previous integration approaches, this database creation pipeline includes a taxonomy unification step to ensure consistency in taxonomical annotations. The database was validated with three mock communities and two real datasets and compared with existing 16S databases such as Greengenes, GTDB, ITGDB, SILVA, RDP, and MetaSquare. Results showed that the GSR database enhances taxonomical annotations of 16S sequences, outperforming current 16S databases at the species level. The GSR database is available for full-length 16S sequences and the most commonly used hypervariable regions: V4, V1-V3, V3-V4, and V3-V5.↩︎",
    "crumbs": [
      "16S rRNA Processing",
      "3. OTU Workflow"
    ]
  },
  {
    "objectID": "docs/mg-workflows/assembly/index.html",
    "href": "docs/mg-workflows/assembly/index.html",
    "title": "2. Co-Assembly & Annotations",
    "section": "",
    "text": "Click here for libraries and setup information.knitr::opts_chunk$set(echo = TRUE, eval = FALSE)\nset.seed(919191)\nlibrary(ggplot2); packageVersion(\"ggplot2\")\nlibrary(Biostrings); packageVersion(\"Biostrings\")\npacman::p_load(tidyverse, gridExtra, grid,\n               formatR, reactable, gdata, patchwork,\n               kableExtra, magrittr, \n               install = FALSE, update = FALSE)\n\noptions(scipen = 999)\nknitr::opts_current$get(c(\n  \"cache\",\n  \"cache.path\",\n  \"cache.rebuild\",\n  \"dependson\",\n  \"autodep\"\n))\n#root &lt;- find_root(has_file(\"_quarto.yml\"))\n#source(file.path(root, \"assets\", \"functions.R\"))",
    "crumbs": [
      "Metagenomics",
      "2. Co-Assembly & Annotations"
    ]
  },
  {
    "objectID": "docs/mg-workflows/assembly/index.html#snakemake-citations",
    "href": "docs/mg-workflows/assembly/index.html#snakemake-citations",
    "title": "2. Co-Assembly & Annotations",
    "section": "Snakemake Citations",
    "text": "Snakemake Citations\nThere are many tools used in the workflow that need to be cited.\n\n\nJob\nTool\nReference\n\n\n\nWORKFLOW\nSnakemake\n(Köster and Rahmann 2012)\n\n\nQUALITY-FILTERING\nIllumina Utils\n(Eren et al. 2013; Minoche, Dohm, and Himmelbauer 2011)\n\n\nCO-ASSEMBLY\nMegahit\n(D. Li et al. 2015)\n\n\nGENE CALLING\nProdigal\n(Hyatt et al. 2010)\n\n\nRECRUITMENT\nBOWTIE2\n(Langmead and Salzberg 2012)\n\n\n\nSAMtools\n(H. Li et al. 2009)\n\n\nCLASSIFICATION\nCENTRIFUGE\n(Kim et al. 2016)\n\n\n\nAlso, there are a few tools that we ran outside of the Snakemake workflow. Results from these steps need to be added to the individual PROFILE.db’s, merged PROFILE.db, or CONTIGS.db. Therefore, before the anvi-merge portion of the Snakemake workflow finished, we killed the job, ran the accessory analyses described below, and then restarted the workflow to finish the missing step. Cumbersome, yes, but it got the job done.",
    "crumbs": [
      "Metagenomics",
      "2. Co-Assembly & Annotations"
    ]
  },
  {
    "objectID": "docs/mg-workflows/assembly/index.html#taxonomic",
    "href": "docs/mg-workflows/assembly/index.html#taxonomic",
    "title": "2. Co-Assembly & Annotations",
    "section": "Taxonomic",
    "text": "Taxonomic\nIn this section we discuss taxonomic classification of short reads, contigs, and gene calls. We go through the steps of analyzing the data and getting the results into anvi’o databases.\nShort-reads with Kraken2\nIn this section we use Kraken2 (Wood, Lu, and Langmead 2019) to classify the short reads. Our goal is to classify short-reads, generate an input file for anvi’o, and create Krona plots for data visualization. Brace yourself.\n\n\n\n\n\n\nImportant\n\n\n\nSince Kraken2 annotation is performed on individual samples and the results are imported into the individual profile.db’s we will need to re-merge the all profile.db after these steps are completed. The merging step is basically the last step of the Snakemake workflow.\n\n\n\nconda activate kraken2\nfor sample in `cat sampleskraken2.txt`\ndo\n    kraken2 --paired 01_QC/$sample-QUALITY_PASSED_R1.fastq.gz  \\\n                     01_QC/$sample-QUALITY_PASSED_R2.fastq.gz \\\n                     --db kraken2_db/ \\\n                     --use-names \\\n                     --threads $NSLOTS \\\n                     --output $sample-kraken.out \\\n                     --report $sample-kraken-report.txt\ndone\nconda deactivate \n\nAfter this is finished we should have two files for each sample–a .out file containing the results of the Kraken2 annotation and a .report.txt file that summarizes the results.\nFirst, we generate the file that anvi’o needs–the format is very specific. For this task we use KrakenTool–a suite of very handy scripts to Kraken 2 data. We will use a tool called kreport2mpa.py, which takes a Kraken report file and prints out a MPA (MetaPhlAn)-style TEXT file.\n\nconda activate krakentools\nfor sample in `cat sampleskraken2.txt`\ndo\n    kreport2mpa.py -r $sample-kraken-report.txt -o $sample-kraken-mpa.txt    \ndone\nconda deactivate \n\nEasy as that. Now we can import all MPA files into their respective contig databases. Here we had to split the sample list by co-assembly because I could not figure out an easier way.\n\nfor sample in `cat EP_list.txt`\ndo \n    anvi-import-taxonomy-for-layers \\\n                   -p 05_ANVIO_PROFILE/EP/$sample/PROFILE.db \\\n                   --parse krakenuniq \\\n                   -i 07_TAXONOMY/KRAKEN_TAXONOMY/$sample-kraken-mpa.txt\ndone\n\nfor sample in `cat WA_list.txt`\ndo \n    anvi-import-taxonomy-for-layers \\\n                   -p 05_ANVIO_PROFILE/WA/$sample/PROFILE.db \\\n                   --parse krakenuniq -i \\\n                   07_TAXONOMY/KRAKEN_TAXONOMY/$sample-kraken-mpa.txt\ndone\n\nRad. With this done we can re-merge the profile databases.\n\nanvi-merge 05_ANVIO_PROFILE/WA/*/PROFILE.db \\\n          -c 03_CONTIGS/WA-contigs.db \\\n          -o 06_MERGED/WA\nanvi-merge 05_ANVIO_PROFILE/EP/*/PROFILE.db \\\n          -c 03_CONTIGS/EP-contigs.db \\\n          -o 06_MERGED/EP\n\nAlright, time to make some Krona plots. This is a two-steo process. First, we use two scripts from metaWRAP. The first, called kraken2_translate.py, is used to generate full taxonomic lineages from a taxid. The input file for this is the output of the Kraken2 annotation. The next script is called kraken_to_krona.py which takes the output of the first script (translated Kraken file) and parses it into a format that Krona can use to produce plots.\n\nconda activate metawrap\nfor sample in `cat sampleskraken2.txt`\ndo\n    kraken2_translate.py kraken2_db $sample-kraken.out $sample-kraken.trans\n    kraken_to_krona.py $sample-kraken.trans &gt; $KRAKEN/$sample-kraken.krona\ndone\nconda deactivate \n\nOnce that is complete, we can use the output files and a script called ktImportText from the Krona package to produce HTML Krona plots for each sample.\n\nconda activate krona\nfor sample in `cat sampleskraken2.txt`\ndo\n    ktImportText $sample-kraken.krona -o $sample-kraken.krona.html\ndone\nconda deactivate \n\nA plot for every sample. How great is that?\nVirSorter Annotation\nTo classify any viral sequences, we ran VirSorter2 (Guo et al. 2021) on contigs from the co-assembly using our newly created contig.db. First, we need something for VirSorter2 to classify. For that we export fasta files from each anvi’o co-assembly.\n\nanvi-export-contigs -c 03_CONTIGS/WA-contigs.db \\\n                    -o 03_CONTIGS/WA-splits.fa \\ \n                    --splits-mode --no-wrap\nanvi-export-contigs -c 03_CONTIGS/EP-contigs.db \\\n                    -o 03_CONTIGS/EP-splits.fa \\\n                    --splits-mode --no-wrap\n\nAnd the code we used to run VirSorter2.\n\nconda activate virsorter2\nvirsorter run --seqfile 03_CONTIGS/WA-splits.fa \\\n              --working-dir WA/ \\\n              --keep-original-seq \\\n              --prep-for-dramv \\\n              --hallmark-required-on-short \\\n              --db-dir virsorter2_db \nvirsorter run --seqfile 03_CONTIGS/EP-splits.fa \\\n              --working-dir EP/ \\\n              --keep-original-seq \\\n              --prep-for-dramv \\\n              --hallmark-required-on-short \\\n              --db-dir virsorter2_db\nconda deactivate\n\nNow, to get Virsorter2 annotations into the anvi’o contig databases there are a few special steps that need to be taken. Please see this post and associated virsorter_to_anvio.py script for more details. Here we will only include the code with minimal explanation. Too long…\nFirst we export two tables that the virsorter_to_anvio.py script needs for import.\n\nanvi-export-table 03_CONTIGS/WA-contigs.db  --table splits_basic_info \\\n                  --output-file WA_splits_basic_info.txt\nanvi-export-table 03_CONTIGS/EP-contigs.db  --table splits_basic_info \\\n                  --output-file EP_splits_basic_info.txt\n\nanvi-export-gene-calls -c 03_CONTIGS/WA-contigs.db \\\n                       --output-file WA_all_gene_calls.txt \\\n                       --gene-caller prodigal\nanvi-export-gene-calls -c 03_CONTIGS/EP-contigs.db \\\n                       --output-file EP_all_gene_calls.txt \\\n                       --gene-caller prodigal\n\nTime to get messy. At the time of this writing, the gene_calls file exported from anvi’o is a 10-column tab-delimited text file. The virsorter_to_anvio.py script needs only 8 of these, and they need to be in a specific order. No problem, we can use awk.\n\n\nThese are the column values needed by the virsorter_to_anvio.py script:gene_callers_id, contig, start, stop, direction, partial, source, version\n\nawk 'BEGIN {FS=\"\\t\"; OFS=\"\\t\"} {print $1, $2, $3, $4, $5, $6, $8, $9}' WA_all_gene_calls.txt\nawk 'BEGIN {FS=\"\\t\"; OFS=\"\\t\"} {print $1, $2, $3, $4, $5, $6, $8, $9}' EP_all_gene_calls.txt\n\nAfter this we run the parsing script.\n\nvirsorter_to_anvio.py -i WA/ -s WA/WA_splits_basic_info.txt \\\n                      -n WA/WA_all_gene_calls.txt \\\n                      -d virsorter2_db \\\n                      -A WA/WA_virsorter_additional_info.txt \\\n                      -C WA/WA_virsorter_collection.txt \\\n                      -F WA/WA_virsorter_annotations.txt\n\nvirsorter_to_anvio.py -i EP/ -s EP/EP_splits_basic_info.txt \\\n                      -n EP/EP_all_gene_calls.txt \\\n                      -d virsorter2_db \\\n                      -A EP/EP_virsorter_additional_info.txt \\\n                      -C EP/EP_virsorter_collection.txt \\\n                      -F EP/EP_virsorter_annotations.txt\n\nAnd import the resulting files to anvi’o\n\nanvi-import-misc-data WA-virsorter_additional_info.txt \n                      -p 06_MERGED/WA/PROFILE.db  \n                      --target-data-table items\nanvi-import-misc-data EP-virsorter_additional_info.txt \n                      -p 06_MERGED/EP/PROFILE.db  \n                      --target-data-table items\n\nanvi-import-collection WA-virsorter_collection.txt \n                      -c 03_CONTIGS/WA-contigs.db \n                      -p 06_MERGED/WA/PROFILE.db \n                      -C VIRSORTER2\nanvi-import-collection EP-virsorter_collection.txt \n                      -c 03_CONTIGS/EP-contigs.db \n                      -p 06_MERGED/EP/PROFILE.db\n                      -C VIRSORTER2\n\nVirSorter2 annotation complete.",
    "crumbs": [
      "Metagenomics",
      "2. Co-Assembly & Annotations"
    ]
  },
  {
    "objectID": "docs/mg-workflows/assembly/index.html#kaiju-annotation",
    "href": "docs/mg-workflows/assembly/index.html#kaiju-annotation",
    "title": "2. Co-Assembly & Annotations",
    "section": "Kaiju Annotation",
    "text": "Kaiju Annotation\nHere we use Kaiju (Menzel, Ng, and Krogh 2016) to classify gene calls. We do this against the progenomes databases, a r epresentative set of genomes from the proGenomes database and viruses from the NCBI RefSeq database. We describe the contruction of this database here.\nStart by grabbing gene call fasta files.\n\nanvi-get-sequences-for-gene-calls -c 03_CONTIGS/EP-contigs.db -o EP_gene_calls.fna\nanvi-get-sequences-for-gene-calls -c 03_CONTIGS/WA-contigs.db -o WA_gene_calls.fna\n\n\nconda activate kaiju\nkaiju -t kaiju_db/nodes.dmp \\\n      -f kaiju_db/kaiju_db_progenomes.fmi \\\n      -i EP_gene_calls.fna \\ \n      -o EP_kaiju_nr.out \\\n      -z $NSLOTS -v\n\nkaiju -t kaiju_db/nodes.dmp \\\n      -f kaiju_db/kaiju_db_progenomes.fmi \\\n      -i WA_gene_calls.fna \\\n      -o WA_kaiju_nr.out \\\n      -z $NSLOTS -v\n\nkaiju-addTaxonNames -t kaiju_db/nodes.dmp \\\n                    -n kaiju_db/names.dmp \\\n                    -i EP_kaiju_nr.out \\\n                    -o EP_kaiju_nr.names \\\n                    -r superkingdom,phylum,order,class,family,genus,species\n\nkaiju-addTaxonNames -t kaiju_db/nodes.dmp \n                    -n kaiju_db/names.dmp \\\n                    -i WA_kaiju_nr.out \\\n                    -o WA_kaiju_nr.names \\ \n                    -r superkingdom,phylum,order,class,family,genus,species\nconda deactivate\n\nImport the output to anvi’o.\n\nanvi-import-taxonomy-for-genes -c 03_CONTIGS/EP-contigs.db \\\n                               -p kaiju \\\n                               -i EP_kaiju_nr.names \n\nanvi-import-taxonomy-for-genes -c 03_CONTIGS/WA-contigs.db \\\n                               -p kaiju \\\n                               -i WA_kaiju_nr.names\n\nAnd generate Krona plots of the data. A little dance between the Kaiju and Krona environments.\n\nconda activate kaiju\nkaiju2krona -t kaiju_db/nodes.dmp \\\n            -n kaiju_db/names.dmp \\\n            -i $KAIJU/WA_kaiju_nr.out \\\n            -o WA_kaiju_nr.out.krona\n\nkaiju2krona -t kaiju_db/nodes.dmp \\\n            -n kaiju_db/names.dmp \\\n            -i EP_kaiju_nr.out \\\n            -o EP_kaiju_nr.out.krona\nconda deactivate\n\n\nconda activate krona\nktImportText -o WA_kaiju_nr.out.html WA_kaiju_nr.out.krona\nktImportText -o EP_kaiju_nr.out.html EP_kaiju_nr.out.krona\nconda deactivate \n\n\nconda activate kaiju\n\nkaiju2table -t kaiju_db/nodes.dmp \\\n            -n kaiju_db/names.dmp \\\n            -r class \\\n            -l phylum,class,order,family \\\n            -o WA_kaiju_nr.out.summary WA_kaiju_nr.out \\\nkaiju2table -t kaiju_db/nodes.dmp \\\n            -n kaiju_db/names.dmp \\\n            -r class \\\n            -l phylum,class,order,family \\\n            -o EP_kaiju_nr.out.summary EP_kaiju_nr.out\n\nconda deactivate\n\nKaiju done.",
    "crumbs": [
      "Metagenomics",
      "2. Co-Assembly & Annotations"
    ]
  },
  {
    "objectID": "docs/data/data-med.html",
    "href": "docs/data/data-med.html",
    "title": "Data & Scripts",
    "section": "",
    "text": "Quick access to pipeline processing scripts and raw data. With these scripts and associated data you can run the processing steps for each analysis.\nAll sequence data is linked to projects on the European Nucleotide Archive (ENA). The scripts and associated files can be downloaded as .zip archives by clicking the links. Or if you prefer, you can copy the scripts directly from the code blocks below."
  },
  {
    "objectID": "docs/data/data-med.html#med-data-and-scripts",
    "href": "docs/data/data-med.html#med-data-and-scripts",
    "title": "Data & Scripts",
    "section": "MED Data and Scripts",
    "text": "MED Data and Scripts\n\n\n\n\nName\n\n\nDescription\n\n\n\n\n\nSequence data\n\n\nOutput files from alignment step of the mothur OTU pipeline for 1809 samples \n\n\n\n\nSample data\n\n\n16S rRNA sample data \n\n\n\n\nmothur processing scripts\n\n\nThe mothur portion of the MED pipeline \n\n\n\n\nData prep scripts\n\n\nThe mothur2oligo scripts needed to prep the data for MED analysis \n\n\n\n\nMapping file\n\n\nThe sample mapping needed to run the MED analysis \n\n\n\n\nHydra job scripts\n\n\nHydra job scripts. \n\n\n\n\nPipeline read changes\n\n\nTable showing read changes for each sample through the processing pipeline \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "docs/data/data-med.html#med-processing",
    "href": "docs/data/data-med.html#med-processing",
    "title": "Data & Scripts",
    "section": "MED Processing",
    "text": "MED Processing\nProcessing scripts for Minimum Entropy Decomposition (MED) analysis. The pipeline begins with the output fasta and count files from the align.seqs part of the mothur OTU pipeline. From there we use mothur to remove negative control samples, check for chimera, and run taxonomic classifications. It is important to note that the MED workflow does not precluster sequences (as in the mothur pipeline) because MED relies on every sequence (including redundant reads) for the analysis. This pipeline has four main steps:\n\nrun the mothur workflow.\n\nmodify and run the mothur2oligo.sh script. This script transforms the mothur output to appropriate format for MED. It must be run in the mothur environment because the script uses mothur. We need access to the following mothur files to run this script.\n\n\ntaxonomy file: final_med.taxonomy\n\n\ncount file: final_med.count_table\n\n\nfasta file: final_med.fasta\n\n\n\ntrim uninformative columns from alignment (in the MED environment)\n\nrun the MED command\n\n\nmothur processing script for MED analysisset.dir(output=/pool/genomics/stri_istmobiome/data/TRANS_SHRIMP/16S/MOTHUR/pipelineFiles_med/)\n\nsystem(cp pipelineFiles/PROCESSING_FILES/shrimp.trim.contigs.good.unique.good.filter.unique.fasta pipelineFiles_med/)\nsystem(cp pipelineFiles/PROCESSING_FILES/shrimp.trim.contigs.good.unique.good.filter.count_table pipelineFiles_med/)\n\n############################################################\n### Just copied the fasta and count files from last command\n############################################################\n\n############################################################\n### ############  FOR MED REMOVED PRECLUST #################\n############################################################\n\n############################################################\n### ############  REMOVE NEGATIVE CONTROL ##################\n### from https://forum.mothur.org/t/negative-control/2754/16\n############################################################\n\nget.groups(fasta=shrimp.trim.contigs.good.unique.good.filter.unique.fasta, count=shrimp.trim.contigs.good.unique.good.filter.count_table, groups=Control_49-Control_23-Control_17-Control_24-Control_14-Control_44-Control_20-Control_33-Control_41-Control_29-Control_50-Control_22-Control_19-Control_18-Control_48-Control_13-Control_21-Control_16-Control_30-Control_5-Control_42-Control_25-Control_51-Control_40-Control_15-Control_36-Control_47-Control_27-Control_32-Control_8-Control_3-Control_4-Control_6-Control_45-Control_26-Control_46-Control_53-Control_7-Control_12-Control_10-Control_9-Control_35-Control_54-Control_2-Control_43-Control_1-Control_11-Control_52-Control_38-Control_34-Control_56-Control_37-Control_28-Control_57-Control_31-Control_39-Control_59-Control_55-Control_60-Control_58)\nrename.file(input=shrimp.trim.contigs.good.unique.good.filter.unique.pick.fasta, new=neg_control.fasta)\nrename.file(input=shrimp.trim.contigs.good.unique.good.filter.pick.count_table, new=neg_control.count_table)\nsummary.seqs(fasta=neg_control.fasta, count=neg_control.count_table, processors=30)\nlist.seqs(count=neg_control.count_table)\n\n########################################################################################################\n### modified neg_control.count_table: \n### 1. remove first two rows\n### 2. in BBEdit run \\t\\d+,.*\n### 3. in R\n###     library(tidyverse)\n###     a &lt;- read_tsv(\"neg_control.count_table\")\n###     b &lt;- read_tsv(\"shrimp.trim.contigs.good.unique.good.filter.count_table\")\n###     e &lt;- dplyr::left_join(a, b, by = \"Representative_Sequence\")\n###     write.table(e, \"results.txt\", row.names = FALSE, quote = FALSE, sep = \"\\t\")\n### 4. Replace accnos file with new list. Removed sequences that were found less than 10% in Neg control\n########################################################################################################\n    \nremove.seqs(accnos=neg_control_subset.accnos, fasta=shrimp.trim.contigs.good.unique.good.filter.unique.fasta, count=shrimp.trim.contigs.good.unique.good.filter.count_table)\ncount.groups(count=current)\n########################################################\n## ADDED this command to remove NC samples\n#\n\n## [ERROR]: Control_18 is not in your count table. Please correct.\n## [ERROR]: Control_5 is not in your count table. Please correct.\n\nremove.groups(count=shrimp.trim.contigs.good.unique.good.filter.pick.count_table, fasta=shrimp.trim.contigs.good.unique.good.filter.unique.pick.fasta, groups=Control_1-Control_10-Control_11-Control_12-Control_13-Control_14-Control_15-Control_16-Control_17-Control_19-Control_2-Control_20-Control_21-Control_22-Control_23-Control_24-Control_25-Control_26-Control_27-Control_28-Control_29-Control_3-Control_30-Control_31-Control_32-Control_33-Control_34-Control_35-Control_36-Control_37-Control_38-Control_39-Control_4-Control_40-Control_41-Control_42-Control_43-Control_44-Control_45-Control_46-Control_47-Control_48-Control_49-Control_50-Control_51-Control_52-Control_53-Control_54-Control_55-Control_56-Control_57-Control_58-Control_59-Control_6-Control_60-Control_7-Control_8-Control_9)\n########################################################\n\nsummary.seqs(fasta=current, count=current, processors=30)\ncount.groups(count=current)\n\n########################################\n### NEGATIVE CONTROLS Should be GONE ###\n########################################\n\nchimera.vsearch(fasta=shrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.fasta, count=shrimp.trim.contigs.good.unique.good.filter.pick.pick.count_table, dereplicate=t, processors=30)\nsummary.seqs(fasta=current, count=current, processors=30)\ncount.groups(count=current)\n\nclassify.seqs(fasta=shrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.fasta, count=shrimp.trim.contigs.good.unique.good.filter.pick.pick.denovo.vsearch.count_table, reference=reference_dbs/gsrdb.fasta, taxonomy=reference_dbs/gsrdb.tax, processors=30)\nremove.lineage(fasta=shrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.fasta, count=shrimp.trim.contigs.good.unique.good.filter.pick.pick.denovo.vsearch.count_table, taxonomy=shrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.gsrdb.wang.taxonomy, taxon=Chloroplast-Mitochondria-unknown-Eukaryota)\n\nsummary.seqs(fasta=current, count=current, processors=30)\nsummary.tax(taxonomy=current, count=current, processors=30)\ncount.groups(count=current)\n\n##########################\nrename.file(fasta=shrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.pick.fasta, count=shrimp.trim.contigs.good.unique.good.filter.pick.pick.denovo.vsearch.pick.count_table, taxonomy=shrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.gsrdb.wang.pick.taxonomy, prefix=final_med)\n\n\nOnce you have the script and data you simply run the pipeline like so.\n\nmothur med_batchfile_processing\n\nOnce the mothur portion of the workflow is complete, the script mothur2oligo.sh needs to be run in the mothur environment and modified for your specific purposes. You should not need to modify the associated renamer.pl script but it does need to be in the same location as mothur2oligo.sh.\n\nbash mothur2oligo.sh\n\n\n\nmothur2oligo\nrenamer\n\n\n\n\nExpand for the mothur2oligo.sh scriptconda activate mothur\n# USAGE: sh mothur2oligo.sh\n# This is a shell script for transforming mothur output to appropriate format for \n# A. murat Eren's oligotyping pipeline \n\n## Set variables\n\n# Adjust the file names to your own study - these are the files from the mothur SOP\ntaxonomy=\"pipelineFiles_med/final_med.taxonomy\"\nfasta=\"pipelineFiles_med/final_med.fasta\"\ncount=\"pipelineFiles_med/final_med.count_table\"\nprocessors=28\n\n# Set the taxon you want to select, separate taxonomic levels with \";\" \n# Do not touch inner and outer quotes\ntaxon=\"'Bacteria;-Archaea;'\"\n\n\n################################\n########## Script  #############\n################################\n\nredundantFasta=$(echo ${fasta}.pick.redundant.fasta | sed 's/.fasta//')\ngroups=$(echo ${count}.pick.redundant.groups | sed 's/.count_table//') \n\n# Call mothur commands for generating deuniqued fasta file for a specific lineage\nmothur \"#set.current(processors=$processors); get.lineage(taxonomy=$taxonomy, taxon=$taxon, count=$count); list.seqs(count=current); get.seqs(accnos=current, fasta=$fasta); deunique.seqs(fasta=current, count=current)\"\n\n# Replace all \"_\" in fasta header with a \":\"\ncat $groups | sed 's/_/:/g' &gt; intermediate1\n# Make a file which maps sample names to sequence headers\npaste $groups intermediate1 | awk 'BEGIN{FS=\"\\t\"}{print $1\"\\t\"$2\"_\"$3}' &gt; intermediate2\n\n# Perl script to rename the headers of the fasta to include the sample name at the beginning followed by a \"_\"\nperl renamer.pl $redundantFasta intermediate2 \n\n\n\n\n\nand the companion renamer.pl scriptconda activate mothur\n#! /usr/bin/perl\n#from http://www.perlmonks.org/?node_id=975419\n\nuse strict;\nuse warnings;\n\n@ARGV == 2 or die \"usage: $0 &lt;multifasta file&gt; &lt;header replacement fil\n+e&gt;\\n\";\n\nmy ( $fasta_file, $header_file ) = @ARGV;\nmy $destination = $fasta_file . '_headers-replaced.fasta';\n\nopen IN2, '&lt;', $header_file or die \"Can't read from tab-delimited head\n+er replacement file $header_file: $!\\n\";\n\nmy %head_seqs;\nwhile ( &lt;IN2&gt; ) {\n    chomp;\n    my ( $old, $new ) = split /\\t/;\n    $head_seqs{ $old } = $new;\n    }\nclose IN2;\n\nopen IN1, '&lt;', $fasta_file or die \"Can't read from multifasta file wit\n+h alternating lines of headers and sequences $fasta_file: $!\\n\";\n\nopen OUT, '&gt;', $destination or die \"Can't write to file $destination: \n+$!\\n\";    \n\nwhile ( &lt;IN1&gt; ) {\n    if ( /^&gt;(.+)$/ && exists $head_seqs{ $1 } ) {\n        $_ = \"&gt;$head_seqs{ $1 }\\n\";\n        }\n    print OUT $_;\n    }\nclose IN1;\nclose OUT;\n\n\n\n\n\nGreat. Now within the oligotype/MED environment run the following commands for the MED analysis. You will need the mapping.txt file linked above for this step.\n\nconda activate oligotyping\no-trim-uninformative-columns-from-alignment \\\n        final_med.pick.redundant.fasta_headers-replaced.fasta\n\ndecompose final_med.pick.redundant.fasta_headers-replaced.fasta-TRIMMED \\\n        -E mapping.txt \\\n        --output-directory MED \\\n        --number-of-threads 24 \\\n        --skip-gen-figures\n\nIn the resources listed above, we include a table that summarizes read changes for each sample through the pipeline."
  },
  {
    "objectID": "docs/data/index.html",
    "href": "docs/data/index.html",
    "title": "Data & Scripts",
    "section": "",
    "text": "Quick access to pipeline processing scripts and raw data. With these scripts and associated data you can run the processing steps for each analysis.\nAll sequence data is linked to projects on the European Nucleotide Archive (ENA). The scripts and associated files can be downloaded as .zip archives by clicking the links. Or if you prefer, you can copy the scripts directly from the code blocks below.",
    "crumbs": [
      "Data & Scripts"
    ]
  },
  {
    "objectID": "docs/data/index.html#asv-data-and-scripts",
    "href": "docs/data/index.html#asv-data-and-scripts",
    "title": "Data & Scripts",
    "section": "ASV Data and Scripts",
    "text": "ASV Data and Scripts\n\n\n\n\nName\n\n\nDescription\n\n\n\n\n\nSequence data\n\n\nTrimmed (primers removed) 16S rRNA data from 1809 samples for ASV analysis \n\n\n\n\nFastq rename tables\n\n\nLookup tables for renaming fastq files prior to analysis \n\n\n\n\nSample data\n\n\n16S rRNA sample data \n\n\n\n\nProcessing scripts\n\n\nIndividual run and merge run R scripts for ASV analysis with \n\n\n\n\nHydra job scripts\n\n\nHydra job scripts \n\n\n\n\nPipeline read changes\n\n\nTable showing read changes for each sample through the processing pipeline \n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Data & Scripts"
    ]
  },
  {
    "objectID": "docs/data/index.html#asv-processing",
    "href": "docs/data/index.html#asv-processing",
    "title": "Data & Scripts",
    "section": "ASV Processing",
    "text": "ASV Processing\nIndividual Runs\nProcessing scripts for ASV analysis of individual sequencing runs using dada2. In total, 16S rRNA sequencing was performed on 6 sequencing runs. In the first workflow of the pipeline, runs are processed separately for error rates, dereplication, and ASV inference. At the end of each workflow, forward and reverse reads are merged.\n\n\nBCS_26\nBCS_28\nBCS_29\nBCS_30\nBCS_34\nBCS_35\n\n\n\n\nProcessing script for run BCS_26################################################################\n#####           TRANSISTHMIAN SHRIMP MICROBIOME            #####\n#####                 MISEQ RUN: BCS_26                    #####\n#####            PLATES ISTHMO S5, S5, S7, S8              #####\n################################################################\n#\n# SCRIPT prepared by Matthieu Leray\n# last modified January 28th 2023\n#\n# READ PROCESSING\n#\n# Modified by Jarrod Scott\n# last modified September 11 2024\n################################################################\n\nlibrary(dada2)\nlibrary(tidyverse)\nlibrary(ff)\nlibrary(phyloseq)\nlibrary(gridExtra)\nlibrary(dplyr)\nlibrary(decontam)\n\n##Creating filepaths to data \npath &lt;- \"RAW_DATA/BCS_26\"\nhead(list.files(path)) #eventually to check if the path works\n\n##File preparation\n#extracting Forward (fnFs) and Reverse (fnRs) reads from files\nfnFs &lt;- sort(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\nfnRs &lt;- sort(list.files(path, pattern = \"_R2_001.trimmed.fastq\"))\nsample.names &lt;- sapply(strsplit(fnFs, \"_R1_\"), `[`, 1)\nfnFs &lt;-file.path(path, fnFs)\nfnRs &lt;-file.path(path, fnRs)\n\nx &lt;- length(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\n\nqprofile_fwd &lt;- plotQualityProfile(fnFs[1:x], aggregate = TRUE, n = 20000)\nqprofile_rev &lt;- plotQualityProfile(fnRs[1:x], aggregate = TRUE, n = 20000)\n\nqprofile &lt;- grid.arrange(qprofile_fwd, qprofile_rev, nrow = 1)\nggsave(\"figures/BCS_26_filt_plot_qscores.png\", qprofile, width = 7, height = 3)\n\n#placing filtered files in a new filtered subdirectory\nfiltFs &lt;- file.path(path, \"filtered\", paste0(sample.names, \"_F_filt.fastq\"))\nfiltRs &lt;- file.path(path, \"filtered\", paste0(sample.names, \"_R_filt.fastq\"))\nnames(filtFs) &lt;- sample.names\nnames(filtRs) &lt;- sample.names\n\n#filtering and trimming, here truncation at 220 (Fwd) and 180 (Rev) bp, \n#2expected errors max (N discarded automatically)\n\nout &lt;- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(220,180),\n                        maxN=0, maxEE=2, truncQ=2, rm.phix=TRUE,\n                        compress=TRUE, multithread=20)\n\nhead(out)\n\n#learning error rates\nerrF &lt;- learnErrors(filtFs, multithread = TRUE)\nerrR &lt;- learnErrors(filtRs, multithread = TRUE)\n#plotting errors\n#plotErrors(errF, nominalQ=TRUE)\n#plotErrors(errR, nominalQ=TRUE)\n\n## ----plot_errF------------------------------\np3 &lt;- plotErrors(errF, nominalQ = TRUE)\nggsave(\"figures/BCS_26_plot_errorF_1.png\", p3, width = 7, height = 5)\nggsave(\"figures/BCS_26_plot_errorF_2.png\", p3)\n## ----plot_errR------------------------------\np4 &lt;- plotErrors(errR, nominalQ = TRUE)\nggsave(\"figures/BCS_26_plot_errorR_1.png\", p4, width = 7, height = 5)\nggsave(\"figures/BCS_26_plot_errorR_2.png\", p4)\n\n##Dereplicating reads\nsam.names &lt;- sapply(strsplit(basename(filtFs), \"_F_\"), `[`, 1)\nderepFs &lt;- derepFastq(filtFs)\nnames(derepFs) &lt;- sam.names\nderepRs &lt;- derepFastq(filtRs)\nnames(derepRs) &lt;- sam.names\n\n##Infering Sequence Variants\ndadaFs &lt;- dada(derepFs, err = errF, pool = \"pseudo\", multithread = TRUE)\ndadaFs[[1]]\ndadaRs &lt;- dada(derepRs, err = errR, pool = \"pseudo\", multithread = TRUE)\ndadaRs[[1]]\n\n##Merging paired ends\nmergers &lt;- mergePairs(dadaFs, derepFs, dadaRs, derepRs)\nBCS_26 &lt;- makeSequenceTable(mergers)\ndim(BCS_26)\n# [1]   384 29481\ntable(nchar(getSequences(BCS_26)))\n\n#exporting files to use in the next part of the workflow\nsaveRDS(BCS_26, \"BCS_26/BCS_26.rds\")\n\n#tracking changes through each step\ngetN &lt;- function(x) sum(getUniques(x))\ntrack &lt;-    cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN),\n                  sapply(mergers, getN))\ncolnames(track) &lt;- c(\"input\", \"filtered\", \"denoisedF\", \"denoisedR\",\n                        \"merged\")\nrownames(track) &lt;- sam.names\nwrite.table(track, \"BCS_26/BCS_26_read_changes.txt\", sep = \"\\t\", quote = FALSE,\n            col.names=NA)\n\nread_length &lt;-  data.frame(nchar(getSequences(BCS_26)))\n\ncolnames(read_length) &lt;- \"length\"\n\nplot_BCS_26 &lt;- qplot(length, data = read_length, \n                     geom = \"histogram\", binwidth = 1, \n                     xlab = \"read length\", \n                     ylab = \"total variants\", \n                     xlim = c(225,275)) \nggsave(\"figures/read_length_before_pseudo_BCS_26.png\", plot_BCS_26, width = 7, height = 3)\n\nsave.image(\"BCS_26.rdata\")\n\n\n\n\n\nProcessing script for run BCS_28################################################################\n#####           TRANSISTHMIAN SHRIMP MICROBIOME            #####\n#####                 MISEQ RUN: BCS_28                    #####\n#####               PLATES ISTHMO S3, S4                   #####\n################################################################\n#\n# SCRIPT prepared by Matthieu Leray\n# last modified January 28th 2023\n#\n# READ PROCESSING\n#\n# Modified by Jarrod Scott\n# last modified September 11 2024\n################################################################\n\nlibrary(dada2)\nlibrary(tidyverse)\nlibrary(ff)\nlibrary(phyloseq)\nlibrary(gridExtra)\nlibrary(dplyr)\nlibrary(decontam)\n\n##Creating filepaths to data \npath &lt;- \"RAW_DATA/BCS_28\"\nhead(list.files(path)) #eventually to check if the path works\n\n##File preparation\n#extracting Forward (fnFs) and Reverse (fnRs) reads from files\nfnFs &lt;- sort(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\nfnRs &lt;- sort(list.files(path, pattern = \"_R2_001.trimmed.fastq\"))\nsample.names &lt;- sapply(strsplit(fnFs, \"_R1_\"), `[`, 1)\nfnFs &lt;-file.path(path, fnFs)\nfnRs &lt;-file.path(path, fnRs)\n\nx &lt;- length(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\n\nqprofile_fwd &lt;- plotQualityProfile(fnFs[1:x], aggregate = TRUE)\nqprofile_rev &lt;- plotQualityProfile(fnRs[1:x], aggregate = TRUE)\n\nqprofile &lt;- grid.arrange(qprofile_fwd, qprofile_rev, nrow = 1)\nggsave(\"figures/BCS_28_filt_plot_qscores.png\", qprofile, width = 7, height = 3)\n\n#placing filtered files in a new filtered subdirectory\nfiltFs &lt;- file.path(path, \"filtered\", paste0(sample.names, \"_F_filt.fastq\"))\nfiltRs &lt;- file.path(path, \"filtered\", paste0(sample.names, \"_R_filt.fastq\"))\nnames(filtFs) &lt;- sample.names\nnames(filtRs) &lt;- sample.names\n\n# filtering and trimming, here truncation at 220 (Fwd) and 180 (Rev) bp, \n# 2expected errors max (N discarded automatically)\n\nout &lt;- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(220,180),\n                        maxN=0, maxEE=2, truncQ=2, rm.phix=TRUE,\n                        compress=TRUE, multithread=20)\n\nhead(out)\n\n#learning error rates\nerrF &lt;- learnErrors(filtFs, multithread = TRUE)\nerrR &lt;- learnErrors(filtRs, multithread = TRUE)\n\n## ----plot_errF------------------------------\np3 &lt;- plotErrors(errF, nominalQ = TRUE)\nggsave(\"figures/BCS_28_plot_errorF_1.png\", p3, width = 7, height = 5)\nggsave(\"figures/BCS_28_plot_errorF_2.png\", p3)\n## ----plot_errR------------------------------\np4 &lt;- plotErrors(errR, nominalQ = TRUE)\nggsave(\"figures/BCS_28_plot_errorR_1.png\", p4, width = 7, height = 5)\nggsave(\"figures/BCS_28_plot_errorR_2.png\", p4)\n\n##Dereplicating reads\nsam.names &lt;- sapply(strsplit(basename(filtFs), \"_F_\"), `[`, 1)\nderepFs &lt;- derepFastq(filtFs)\nnames(derepFs) &lt;- sam.names\nderepRs &lt;- derepFastq(filtRs)\nnames(derepRs) &lt;- sam.names\n\n##Infering Sequence Variants\ndadaFs &lt;- dada(derepFs, err = errF, pool = \"pseudo\", multithread = TRUE)\ndadaFs[[1]]\ndadaRs &lt;- dada(derepRs, err = errR, pool = \"pseudo\", multithread = TRUE)\ndadaRs[[1]]\n\n##Merging paired ends\nmergers &lt;- mergePairs(dadaFs, derepFs, dadaRs, derepRs)\nBCS_28 &lt;- makeSequenceTable(mergers)\ndim(BCS_28)\n# [1]   384 29481\ntable(nchar(getSequences(BCS_28)))\n\n#exporting files to use in the next part of the workflow\nsaveRDS(BCS_28, \"BCS_28/BCS_28.rds\")\n\n#tracking changes through each step\ngetN &lt;- function(x) sum(getUniques(x))\ntrack &lt;-    cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN),\n                  sapply(mergers, getN))\ncolnames(track) &lt;- c(\"input\", \"filtered\", \"denoisedF\", \"denoisedR\",\n                        \"merged\")\nrownames(track) &lt;- sam.names\nwrite.table(track, \"BCS_28/BCS_28_read_changes.txt\", sep = \"\\t\", quote = FALSE,\n            col.names=NA)\n\nread_length &lt;-  data.frame(nchar(getSequences(BCS_28)))\n\ncolnames(read_length) &lt;- \"length\"\n\nplot_BCS_28 &lt;- qplot(length, data = read_length, \n                     geom = \"histogram\", binwidth = 1, \n                     xlab = \"read length\", \n                     ylab = \"total variants\", \n                     xlim = c(225,275)) \nggsave(\"figures/read_length_before_pseudo_BCS_28.png\", plot_BCS_28, width = 7, height = 3)\n\nsave.image(\"BCS_28.rdata\")\n\n\n\n\n\nProcessing script for run BCS_29################################################################\n#####           TRANSISTHMIAN SHRIMP MICROBIOME            #####\n#####                 MISEQ RUN: BCS_29                    #####\n#####          PLATES ISTHMO S13, S14, S15, S16            #####\n################################################################\n#\n# SCRIPT prepared by Matthieu Leray\n# last modified January 28th 2023\n#\n# READ PROCESSING\n#\n# Modified by Jarrod Scott\n# last modified September 11 2024\n################################################################\n\nlibrary(dada2)\nlibrary(tidyverse)\nlibrary(ff)\nlibrary(phyloseq)\nlibrary(gridExtra)\nlibrary(dplyr)\nlibrary(decontam)\n\n##Creating filepaths to data \npath &lt;- \"RAW_DATA/BCS_29\"\nhead(list.files(path)) #eventually to check if the path works\n\n##File preparation\n#extracting Forward (fnFs) and Reverse (fnRs) reads from files\nfnFs &lt;- sort(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\nfnRs &lt;- sort(list.files(path, pattern = \"_R2_001.trimmed.fastq\"))\nsample.names &lt;- sapply(strsplit(fnFs, \"_R1_\"), `[`, 1)\nfnFs &lt;-file.path(path, fnFs)\nfnRs &lt;-file.path(path, fnRs)\n\nx &lt;- length(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\n\nqprofile_fwd &lt;- plotQualityProfile(fnFs[1:x], aggregate = TRUE)\nqprofile_rev &lt;- plotQualityProfile(fnRs[1:x], aggregate = TRUE)\n\nqprofile &lt;- grid.arrange(qprofile_fwd, qprofile_rev, nrow = 1)\nggsave(\"figures/BCS_29_filt_plot_qscores.png\", qprofile, width = 7, height = 3)\n\n#placing filtered files in a new filtered subdirectory\nfiltFs &lt;- file.path(path, \"filtered\", paste0(sample.names, \"_F_filt.fastq\"))\nfiltRs &lt;- file.path(path, \"filtered\", paste0(sample.names, \"_R_filt.fastq\"))\nnames(filtFs) &lt;- sample.names\nnames(filtRs) &lt;- sample.names\n\n#filtering and trimming, here truncation at 220 (Fwd) and 180 (Rev) bp, \n#2expected errors max (N discarded automatically)\n\nout &lt;- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(220,180),\n                        maxN=0, maxEE=2, truncQ=2, rm.phix=TRUE,\n                        compress=TRUE, multithread=20)\n\nhead(out)\n\n#learning error rates\nerrF &lt;- learnErrors(filtFs, multithread = TRUE)\nerrR &lt;- learnErrors(filtRs, multithread = TRUE)\n\n## ----plot_errF------------------------------\np3 &lt;- plotErrors(errF, nominalQ = TRUE)\nggsave(\"figures/BCS_29_plot_errorF_1.png\", p3, width = 7, height = 5)\nggsave(\"figures/BCS_29_plot_errorF_2.png\", p3)\n## ----plot_errR------------------------------\np4 &lt;- plotErrors(errR, nominalQ = TRUE)\nggsave(\"figures/BCS_29_plot_errorR_1.png\", p4, width = 7, height = 5)\nggsave(\"figures/BCS_29_plot_errorR_2.png\", p4)\n\n##Dereplicating reads\nsam.names &lt;- sapply(strsplit(basename(filtFs), \"_F_\"), `[`, 1)\nderepFs &lt;- derepFastq(filtFs)\nnames(derepFs) &lt;- sam.names\nderepRs &lt;- derepFastq(filtRs)\nnames(derepRs) &lt;- sam.names\n\n##Infering Sequence Variants\ndadaFs &lt;- dada(derepFs, err = errF, pool = \"pseudo\", multithread = TRUE)\ndadaFs[[1]]\ndadaRs &lt;- dada(derepRs, err = errR, pool = \"pseudo\", multithread = TRUE)\ndadaRs[[1]]\n\n##Merging paired ends\nmergers &lt;- mergePairs(dadaFs, derepFs, dadaRs, derepRs)\nBCS_29 &lt;- makeSequenceTable(mergers)\ndim(BCS_29)\n# [1]   384 29481\ntable(nchar(getSequences(BCS_29)))\n\n#exporting files to use in the next part of the workflow\nsaveRDS(BCS_29, \"BCS_29/BCS_29.rds\")\n\n#tracking changes through each step\ngetN &lt;- function(x) sum(getUniques(x))\ntrack &lt;-    cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN),\n                  sapply(mergers, getN))\ncolnames(track) &lt;- c(\"input\", \"filtered\", \"denoisedF\", \"denoisedR\",\n                        \"merged\")\nrownames(track) &lt;- sam.names\nwrite.table(track, \"BCS_29/BCS_29_read_changes.txt\", sep = \"\\t\", quote = FALSE,\n            col.names=NA)\n\nread_length &lt;-  data.frame(nchar(getSequences(BCS_29)))\n\ncolnames(read_length) &lt;- \"length\"\n\nplot_BCS_29 &lt;- qplot(length, data = read_length, \n                     geom = \"histogram\", binwidth = 1, \n                     xlab = \"read length\", \n                     ylab = \"total variants\", \n                     xlim = c(225,275)) \nggsave(\"figures/read_length_before_pseudo_BCS_29.png\", plot_BCS_29, width = 7, height = 3)\n\nsave.image(\"BCS_29.rdata\")\n\n\n\n\n\nProcessing script for run BCS_30################################################################\n#####           TRANSISTHMIAN SHRIMP MICROBIOME            #####\n#####                 MISEQ RUN: BCS_30                    #####\n#####          PLATES ISTHMO S17, S18, S19, S20            #####\n################################################################\n#\n# SCRIPT prepared by Matthieu Leray\n# last modified January 28th 2023\n#\n# READ PROCESSING\n#\n# Modified by Jarrod Scott\n# last modified September 11 2024\n################################################################\n\nlibrary(dada2)\nlibrary(tidyverse)\nlibrary(ff)\nlibrary(phyloseq)\nlibrary(gridExtra)\nlibrary(dplyr)\nlibrary(decontam)\n\n##Creating filepaths to data \npath &lt;- \"RAW_DATA/BCS_30\"\nhead(list.files(path)) #eventually to check if the path works\n\n##File preparation\n#extracting Forward (fnFs) and Reverse (fnRs) reads from files\nfnFs &lt;- sort(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\nfnRs &lt;- sort(list.files(path, pattern = \"_R2_001.trimmed.fastq\"))\nsample.names &lt;- sapply(strsplit(fnFs, \"_R1_\"), `[`, 1)\nfnFs &lt;-file.path(path, fnFs)\nfnRs &lt;-file.path(path, fnRs)\n\nx &lt;- length(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\n\nqprofile_fwd &lt;- plotQualityProfile(fnFs[1:x], aggregate = TRUE, n = 20000)\nqprofile_rev &lt;- plotQualityProfile(fnRs[1:x], aggregate = TRUE, n = 20000)\n\nqprofile &lt;- grid.arrange(qprofile_fwd, qprofile_rev, nrow = 1)\nggsave(\"figures/BCS_30_filt_plot_qscores.png\", qprofile, width = 7, height = 3)\n\n#placing filtered files in a new filtered subdirectory\nfiltFs &lt;- file.path(path, \"filtered\", paste0(sample.names, \"_F_filt.fastq\"))\nfiltRs &lt;- file.path(path, \"filtered\", paste0(sample.names, \"_R_filt.fastq\"))\nnames(filtFs) &lt;- sample.names\nnames(filtRs) &lt;- sample.names\n\n#filtering and trimming, here truncation at 220 (Fwd) and 180 (Rev) bp, \n#2expected errors max (N discarded automatically)\n\nout &lt;- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(220,180),\n                        maxN=0, maxEE=2, truncQ=2, rm.phix=TRUE,\n                        compress=TRUE, multithread=20)\n\nhead(out)\n\n#learning error rates\nerrF &lt;- learnErrors(filtFs, multithread = TRUE)\nerrR &lt;- learnErrors(filtRs, multithread = TRUE)\n\n## ----plot_errF------------------------------\np3 &lt;- plotErrors(errF, nominalQ = TRUE)\nggsave(\"figures/BCS_30_plot_errorF_1.png\", p3, width = 7, height = 5)\nggsave(\"figures/BCS_30_plot_errorF_2.png\", p3)\n## ----plot_errR------------------------------\np4 &lt;- plotErrors(errR, nominalQ = TRUE)\nggsave(\"figures/BCS_30_plot_errorR_1.png\", p4, width = 7, height = 5)\nggsave(\"figures/BCS_30_plot_errorR_2.png\", p4)\n\n##Dereplicating reads\nsam.names &lt;- sapply(strsplit(basename(filtFs), \"_F_\"), `[`, 1)\nderepFs &lt;- derepFastq(filtFs)\nnames(derepFs) &lt;- sam.names\nderepRs &lt;- derepFastq(filtRs)\nnames(derepRs) &lt;- sam.names\n\n##Infering Sequence Variants\ndadaFs &lt;- dada(derepFs, err = errF, pool = \"pseudo\", multithread = TRUE)\ndadaFs[[1]]\ndadaRs &lt;- dada(derepRs, err = errR, pool = \"pseudo\", multithread = TRUE)\ndadaRs[[1]]\n\n##Merging paired ends\nmergers &lt;- mergePairs(dadaFs, derepFs, dadaRs, derepRs)\nBCS_30 &lt;- makeSequenceTable(mergers)\ndim(BCS_30)\n# [1]   384 29481\ntable(nchar(getSequences(BCS_30)))\n\n#exporting files to use in the next part of the workflow\nsaveRDS(BCS_30, \"BCS_30/BCS_30.rds\")\n\n#tracking changes through each step\ngetN &lt;- function(x) sum(getUniques(x))\ntrack &lt;-    cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN),\n                  sapply(mergers, getN))\ncolnames(track) &lt;- c(\"input\", \"filtered\", \"denoisedF\", \"denoisedR\",\n                        \"merged\")\nrownames(track) &lt;- sam.names\nwrite.table(track, \"BCS_30/BCS_30_read_changes.txt\", sep = \"\\t\", quote = FALSE,\n            col.names=NA)\n\nread_length &lt;-  data.frame(nchar(getSequences(BCS_30)))\n\ncolnames(read_length) &lt;- \"length\"\n\nplot_BCS_30 &lt;- qplot(length, data = read_length, \n                     geom = \"histogram\", binwidth = 1, \n                     xlab = \"read length\", \n                     ylab = \"total variants\", \n                     xlim = c(225,275)) \nggsave(\"figures/read_length_before_pseudo_BCS_30.png\", plot_BCS_30, width = 7, height = 3)\n\nsave.image(\"BCS_30.rdata\")\n\n\n\n\n\nProcessing script for run BCS_34################################################################\n#####           TRANSISTHMIAN SHRIMP MICROBIOME            #####\n#####                 MISEQ RUN: BCS_34                    #####\n#####               PLATES ISTHMO S01, S02                 #####\n################################################################\n#\n# SCRIPT prepared by Matthieu Leray\n# last modified January 28th 2023\n#\n# READ PROCESSING\n#\n# Modified by Jarrod Scott\n# last modified September 11 2024\n################################################################\n\nlibrary(dada2)\nlibrary(tidyverse)\nlibrary(ff)\nlibrary(phyloseq)\nlibrary(gridExtra)\nlibrary(dplyr)\nlibrary(decontam)\n\n##Creating filepaths to data \npath &lt;- \"RAW_DATA/BCS_34\"\nhead(list.files(path)) #eventually to check if the path works\n\n##File preparation\n#extracting Forward (fnFs) and Reverse (fnRs) reads from files\nfnFs &lt;- sort(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\nfnRs &lt;- sort(list.files(path, pattern = \"_R2_001.trimmed.fastq\"))\nsample.names &lt;- sapply(strsplit(fnFs, \"_R1_\"), `[`, 1)\nfnFs &lt;-file.path(path, fnFs)\nfnRs &lt;-file.path(path, fnRs)\n\nx &lt;- length(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\n\nqprofile_fwd &lt;- plotQualityProfile(fnFs[1:x], aggregate = TRUE)\nqprofile_rev &lt;- plotQualityProfile(fnRs[1:x], aggregate = TRUE)\n\nqprofile &lt;- grid.arrange(qprofile_fwd, qprofile_rev, nrow = 1)\nggsave(\"figures/BCS_34_filt_plot_qscores.png\", qprofile, width = 7, height = 3)\n\n#placing filtered files in a new filtered subdirectory\nfiltFs &lt;- file.path(path, \"filtered\", paste0(sample.names, \"_F_filt.fastq\"))\nfiltRs &lt;- file.path(path, \"filtered\", paste0(sample.names, \"_R_filt.fastq\"))\nnames(filtFs) &lt;- sample.names\nnames(filtRs) &lt;- sample.names\n\n#filtering and trimming, here truncation at 220 (Fwd) and 180 (Rev) bp, \n#2expected errors max (N discarded automatically)\n\nout &lt;- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(220,180),\n                        maxN=0, maxEE=2, truncQ=2, rm.phix=TRUE,\n                        compress=TRUE, multithread=20)\n\nhead(out)\n\n#learning error rates\nerrF &lt;- learnErrors(filtFs, multithread = TRUE)\nerrR &lt;- learnErrors(filtRs, multithread = TRUE)\n\n## ----plot_errF------------------------------\np3 &lt;- plotErrors(errF, nominalQ = TRUE)\nggsave(\"figures/BCS_34_plot_errorF_1.png\", p3, width = 7, height = 5)\nggsave(\"figures/BCS_34_plot_errorF_2.png\", p3)\n## ----plot_errR------------------------------\np4 &lt;- plotErrors(errR, nominalQ = TRUE)\nggsave(\"figures/BCS_34_plot_errorR_1.png\", p4, width = 7, height = 5)\nggsave(\"figures/BCS_34_plot_errorR_2.png\", p4)\n\n##Dereplicating reads\nsam.names &lt;- sapply(strsplit(basename(filtFs), \"_F_\"), `[`, 1)\nderepFs &lt;- derepFastq(filtFs)\nnames(derepFs) &lt;- sam.names\nderepRs &lt;- derepFastq(filtRs)\nnames(derepRs) &lt;- sam.names\n\n##Infering Sequence Variants\ndadaFs &lt;- dada(derepFs, err = errF, pool = \"pseudo\", multithread = TRUE)\ndadaFs[[1]]\ndadaRs &lt;- dada(derepRs, err = errR, pool = \"pseudo\", multithread = TRUE)\ndadaRs[[1]]\n\n##Merging paired ends\nmergers &lt;- mergePairs(dadaFs, derepFs, dadaRs, derepRs)\nBCS_34 &lt;- makeSequenceTable(mergers)\ndim(BCS_34)\n# [1]   384 29481\ntable(nchar(getSequences(BCS_34)))\n\n#exporting files to use in the next part of the workflow\nsaveRDS(BCS_34, \"BCS_34/BCS_34.rds\")\n\n#tracking changes through each step\ngetN &lt;- function(x) sum(getUniques(x))\ntrack &lt;-    cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN),\n                  sapply(mergers, getN))\ncolnames(track) &lt;- c(\"input\", \"filtered\", \"denoisedF\", \"denoisedR\",\n                        \"merged\")\nrownames(track) &lt;- sam.names\nwrite.table(track, \"BCS_34/BCS_34_read_changes.txt\", sep = \"\\t\", quote = FALSE,\n            col.names=NA)\n\nread_length &lt;-  data.frame(nchar(getSequences(BCS_34)))\n\ncolnames(read_length) &lt;- \"length\"\n\nplot_BCS_34 &lt;- qplot(length, data = read_length, \n                      geom = \"histogram\", binwidth = 1, \n                      xlab = \"read length\", \n                      ylab = \"total variants\", \n                      xlim = c(225,275)) \nggsave(\"figures/read_length_before_pseudo_BCS_34.png\", plot_BCS_34, width = 7, height = 3)\n\nsave.image(\"BCS_34.rdata\")\n\n\n\n\n\nProcessing script for run BCS_35################################################################\n#####           TRANSISTHMIAN SHRIMP MICROBIOME            #####\n#####                 MISEQ RUN: BCS_35                    #####\n#####           PLATES ISTHMO S9, S10, S11, S12            #####\n################################################################\n#\n# SCRIPT prepared by Matthieu Leray\n# last modified January 28th 2023\n#\n# READ PROCESSING\n#\n# Modified by Jarrod Scott\n# last modified September 11 2024\n################################################################\n\nlibrary(dada2)\nlibrary(tidyverse)\nlibrary(ff)\nlibrary(phyloseq)\nlibrary(gridExtra)\nlibrary(dplyr)\nlibrary(decontam)\n\n##Creating filepaths to data \npath &lt;- \"RAW_DATA/BCS_35\"\nhead(list.files(path)) #eventually to check if the path works\n\n##File preparation\n#extracting Forward (fnFs) and Reverse (fnRs) reads from files\nfnFs &lt;- sort(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\nfnRs &lt;- sort(list.files(path, pattern = \"_R2_001.trimmed.fastq\"))\nsample.names &lt;- sapply(strsplit(fnFs, \"_R1_\"), `[`, 1)\nfnFs &lt;-file.path(path, fnFs)\nfnRs &lt;-file.path(path, fnRs)\n\nx &lt;- length(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\n\nqprofile_fwd &lt;- plotQualityProfile(fnFs[1:x], aggregate = TRUE)\nqprofile_rev &lt;- plotQualityProfile(fnRs[1:x], aggregate = TRUE)\n\nqprofile &lt;- grid.arrange(qprofile_fwd, qprofile_rev, nrow = 1)\nggsave(\"figures/BCS_35_filt_plot_qscores.png\", qprofile, width = 7, height = 3)\n\n#placing filtered files in a new filtered subdirectory\nfiltFs &lt;- file.path(path, \"filtered\", paste0(sample.names, \"_F_filt.fastq\"))\nfiltRs &lt;- file.path(path, \"filtered\", paste0(sample.names, \"_R_filt.fastq\"))\nnames(filtFs) &lt;- sample.names\nnames(filtRs) &lt;- sample.names\n\n#filtering and trimming, here truncation at 220 (Fwd) and 180 (Rev) bp, \n#2expected errors max (N discarded automatically)\n\nout &lt;- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(220,180),\n                        maxN=0, maxEE=2, truncQ=2, rm.phix=TRUE,\n                        compress=TRUE, multithread=20)\n\nhead(out)\n\n#learning error rates\nerrF &lt;- learnErrors(filtFs, multithread = TRUE)\nerrR &lt;- learnErrors(filtRs, multithread = TRUE)\n\n## ----plot_errF------------------------------\np3 &lt;- plotErrors(errF, nominalQ = TRUE)\nggsave(\"figures/BCS_35_plot_errorF_1.png\", p3, width = 7, height = 5)\nggsave(\"figures/BCS_35_plot_errorF_2.png\", p3)\n## ----plot_errR------------------------------\np4 &lt;- plotErrors(errR, nominalQ = TRUE)\nggsave(\"figures/BCS_35_plot_errorR_1.png\", p4, width = 7, height = 5)\nggsave(\"figures/BCS_35_plot_errorR_2.png\", p4)\n\n##Dereplicating reads\nsam.names &lt;- sapply(strsplit(basename(filtFs), \"_F_\"), `[`, 1)\nderepFs &lt;- derepFastq(filtFs)\nnames(derepFs) &lt;- sam.names\nderepRs &lt;- derepFastq(filtRs)\nnames(derepRs) &lt;- sam.names\n\n##Infering Sequence Variants\ndadaFs &lt;- dada(derepFs, err = errF, pool = \"pseudo\", multithread = TRUE)\ndadaFs[[1]]\ndadaRs &lt;- dada(derepRs, err = errR, pool = \"pseudo\", multithread = TRUE)\ndadaRs[[1]]\n\n##Merging paired ends\nmergers &lt;- mergePairs(dadaFs, derepFs, dadaRs, derepRs)\nBCS_35 &lt;- makeSequenceTable(mergers)\ndim(BCS_35)\n# [1]   384 29481\ntable(nchar(getSequences(BCS_35)))\n\n#exporting files to use in the next part of the workflow\nsaveRDS(BCS_35, \"BCS_35/BCS_35.rds\")\n\n#tracking changes through each step\ngetN &lt;- function(x) sum(getUniques(x))\ntrack &lt;-    cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN),\n                  sapply(mergers, getN))\ncolnames(track) &lt;- c(\"input\", \"filtered\", \"denoisedF\", \"denoisedR\",\n                        \"merged\")\nrownames(track) &lt;- sam.names\nwrite.table(track, \"BCS_35/BCS_35_read_changes.txt\", sep = \"\\t\", quote = FALSE,\n            col.names=NA)\n\nread_length &lt;-  data.frame(nchar(getSequences(BCS_35)))\n\ncolnames(read_length) &lt;- \"length\"\n\nplot_BCS_35 &lt;- qplot(length, data = read_length, \n                     geom = \"histogram\", binwidth = 1, \n                     xlab = \"read length\", \n                     ylab = \"total variants\", \n                     xlim = c(225,275)) \nggsave(\"figures/read_length_before_pseudo_BCS_35.png\", plot_BCS_35, width = 7, height = 3)\n\nsave.image(\"BCS_35.rdata\")\n\n\n\n\n\n\nconda activate R\nRscript BCS_26.R\nRscript BCS_28.R\nRscript BCS_29.R\nRscript BCS_30.R\nRscript BCS_34.R\nRscript BCS_35.R\n\nMerged Runs\nOnce these workflows finish, we then merge the 6 sequence tables together and proceed with chimera removal and taxonomic classification.\n\nProcessing script for run merged runs#!/usr/bin/env Rscript\nset.seed(919191)\nlibrary(dada2); packageVersion(\"dada2\")\nlibrary(ggplot2)\nlibrary(ff)\nlibrary(phyloseq)\nlibrary(gridExtra)\nlibrary(dplyr)\nlibrary(decontam)\nlibrary(grid)\nlibrary(ShortRead); packageVersion(\"ShortRead\")\nlibrary(Biostrings); packageVersion(\"Biostrings\")\nlibrary(DECIPHER); packageVersion(\"DECIPHER\")\n\n########################################\n#\n# 2. MERGE ALL SEQ TABS\n#\n########################################\n\n############################################\n# PROBLEM: Duplicated sample names \n# detected in the sequence table row names: \n# 7512-G, 7512-H, 7512-M, 7512-S\n# BCS_30 and BCS_35\n# FOR BCS_30 changed name to 7512A...\n# FOR BCS_35 changed name to 7512B...\n############################################\n\nBCS_26 &lt;- readRDS(\"BCS_26/BCS_26.rds\")\nBCS_28 &lt;- readRDS(\"BCS_28/BCS_28.rds\")\nBCS_29 &lt;- readRDS(\"BCS_29/BCS_29.rds\")\nBCS_30 &lt;- readRDS(\"BCS_30/BCS_30.rds\")\nBCS_34 &lt;- readRDS(\"BCS_34/BCS_34.rds\")\nBCS_35 &lt;- readRDS(\"BCS_35/BCS_35.rds\")\n\nseqtab.merge &lt;- mergeSequenceTables(BCS_26, BCS_28, BCS_29, \n                                    BCS_30, BCS_34, BCS_35\n                                    )\ndim(seqtab.merge)\ntable(nchar(getSequences(seqtab.merge)))\n\nread_length_all &lt;-  data.frame(nchar(getSequences(seqtab.merge)))\ncolnames(read_length_all) &lt;- \"length\"\nplot_all &lt;- qplot(length, data = read_length_all, \n                  geom = \"histogram\", binwidth = 1, \n                  xlab = \"read length\", \n                  ylab = \"total variants\", \n                  xlim = c(200,400)) \nggsave(\"figures/read_length_before_collapse.png\", \n        plot_all, width = 7, height = 3)\nsaveRDS(seqtab.merge, \"2.seqtab.merge.rds\")\n\nsave.image(\"rdata/2.merge.seqtabs.rdata\")\n\n########################################\n#\n# collapseNoMismatch\n# TESTED, only minor differences in\n# 13 samples. Takes long time to run\n#\n########################################\n\n# seqtab_to_collapse &lt;- collapseNoMismatch(st_all, minOverlap = 20, orderBy = \"abundance\",\n#   identicalOnly = FALSE, vec = TRUE, band = -1, verbose = TRUE)\n\n# dim(seqtab_to_collapse)\n# table(nchar(getSequences(seqtab_to_collapse)))\n\n# read_length_all_collapse &lt;-  data.frame(nchar(getSequences(seqtab_to_collapse)))\n# colnames(read_length_all_collapse) &lt;- \"length\"\n# plot_all_collapse &lt;- qplot(length, data = read_length_all_collapse, geom = \"histogram\", binwidth = 1, xlab = \"read length\", ylab = \"total variants\", xlim = c(200,400)) \n# ggsave(\"figures/read_length_after_collapse.png\", plot_all_collapse, width = 7, height = 3)\n# saveRDS(seqtab_to_collapse, \"seqtab_after_collapse.rds\")\n\n# save.image(\"rdata/2_merge_seqtabs_collapsed.rdata\")\n\n########################################\n#\n# 3. REMOVING CHIMERAS\n#\n########################################\n\n############################################\n# PROBLEM: Duplicated sample names \n# detected in the sequence table row names: \n# 7512-G, 7512-H, 7512-M, 7512-S\n# BCS_30 and BCS_35\n# FOR BCS_30 changed name to 7512A...\n# FOR BCS_35 changed name to 7512B...\n############################################\n\n## REMVOE OUTLIER READ LENGTHS\nseqtab &lt;- seqtab.merge\n#seqtab.merge &lt;- readRDS(\"seqtab_before_collapse.rds\")\ntable(nchar(getSequences(seqtab)))\n\n################################################################################# \n## \n## 220   221   222   223   224   225   226   227   228   229   230   231   232\n##   125    67    14    36    20    13    10    25     9     6     4    27     2\n##   234   235   236   237   238   239   240   241   242   243   244   245   246\n##     9     8  1373   151    46     6    99   407   298   452    31    14    13\n##   247   248   249   250   251   252   253   254   255   256   257   258   259\n##    26    23    19    49   159  3587 84485  3772   319   123    96    20    10\n##   260   261   262   263   264   265   266   267   268   269   270   271   272\n##     8    16     9     4     2     1     1     1     4     2     9     8     4\n##   273   274   275   276   277   278   279   280   281   282   284   285   286\n##     7     3     2     5     1     7     4     1     1     2     1     4     4\n##   288   289   290   291   292   293   294   295   296   297   298   300   303\n##     1     3     1     2     4     8     7     2     3     2     3     2     3\n##   304   305   307   308   309   310   311   312   313   315   316   317   318\n##     1     5     2     3     2     1     3     1     3     1     4     1     3\n##   319   320   321   322   323   324   325   326   328   329   330   332   333\n##     3     1     2     3     2     1     3     1     3     3     2     1     3\n##   334   335   336   337   338   339   340   341   342   343   344   345   346\n##    13     6     7    18     5    25    16    70    52     8     7     8     4\n##   347   348   349   350   351   352   353   354   355   356   357   358   359\n##    25    17    21    10     2    11     1     1     7     6    31     6    15\n##   360   361   362   363   364   365   366   367   368   369   370   371   372\n##    21   161   188    43   141   108    19     9    26     5     3     3     8\n##   373   374   376   377   378   379   380   384   385   386   387   388\n##    11     2     3     5     2     3     1     1     1     1     2     1\n##\n#################################################################################\n\n#######################################################\n## ----REMOVE OUTLIER READ LENGTHS------------------ ##\n#######################################################\n\nseqtab.trim &lt;- seqtab[,nchar(colnames(seqtab)) %in% seq(252, 254)]\ndim(seqtab.trim)\ntable(nchar(getSequences(seqtab.trim)))\n\n#####################\n##   252   253   254\n##  3587 84485  3772\n#####################\n\n#######################################################\n## ----chimera pooled------------------------------- ##\n#######################################################\n\nseqtab.trim.nochim.pool &lt;- \n          removeBimeraDenovo(seqtab.trim, \n                             method = \"pooled\", \n                             multithread = 20, \n                             verbose = TRUE)\ndim(seqtab.trim.nochim.pool)\nsum(seqtab.trim.nochim.pool)/sum(seqtab.trim)\n\nsaveRDS(seqtab.trim.nochim.pool, \"3.seqtab.trim.nochim.pool.rds\")\n\ntable(nchar(getSequences(seqtab.trim.nochim.pool)))\n\ntable(colSums(seqtab.trim.nochim.pool &gt; 0))\ntable(rowSums(seqtab.trim.nochim.pool &gt; 0))\n\n##########################################################\n## ----chimera consensus------------------------------- ##\n##########################################################\n\nseqtab.trim.nochim.consensus &lt;- \n           removeBimeraDenovo(seqtab.trim, \n                              method = \"consensus\", \n                              multithread = 20, \n                              verbose = TRUE)\ndim(seqtab.trim.nochim.consensus)\nsum(seqtab.trim.nochim.consensus)/sum(seqtab.trim)\n\nsaveRDS(seqtab.trim.nochim.consensus, \"3.seqtab.trim.nochim.consensus.rds\")\n\ntable(nchar(getSequences(seqtab.trim.nochim.consensus)))\n\ntable(colSums(seqtab.trim.nochim.consensus &gt; 0))\ntable(rowSums(seqtab.trim.nochim.consensus &gt; 0))\n\n##########################################################\n## ----tracking changes-------------------------------- ##\n##########################################################\n\ngetN &lt;- function(x) sum(getUniques(x))\ntrack &lt;- cbind(rowSums(seqtab), \n               rowSums(seqtab.trim), \n               rowSums(seqtab.trim.nochim.pool), \n               rowSums(seqtab.trim.nochim.consensus))\n\ncolnames(track) &lt;- c(\"merged\", \"trim\", \"chimera_pool\", \"chimera_concensus\")\nwrite.table(track, \"3.chimera_read_changes_pipeline.txt\", \n            sep = \"\\t\", quote = FALSE, col.names=NA)\n\nsave.image(\"rdata/3.trim.chimera.rdata\")\n\n########################################\n#\n# 4. ASSIGNING TAXONOMY\n#\n########################################\n\n###########################################################\n# reference datasets formatted for DADA2 can be found here: \n# https://benjjneb.github.io/dada2/training.html\n###########################################################\n\n########################################\n#\n# TAXONOMY chimera = pooled\n#\n########################################\n\n# seqtab &lt;- readRDS(\"3.seqtab.trim.nochim.pool.rds\")\n\n########################################\n# TAXONOMY = silva\n########################################\nseqtab.pool &lt;- seqtab.trim.nochim.pool\n\ntax_silva_v138.pool &lt;- \n              assignTaxonomy(seqtab.pool, \n              \"TAXONOMY_FILES/silva_nr99_v138.1_train_set.fa.gz\", \n              multithread = TRUE, verbose = TRUE)\nsaveRDS(tax_silva_v138.pool, \"4.tax_silva_v138.pool.rds\")\n\ntax_silva_v132.pool &lt;- \n              assignTaxonomy(seqtab.pool, \n              \"TAXONOMY_FILES/silva_nr_v132_train_set.fa.gz\", \n              multithread = TRUE, verbose = TRUE)\nsaveRDS(tax_silva_v132.pool, \"4.tax_silva_v132.pool.rds\")\n\n########################################\n# TAXONOMY = RDP\n########################################\n\ntax_rdp_v138.pool &lt;- \n             assignTaxonomy(seqtab.pool, \n             \"TAXONOMY_FILES/rdp_train_set_18.fa.gz\", \n             multithread = TRUE, verbose = TRUE)\nsaveRDS(tax_rdp_v138.pool, \"4.tax_rdp_v138.pool.rds\")\n\n########################################\n#\n# TAXONOMY chimera = consensus\n#\n########################################\n\n#remove(list = ls())\n#seqtab &lt;- readRDS(\"3.seqtab.trim.nochim.consensus.rds\")\n#objects()\n\n########################################\n# TAXONOMY = silva\n########################################\nseqtab.consensus &lt;- seqtab.trim.nochim.consensus\n\ntax_silva_v138.consensus &lt;- \n              assignTaxonomy(seqtab.consensus, \n              \"TAXONOMY_FILES/silva_nr99_v138.1_train_set.fa.gz\", \n              multithread = TRUE, verbose = TRUE)\nsaveRDS(tax_silva_v138.consensus, \"4.tax_silva_v138.consensus.rds\")\n\ntax_silva_v132.consensus &lt;- \n              assignTaxonomy(seqtab.consensus, \n              \"TAXONOMY_FILES/silva_nr_v132_train_set.fa.gz\", \n              multithread = TRUE, verbose = TRUE)\nsaveRDS(tax_silva_v132.consensus, \"4.tax_silva_v132.consensus.rds\")\n\n########################################\n# TAXONOMY = RDP\n########################################\n\ntax_rdp_v138.consensus &lt;- \n              assignTaxonomy(seqtab.consensus, \n              \"TAXONOMY_FILES/rdp_train_set_18.fa.gz\", \n              multithread = TRUE, verbose = TRUE)\nsaveRDS(tax_rdp_v138.consensus, \"4.tax_rdp_v138.consensus.rds\")\n\n########################################\n# TAXONOMY = ITGDB\n########################################\n\ntax_itgdb.consensus &lt;- \n              assignTaxonomy(seqtab.consensus, \n              \"TAXONOMY_FILES/itgdb_dada2.fa\", \n              multithread = TRUE, verbose = TRUE)\nsaveRDS(tax_itgdb.consensus, \"4.tax_itgdb.consensus.rds\")\n\n########################################\n# TAXONOMY = GSRDB\n########################################\n\ntax_gsrdb.consensus &lt;- \n              assignTaxonomy(seqtab.consensus, \n              \"TAXONOMY_FILES/gsrdb_dada2.fa\", \n              multithread = TRUE, verbose = TRUE)\nsaveRDS(tax_gsrdb.consensus, \"4.tax_gsrdb.consensus.rds\")\n\nsave.image(\"rdata/4.dada2.pipeline.rdata\")\n\nsessionInfo()\ndevtools::session_info()\n\nquit()\n\n\n\nconda activate R\nRscript 2.dada2_pipeline.R\n\nIn the resources listed above, we include a table that summarizes read changes for each sample through the pipeline.",
    "crumbs": [
      "Data & Scripts"
    ]
  },
  {
    "objectID": "docs/data/data-otu.html",
    "href": "docs/data/data-otu.html",
    "title": "Data & Scripts",
    "section": "",
    "text": "Quick access to pipeline processing scripts and raw data. With these scripts and associated data you can run the processing steps for each analysis.\nAll sequence data is linked to projects on the European Nucleotide Archive (ENA). The scripts and associated files can be downloaded as .zip archives by clicking the links. Or if you prefer, you can copy the scripts directly from the code blocks below."
  },
  {
    "objectID": "docs/data/data-otu.html#otu-data-and-scripts",
    "href": "docs/data/data-otu.html#otu-data-and-scripts",
    "title": "Data & Scripts",
    "section": "OTU Data and Scripts",
    "text": "OTU Data and Scripts\n\n\n\n\nName\n\n\nDescription\n\n\n\n\n\nSequence data\n\n\nTrimmed (primers removed) 16S rRNA data from 1809 samples for OTU analysis \n\n\n\n\nFastq rename tables\n\n\nLookup tables for renaming fastq files prior to analysis \n\n\n\n\nSample data\n\n\n16S rRNA sample data \n\n\n\n\nProcessing scripts\n\n\nmothur formatted batch files for OTU processing \n\n\n\n\nHydra job scripts\n\n\nHydra job scripts \n\n\n\n\nPipeline read changes\n\n\nTable showing read changes for each sample through the processing pipeline \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "docs/data/data-otu.html#otu-processing",
    "href": "docs/data/data-otu.html#otu-processing",
    "title": "Data & Scripts",
    "section": "OTU Processing",
    "text": "OTU Processing\nProcessing scripts for OTU analysis using mothur. All steps for processing are contained within a single mothur batchfile.\nScripts\n\nProcessing script for mothur OTU analysisset.dir(output=pipelineFiles/)\nmake.file(inputdir=01_TRIMMED_DATA/, type=fastq, prefix=shrimp)\n\nmake.contigs(file=DATA/shrimp.files, processors=30)\nsummary.seqs(fasta=shrimp.trim.contigs.fasta, count=shrimp.contigs.count_table, processors=20, processors=30)\n\ncount.groups(count=shrimp.contigs.count_table, processors=30)\n\nscreen.seqs(fasta=shrimp.trim.contigs.fasta, count=shrimp.contigs.count_table, maxambig=0, minlength=252, maxlength=254, maxhomop=6, processors=30)\nsummary.seqs(fasta=shrimp.trim.contigs.good.fasta, count=shrimp.contigs.good.count_table, processors=30)\ncount.groups(count=shrimp.contigs.good.count_table)\nunique.seqs(fasta=shrimp.trim.contigs.good.fasta, count=shrimp.contigs.good.count_table)\nsummary.seqs(count=shrimp.trim.contigs.good.count_table, processors=30)\n\npcr.seqs(fasta=reference_dbs/silva.nr_v132.align, start=11895, end=25318, keepdots=F, processors=30)\n#############################################################\n#### ############  ARB-SILVA WEB ALIGNER   ##################\n#### from 13862 to 23445\n#############################################################\n\nrename.file(input=reference_dbs/silva.nr_v132.pcr.align, new=reference_dbs/silva.v4.fasta)\nsummary.seqs(fasta=reference_dbs/silva.v4.fasta, processors=30)\nalign.seqs(fasta=shrimp.trim.contigs.good.unique.fasta, reference=reference_dbs/silva.v4.fasta, processors=30)\nsummary.seqs(fasta=shrimp.trim.contigs.good.unique.align, count=shrimp.trim.contigs.good.count_table, processors=30)\n\nscreen.seqs(fasta=shrimp.trim.contigs.good.unique.align, count=shrimp.trim.contigs.good.count_table, start=1968, end=11550, processors=30)\n\nsummary.seqs(fasta=current, count=current, processors=30)\n\ncount.groups(count=current)\n\nfilter.seqs(fasta=shrimp.trim.contigs.good.unique.good.align, vertical=T, trump=., processors=30)\nunique.seqs(fasta=shrimp.trim.contigs.good.unique.good.filter.fasta, count=shrimp.trim.contigs.good.good.count_table)\ncount.groups(count=current, processors=30)\nsummary.seqs(fasta=shrimp.trim.contigs.good.unique.good.filter.unique.fasta, count=shrimp.trim.contigs.good.unique.good.filter.count_table, processors=30)\n\npre.cluster(fasta=shrimp.trim.contigs.good.unique.good.filter.unique.fasta, count=shrimp.trim.contigs.good.unique.good.filter.count_table, diffs=2, processors=30)\nsummary.seqs(fasta=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.fasta, count=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.count_table, processors=30)\ncount.groups(count=current, processors=30)\n\n#############################################################\n#### ############  REMOVE NEGATIVE CONTROL ##################\n#### from https://forum.mothur.org/t/negative-control/2754/16\n#############################################################\n\nget.groups(fasta=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.fasta, count=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.count_table, groups=Control_49-Control_23-Control_17-Control_24-Control_14-Control_44-Control_20-Control_33-Control_41-Control_29-Control_50-Control_22-Control_19-Control_18-Control_48-Control_13-Control_21-Control_16-Control_30-Control_5-Control_42-Control_25-Control_51-Control_40-Control_15-Control_36-Control_47-Control_27-Control_32-Control_8-Control_3-Control_4-Control_6-Control_45-Control_26-Control_46-Control_53-Control_7-Control_12-Control_10-Control_9-Control_35-Control_54-Control_2-Control_43-Control_1-Control_11-Control_52-Control_38-Control_34-Control_56-Control_37-Control_28-Control_57-Control_31-Control_39-Control_59-Control_55-Control_60-Control_58)\nrename.file(input=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.fasta, new=neg_control.fasta)\nrename.file(input=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.count_table, new=neg_control.count_table)\nsummary.seqs(fasta=neg_control.fasta, count=neg_control.count_table, processors=30)\nlist.seqs(count=neg_control.count_table)\n\n########################################################################################################\n### modified neg_control.count_table: \n### 1. remove first two rows\n### 2. in BBEdit run \\t\\d+,.*\n### 3. in R\n###     library(tidyverse)   \n###     a &lt;- read_tsv(\"neg_control.count_table\")\n###     b &lt;- read_tsv(\"shrimp.trim.contigs.good.unique.good.filter.unique.precluster.count_table\")\n###     e &lt;- dplyr::left_join(a, b, by = \"Representative_Sequence\")\n###     write.table(e, \"results.txt\", row.names = FALSE, quote = FALSE, sep = \"\\t\")\n### 4. Replace accnos file with new list. Removed sequences that were found less than 10% in Neg control\n########################################################################################################\n    \nremove.seqs(accnos=neg_control.accnos, fasta=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.fasta, count=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.count_table)\n\n########################################################\n## REMOVED these NC samples from next remove.groups command\n## [ERROR]: Control_29 is not in your count table. Please correct.\n## [ERROR]: Control_18 is not in your count table. Please correct.\n## [ERROR]: Control_21 is not in your count table. Please correct.\n## [ERROR]: Control_5 is not in your count table. Please correct.\n## [ERROR]: Control_15 is not in your count table. Please correct.\n\nremove.groups(count=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.count_table, fasta=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.fasta, groups=Control_1-Control_10-Control_11-Control_12-Control_13-Control_14-Control_16-Control_17-Control_19-Control_2-Control_20-Control_22-Control_23-Control_24-Control_25-Control_26-Control_27-Control_28-Control_3-Control_30-Control_31-Control_32-Control_33-Control_34-Control_35-Control_36-Control_37-Control_38-Control_39-Control_4-Control_40-Control_41-Control_42-Control_43-Control_44-Control_45-Control_46-Control_47-Control_48-Control_49-Control_50-Control_51-Control_52-Control_53-Control_54-Control_55-Control_56-Control_57-Control_58-Control_59-Control_6-Control_60-Control_7-Control_8-Control_9)\n########################################################\n\nsummary.seqs(fasta=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.fasta, count=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.count_table, processors=30)\n\ncount.groups(count=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.count_table)\n\n########################################\n### NEGATIVE CONTROLS Should be GONE ###\n########################################\n\nchimera.vsearch(fasta=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.fasta, count=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.count_table, dereplicate=t, processors=30)\nsummary.seqs(fasta=current, count=current, processors=30)\ncount.groups(count=current, processors=30)\n\nclassify.seqs(fasta=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.fasta, count=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.count_table, processors=30, reference=reference_dbs/gsrdb.fasta, taxonomy=reference_dbs/gsrdb.tax)\n\nremove.lineage(fasta=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.fasta, count=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.count_table, taxonomy=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.gsrdb.wang.taxonomy, taxon=Chloroplast-Mitochondria-unknown-Eukaryota)\n\nsummary.tax(taxonomy=current, count=current)\ncount.groups(count=current, processors=30)\n\n##########################\nrename.file(fasta=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.pick.fasta, count=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.pick.count_table, taxonomy=shrimp.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.denovo.vsearch.gsrdb.wang.pick.taxonomy, prefix=final)\n\n##########################\n###    CLUSTERING      ###\n##########################\n\n##########################\n###    cluster.split   ###\n##########################\n\ncluster.split(fasta=final.fasta, count=final.count_table, taxonomy=final.taxonomy, taxlevel=4, cluster=f, processors=30) \ncluster.split(file=final.file, count=final.count_table, processors=30)\n\nsystem(mkdir pipelineFiles/cluster.split.gsrdb)\nsystem(mv pipelineFiles/final.opti_mcc.list pipelineFiles/cluster.split.gsrdb/)\nsystem(mv pipelineFiles/final.file pipelineFiles/cluster.split.gsrdb/)\nsystem(mv pipelineFiles/final.dist pipelineFiles/cluster.split.gsrdb/)\n\n##########################\n###    cluster         ###\n##########################\n\ndist.seqs(fasta=final.fasta, cutoff=0.03, processors=30)\ncluster(column=final.dist, count=final.count_table)\n\nquit()\n\n\nOnce you have the script and data you simply run the pipeline like so.\n\nconda activate mothur\nmothur otu_batchfile_processing\n\nIn the resources listed above, we include a table that summarizes read changes for each sample through the pipeline."
  },
  {
    "objectID": "docs/pub/references/index.html",
    "href": "docs/pub/references/index.html",
    "title": "Workflow References",
    "section": "",
    "text": "Order By\nDefault\n\n          Workflow\n        \n\n          Type\n        \n\n    \n      \n      \n\n\n\n\n\nName\n\n\nVersion\n\n\nWorkflow\n\n\nType\n\n\nReference\n\n\n\n\n\ndada2\n\n\nv1.26.0\n\n\n16S rRNA\n\n\nsoftware\n\n\n(Callahan et al. 2016)\n\n\n\n\nmothur\n\n\nv.1.43.0\n\n\n16S rRNA\n\n\nsoftware\n\n\n(Schloss et al. 2009)\n\n\n\n\nMED/Oligotyping\n\n\nv3.2-dev\n\n\n16S rRNA\n\n\nsoftware\n\n\n(Eren et al. 2015)\n\n\n\n\nMicroeco\n\n\nv1.9.1\n\n\n16S rRNA\n\n\nsoftware\n\n\n(Liu et al. 2021)\n\n\n\n\nSeqKit\n\n\nv2.8.2\n\n\n16S rRNA\n\n\nsoftware\n\n\n(Shen et al. 2016)\n\n\n\n\nGSRDB\n\n\n \n\n\n16S rRNA\n\n\ndatabase\n\n\n(Molano, Vega-Abellaneda, and Manichanh 2024)\n\n\n\n\nanvi’o\n\n\nmarie (v8dev)\n\n\nmetagenomic\n\n\nsoftware\n\n\n(Eren et al. 2021)\n\n\n\n\nkraken2\n\n\nv2.1.3\n\n\nmetagenomic\n\n\nsoftware\n\n\n(Wood, Lu, and Langmead 2019)\n\n\n\n\nmegahit\n\n\nv1.2.9\n\n\nmetagenomic\n\n\nsoftware\n\n\n(D. Li et al. 2015)\n\n\n\n\nsnakemake\n\n\nv7.32.4\n\n\nmetagenomic\n\n\nsoftware\n\n\n(Köster and Rahmann 2012)\n\n\n\n\nBOWTIE2\n\n\nv2.5.2\n\n\nmetagenomic\n\n\nsoftware\n\n\n(Langmead and Salzberg 2012)\n\n\n\n\nSAMtools\n\n\nv1.18\n\n\nmetagenomic\n\n\nsoftware\n\n\n(H. Li et al. 2009)\n\n\n\n\nIU filterquality Minoche\n\n\nv2.13\n\n\nmetagenomic\n\n\nsoftware\n\n\n(Eren et al. 2013)\n\n\n\n\nKAIJU\n\n\nv1.10.1\n\n\nmetagenomic\n\n\nsoftware\n\n\n(Menzel, Ng, and Krogh 2016)\n\n\n\n\nPRODIGAL\n\n\nV2.6.3\n\n\nmetagenomic\n\n\nsoftware\n\n\n(Hyatt et al. 2010)\n\n\n\n\nVirsorter2\n\n\nv2.2.4\n\n\nmetagenomic\n\n\nsoftware\n\n\n(Guo et al. 2021)\n\n\n\n\nCONCOCT\n\n\nv1.1.0\n\n\nmetagenomic\n\n\nsoftware\n\n\n(Alneberg et al. 2014)\n\n\n\n\nDASTOOL\n\n\nv1.1.7\n\n\nmetagenomic\n\n\nsoftware\n\n\n(Sieber et al. 2018)\n\n\n\n\nMETABAT2\n\n\nv2.17\n\n\nmetagenomic\n\n\nsoftware\n\n\n(Kang et al. 2019)\n\n\n\n\nMAXBIN2\n\n\n2.2.7\n\n\nmetagenomic\n\n\nsoftware\n\n\n(Wu, Simmons, and Singer 2016)\n\n\n\n\nCOG\n\n\nCOG20\n\n\nmetagenomic\n\n\ndatabase\n\n\n(Galperin et al. 2021)\n\n\n\n\nKEGG\n\n\n \n\n\nmetagenomic\n\n\ndatabase\n\n\n(Aramaki et al. 2020)\n\n\n\n\nCAZymes\n\n\nv11\n\n\nmetagenomic\n\n\ndatabase\n\n\n(Drula et al. 2022)\n\n\n\n\nPfams\n\n\nv37.0\n\n\nmetagenomic\n\n\ndatabase\n\n\n(Mistry et al. 2021)\n\n\n\n\nGTDB(SCG taxonomy)\n\n\nv214.1\n\n\nmetagenomic\n\n\ndatabase\n\n\n(Chaumeil et al. 2019)\n\n\n\n\nGTDB(tRNA taxonomy)\n\n\nv89\n\n\nmetagenomic\n\n\ndatabase\n\n\n(Chaumeil et al. 2019)\n\n\n\n\nArchaea_76 &Bacteria_71\n\n\n \n\n\nmetagenomic\n\n\nHMM database\n\n\n(Lee 2019)\n\n\n\n\nBacteria_71\n\n\n \n\n\nmetagenomic\n\n\nHMM database\n\n\n(Lee 2019)\n\n\n\n\nProtista_83\n\n\n0.9\n\n\nmetagenomic\n\n\nHMM database\n\n\n(Seemann 2018)\n\n\n\n\nRibosomal RNA (12S, 16S, 18S,23S, 28S, 5S)\n\n\n0.9\n\n\nmetagenomic\n\n\nHMM database\n\n\n(Seemann 2018)\n\n\n\n\n\nNo matching items\n\n\n\n\n\nReferences\n\n\nAlneberg, Johannes, Brynjar Smári Bjarnason, Ino De Bruijn, Melanie Schirmer, Joshua Quick, Umer Z Ijaz, Leo Lahti, Nicholas J Loman, Anders F Andersson, and Christopher Quince. 2014. “Binning Metagenomic Contigs by Coverage and Composition.” Nature Methods 11 (11): 11441146. https://doi.org/10.1038/nmeth.3103.\n\n\nAramaki, Takuya, Romain Blanc-Mathieu, Hisashi Endo, Koichi Ohkubo, Minoru Kanehisa, Susumu Goto, and Hiroyuki Ogata. 2020. “KofamKOALA: KEGG Ortholog Assignment Based on Profile HMM and Adaptive Score Threshold.” Bioinformatics 36 (7): 22512252. https://doi.org/10.1093/bioinformatics/btz859.\n\n\nCallahan, Benjamin J, Paul J McMurdie, Michael J Rosen, Andrew W Han, Amy Jo A Johnson, and Susan P Holmes. 2016. “DADA2: High-Resolution Sample Inference from Illumina Amplicon Data.” Nature Methods 13 (7): 581. https://doi.org/10.1038/nmeth.3869.\n\n\nChaumeil, Pierre-Alain, Aaron J Mussig, Philip Hugenholtz, and Donovan H Parks. 2019. “GTDB-Tk: A Toolkit to Classify Genomes with the Genome Taxonomy Database.” Bioinformatics 36 (6): 1925–27. https://doi.org/10.1093/bioinformatics/btz848.\n\n\nDrula, Elodie, Marie-Line Garron, Suzan Dogan, Vincent Lombard, Bernard Henrissat, and Nicolas Terrapon. 2022. “The Carbohydrate-Active Enzyme Database: Functions and Literature.” Nucleic Acids Research 50 (D1): D571D577. https://doi.org/10.1093/nar/gkab1045.\n\n\nEren, A Murat, E Kiefl, A Shaiber, I Veseli, SE Miller, MS Schechter, I Fink, et al. 2021. “Community-Led, Integrated, Reproducible Multi-Omics with Anvi’o. 6: 3–6.” Nature Microbiology. https://doi.org/10.1038/s41564-020-00834-3.\n\n\nEren, A Murat, Hilary G Morrison, Pamela J Lescault, Julie Reveillaud, Joseph H Vineis, and Mitchell L Sogin. 2015. “Minimum Entropy Decomposition: Unsupervised Oligotyping for Sensitive Partitioning of High-Throughput Marker Gene Sequences.” The ISME Journal 9 (4): 968–79. https://doi.org/10.1038/ismej.2014.195.\n\n\nEren, A Murat, Joseph H Vineis, Hilary G Morrison, and Mitchell L Sogin. 2013. “A Filtering Method to Generate High Quality Short Reads Using Illumina Paired-End Technology.” PLoS One 8 (6). https://doi.org/10.1371/journal.pone.0066643.\n\n\nGalperin, Michael Y, Yuri I Wolf, Kira S Makarova, Roberto Vera Alvarez, David Landsman, and Eugene V Koonin. 2021. “COG Database Update: Focus on Microbial Diversity, Model Organisms, and Widespread Pathogens.” Nucleic Acids Research 49 (D1): D274D281. https://doi.org/10.1093/nar/gkaa1018.\n\n\nGuo, Jiarong, Ben Bolduc, Ahmed A Zayed, Arvind Varsani, Guillermo Dominguez-Huerta, Tom O Delmont, Akbar Adjie Pratama, et al. 2021. “VirSorter2: A Multi-Classifier, Expert-Guided Approach to Detect Diverse DNA and RNA Viruses.” Microbiome 9: 113. https://doi.org/10.1186/s40168-020-00990-y.\n\n\nHyatt, Doug, Gwo-Liang Chen, Philip F LoCascio, Miriam L Land, Frank W Larimer, and Loren J Hauser. 2010. “Prodigal: Prokaryotic Gene Recognition and Translation Initiation Site Identification.” BMC Bioinformatics 11 (1): 119. https://doi.org/10.1186/1471-2105-11-119.\n\n\nKang, Dongwan D, Feng Li, Edward Kirton, Ashleigh Thomas, Rob Egan, Hong An, and Zhong Wang. 2019. “MetaBAT 2: An Adaptive Binning Algorithm for Robust and Efficient Genome Reconstruction from Metagenome Assemblies.” PeerJ 7: e7359. https://doi.org/10.7717%2Fpeerj.7359.\n\n\nKöster, Johannes, and Sven Rahmann. 2012. “Snakemake—a Scalable Bioinformatics Workflow Engine.” Bioinformatics 28 (19): 2520–22. https://doi.org/10.1093/bioinformatics/bts480.\n\n\nLangmead, Ben, and Steven L Salzberg. 2012. “Fast Gapped-Read Alignment with Bowtie 2.” Nature Methods 9 (4): 357. https://doi.org/10.1038/nmeth.1923.\n\n\nLee, Michael D. 2019. “GToTree: A User-Friendly Workflow for Phylogenomics.” Bioinformatics 35 (20): 41624164. https://doi.org/10.1093/bioinformatics/btz188.\n\n\nLi, Dinghua, Chi-Man Liu, Ruibang Luo, Kunihiko Sadakane, and Tak-Wah Lam. 2015. “MEGAHIT: An Ultra-Fast Single-Node Solution for Large and Complex Metagenomics Assembly via Succinct de Bruijn Graph.” Bioinformatics 31 (10): 1674–76. https://doi.org/10.1093/bioinformatics/btv033.\n\n\nLi, Heng, Bob Handsaker, Alec Wysoker, Tim Fennell, Jue Ruan, Nils Homer, Gabor Marth, Goncalo Abecasis, and Richard Durbin. 2009. “The Sequence Alignment/Map Format and SAMtools.” Bioinformatics 25 (16): 2078–79. https://doi.org/10.1093/bioinformatics/btp352.\n\n\nLiu, Chi, Yaoming Cui, Xiangzhen Li, and Minjie Yao. 2021. “Microeco: An r Package for Data Mining in Microbial Community Ecology.” FEMS Microbiology Ecology 97 (2): fiaa255. https://doi.org/10.1093/femsec/fiaa255.\n\n\nMenzel, Peter, Kim Lee Ng, and Anders Krogh. 2016. “Fast and Sensitive Taxonomic Classification for Metagenomics with Kaiju.” Nature Communications 7: 11257. https://doi.org/10.1038/ncomms11257.\n\n\nMistry, Jaina, Sara Chuguransky, Lowri Williams, Matloob Qureshi, Gustavo A Salazar, Erik LL Sonnhammer, Silvio CE Tosatto, et al. 2021. “Pfam: The Protein Families Database in 2021.” Nucleic Acids Research 49 (D1): D412D419. https://doi.org/10.1093/nar/gkaa913.\n\n\nMolano, Leidy-Alejandra G, Sara Vega-Abellaneda, and Chaysavanh Manichanh. 2024. “GSR-DB: A Manually Curated and Optimized Taxonomical Database for 16S rRNA Amplicon Analysis.” Msystems 9 (2): e00950–23. https://doi.org/10.1128/msystems.00950-23.\n\n\nSchloss, Patrick D, Sarah L Westcott, Thomas Ryabin, Justine R Hall, Martin Hartmann, Emily B Hollister, Ryan A Lesniewski, et al. 2009. “Introducing Mothur: Open-Source, Platform-Independent, Community-Supported Software for Describing and Comparing Microbial Communities.” Applied and Environmental Microbiology 75 (23): 7537–41. https://doi.org/10.1128/AEM.01541-09.\n\n\nSeemann, T. 2018. “Barrnap 0.9 : Rapid ribosomal RNA Prediction.” https://github.com/tseemann/barrnap.\n\n\nShen, Wei, Shuai Le, Yan Li, and Fuquan Hu. 2016. “SeqKit: A Cross-Platform and Ultrafast Toolkit for FASTA/q File Manipulation.” PloS One 11 (10): e0163962. https://doi.org/10.1371/journal.pone.016396.\n\n\nSieber, Christian MK, Alexander J Probst, Allison Sharrar, Brian C Thomas, Matthias Hess, Susannah G Tringe, and Jillian F Banfield. 2018. “Recovery of Genomes from Metagenomes via a Dereplication, Aggregation and Scoring Strategy.” Nature Microbiology 3 (7): 836843. https://doi.org/10.1038/s41564-018-0171-1.\n\n\nWood, Derrick E, Jennifer Lu, and Ben Langmead. 2019. “Improved Metagenomic Analysis with Kraken 2.” Genome Biology 20: 113. https://doi.org/10.1186/s13059-019-1891-0.\n\n\nWu, Yu-Wei, Blake A Simmons, and Steven W Singer. 2016. “MaxBin 2.0: An Automated Binning Algorithm to Recover Genomes from Multiple Metagenomic Datasets.” Bioinformatics 32 (4): 605–7. https://doi.org/10.1093/bioinformatics/btv638.\n\n\nSource Code\nThe source code for this page can be accessed on GitHub  by clicking this link.\nLast updated on\n\n\n[1] \"2024-09-29 11:01:22 PDT\"",
    "crumbs": [
      "Publication Material",
      "Workflow References"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Transisthmian Shrimp Microbiomes",
    "section": "",
    "text": "Data & Code \n Publication \n 16S rRNA \n Metagenomics"
  },
  {
    "objectID": "docs/pub/index.html",
    "href": "docs/pub/index.html",
    "title": "Publication Material",
    "section": "",
    "text": "Submitting Data\n\n\n\n\n\nIn this section you can find information on submitting sequence data to public archives. \n\n\n\n\n\n9/28/24, 7:41:35 AM\n\n\n\n\n\n\n\nWorkflow References\n\n\n\n\n\nQuick access to references for metagenomic and 16S RNA processing. \n\n\n\n\n\n9/28/24, 7:41:11 AM\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Publication Material"
    ]
  },
  {
    "objectID": "docs/mg-workflows/index.html",
    "href": "docs/mg-workflows/index.html",
    "title": "Metagenomic Workflows",
    "section": "",
    "text": "1. Annotation Databases\n\n\n\n\n\nConstructing taxonomic & functional annotation databases. \n\n\n\n\n\n9/28/24, 6:50:49 AM\n\n\n\n\n\n\n\n2. Co-Assembly & Annotations\n\n\n\n\n\nThis section describes the steps we took to co-assemble the metagenomic samples, classify taxonomy, and assign functions. \n\n\n\n\n\n9/28/24, 7:15:08 AM\n\n\n\n\n\n\n\n3. Assembly & Annotation Results\n\n\n\n\n\nThis section describes the steps we took to co-assemble the metagenomic samples, classify taxonomy, and assign functions. \n\n\n\n\n\n9/28/24, 6:55:55 AM\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Metagenomics"
    ]
  },
  {
    "objectID": "docs/ssu-workflows/index.html",
    "href": "docs/ssu-workflows/index.html",
    "title": "16S rRNA Workflows",
    "section": "",
    "text": "1. Sample Metadata\n\n\n\n\n\nWorkflow for wrangling sample metadata and generating informative sample names. \n\n\n\n\n\n9/28/24, 7:16:33 AM\n\n\n\n\n\n\n\n2. ASV Workflow\n\n\n\n\n\nWorkflow for processing 16S rRNA samples for ASV analysis using DADA2. Workflow uses paired end reads, beginning with raw fastq files, ending with sequence and taxonomy tables. A Microtable Object…\n\n\n\n\n\n9/28/24, 7:16:56 AM\n\n\n\n\n\n\n\n3. OTU Workflow\n\n\n\n\n\nWorkflows for processing 16S rRNA shrimp and environmental sample data sets using paired end reads, beginning with raw fastq files, ending with sequence and taxonomy tables. \n\n\n\n\n\n9/29/24, 10:50:32 AM\n\n\n\n\n\n\n\n3. OTU Workflow\n\n\n\n\n\nWorkflow for processing 16S rRNA samples for OTU analysis using mothur. Workflow uses paired end reads, beginning with raw fastq files, ending with sequence and taxonomy tables. A Microtable Object…\n\n\n\n\n\n9/28/24, 7:16:09 AM\n\n\n\n\n\n\n\n4. MED Workflow\n\n\n\n\n\nWorkflow for running MED analysis. Workflow begins with redundant, aligned fasta file from mothur and ends with the MED analysis. A Microtable Object is produced to collate the data for downstream…\n\n\n\n\n\n9/28/24, 7:16:23 AM\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "16S rRNA Processing"
    ]
  },
  {
    "objectID": "docs/pub/submitdata/index.html",
    "href": "docs/pub/submitdata/index.html",
    "title": "Submitting Data",
    "section": "",
    "text": "All sequence data is deposited at the European Nucleotide Archive (ENA). See below for instructions on submitting data to the ENA and the files we used to deposit the data.\nThe trimmed 16S rRNA data (primers removed) are deposited under Project Accession number PRJEB36632 (ERP119845), sample accession numbers ERS4291994-ERS4292031.\nThe metagenomic data from the four water column samples are also deposited under Project Accession number PRJEB36632 (ERP119845). The individual sample accession numbers for these data are ERS4578390-ERS4578393.",
    "crumbs": [
      "Publication Material",
      "Submitting Data"
    ]
  },
  {
    "objectID": "docs/pub/submitdata/index.html#archived-sequence-data",
    "href": "docs/pub/submitdata/index.html#archived-sequence-data",
    "title": "Submitting Data",
    "section": "",
    "text": "All sequence data is deposited at the European Nucleotide Archive (ENA). See below for instructions on submitting data to the ENA and the files we used to deposit the data.\nThe trimmed 16S rRNA data (primers removed) are deposited under Project Accession number PRJEB36632 (ERP119845), sample accession numbers ERS4291994-ERS4292031.\nThe metagenomic data from the four water column samples are also deposited under Project Accession number PRJEB36632 (ERP119845). The individual sample accession numbers for these data are ERS4578390-ERS4578393.",
    "crumbs": [
      "Publication Material",
      "Submitting Data"
    ]
  },
  {
    "objectID": "docs/pub/submitdata/index.html#pipeline-data",
    "href": "docs/pub/submitdata/index.html#pipeline-data",
    "title": "Submitting Data",
    "section": "Pipeline Data",
    "text": "Pipeline Data\nData for each individual pipeline are available through the Smithsonian figshare under a single collection at doi:10.25573/data.c.5025362. In addition, data from each pipeline are available for download from figshare using the links at the bottom of each page (where applicable).",
    "crumbs": [
      "Publication Material",
      "Submitting Data"
    ]
  },
  {
    "objectID": "docs/pub/submitdata/index.html#submitting-sequence-data",
    "href": "docs/pub/submitdata/index.html#submitting-sequence-data",
    "title": "Submitting Data",
    "section": "Submitting Sequence Data",
    "text": "Submitting Sequence Data\nWe submitted out data to the European Nucleotide Archive (ENA). The ENA does not like RAW data and prefers to have primers removed. So we submitted the trimmed Fastq files to the ENA. You can find these data under the study accession number PRJEB36632 (ERP119845). The RAW files on our figshare site (see above).\nTo submit to the ENA you need two data tables (plus your sequence data). One file describes the samples and the other file describes the sequencing data.\nYou can download these data tables here:\n16S rRNA\n\nDescription of sample data\nDescription of sequence data\nMetagenomic\n\nDescription of sample data\nDescription of sequence data\nInstructions for Submitting to the ENA\n\ngo to https://www.ebi.ac.uk/ena/submit and select Submit to ENA.\nLogin or Register.\nGo to New Submission tab and select Register study (project).\nHit Next\nEnter details and hit Submit.\nNext, Select Checklist. This will be specific to the type of samples you have and basically will create a template so you can add your sample metadata. For this study I chose GSC MIxS water, checklist accession number ERC000024\n\nNext\nNow go through and select/deselect fields as needed. Note, some fields are mandatory.\nOnce finished, hit Next to fill in any details that apply to All samples.\nFill in the sheet\nHit the Next button, change the number of samples, and download the sheet. (This is a little messy and you just need to wade through it)\nOnce everything looks good and uploaded, click Next to get to the Run page.\nSelect Two Fastq files (Paired) and Download the template.\nBefore filling out the form, gzip .gz all the trimmed fastq files (these are what you submit)\nThen run md5sum on all the tar.gz files.\nUpload all the fastq files. You must do this before submitting the experiment spreadsheet. There are different options for this step. I opened a terminal and typed ftp webin.ebi.ac.uk. I entered my username and password. Then typed mput *.gz. The problem was that I had to say yes for each file. Should be a way around this. Documentation can be found here. Probably need to type prompt first.\nFill in the sheet including md5 checksum values.\nUpload and submit the sheet.",
    "crumbs": [
      "Publication Material",
      "Submitting Data"
    ]
  },
  {
    "objectID": "docs/data/data-asv.html",
    "href": "docs/data/data-asv.html",
    "title": "Data & Scripts",
    "section": "",
    "text": "Quick access to pipeline processing scripts and raw data. With these scripts and associated data you can run the processing steps for each analysis.\nAll sequence data is linked to projects on the European Nucleotide Archive (ENA). The scripts and associated files can be downloaded as .zip archives by clicking the links. Or if you prefer, you can copy the scripts directly from the code blocks below."
  },
  {
    "objectID": "docs/data/data-asv.html#asv-data-and-scripts",
    "href": "docs/data/data-asv.html#asv-data-and-scripts",
    "title": "Data & Scripts",
    "section": "ASV Data and Scripts",
    "text": "ASV Data and Scripts\n\n\n\n\nName\n\n\nDescription\n\n\n\n\n\nSequence data\n\n\nTrimmed (primers removed) 16S rRNA data from 1809 samples for ASV analysis \n\n\n\n\nFastq rename tables\n\n\nLookup tables for renaming fastq files prior to analysis \n\n\n\n\nSample data\n\n\n16S rRNA sample data \n\n\n\n\nProcessing scripts\n\n\nIndividual run and merge run R scripts for ASV analysis with \n\n\n\n\nHydra job scripts\n\n\nHydra job scripts \n\n\n\n\nPipeline read changes\n\n\nTable showing read changes for each sample through the processing pipeline \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "docs/data/data-asv.html#asv-processing",
    "href": "docs/data/data-asv.html#asv-processing",
    "title": "Data & Scripts",
    "section": "ASV Processing",
    "text": "ASV Processing\nIndividual Runs\nProcessing scripts for ASV analysis of individual sequencing runs using dada2. In total, 16S rRNA sequencing was performed on 6 sequencing runs. In the first workflow of the pipeline, runs are processed separately for error rates, dereplication, and ASV inference. At the end of each workflow, forward and reverse reads are merged.\n\n\nBCS_26\nBCS_28\nBCS_29\nBCS_30\nBCS_34\nBCS_35\n\n\n\n\nProcessing script for run BCS_26################################################################\n#####           TRANSISTHMIAN SHRIMP MICROBIOME            #####\n#####                 MISEQ RUN: BCS_26                    #####\n#####            PLATES ISTHMO S5, S5, S7, S8              #####\n################################################################\n#\n# SCRIPT prepared by Matthieu Leray\n# last modified January 28th 2023\n#\n# READ PROCESSING\n#\n# Modified by Jarrod Scott\n# last modified September 11 2024\n################################################################\n\nlibrary(dada2)\nlibrary(tidyverse)\nlibrary(ff)\nlibrary(phyloseq)\nlibrary(gridExtra)\nlibrary(dplyr)\nlibrary(decontam)\n\n##Creating filepaths to data \npath &lt;- \"RAW_DATA/BCS_26\"\nhead(list.files(path)) #eventually to check if the path works\n\n##File preparation\n#extracting Forward (fnFs) and Reverse (fnRs) reads from files\nfnFs &lt;- sort(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\nfnRs &lt;- sort(list.files(path, pattern = \"_R2_001.trimmed.fastq\"))\nsample.names &lt;- sapply(strsplit(fnFs, \"_R1_\"), `[`, 1)\nfnFs &lt;-file.path(path, fnFs)\nfnRs &lt;-file.path(path, fnRs)\n\nx &lt;- length(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\n\nqprofile_fwd &lt;- plotQualityProfile(fnFs[1:x], aggregate = TRUE, n = 20000)\nqprofile_rev &lt;- plotQualityProfile(fnRs[1:x], aggregate = TRUE, n = 20000)\n\nqprofile &lt;- grid.arrange(qprofile_fwd, qprofile_rev, nrow = 1)\nggsave(\"figures/BCS_26_filt_plot_qscores.png\", qprofile, width = 7, height = 3)\n\n#placing filtered files in a new filtered subdirectory\nfiltFs &lt;- file.path(path, \"filtered\", paste0(sample.names, \"_F_filt.fastq\"))\nfiltRs &lt;- file.path(path, \"filtered\", paste0(sample.names, \"_R_filt.fastq\"))\nnames(filtFs) &lt;- sample.names\nnames(filtRs) &lt;- sample.names\n\n#filtering and trimming, here truncation at 220 (Fwd) and 180 (Rev) bp, \n#2expected errors max (N discarded automatically)\n\nout &lt;- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(220,180),\n                        maxN=0, maxEE=2, truncQ=2, rm.phix=TRUE,\n                        compress=TRUE, multithread=20)\n\nhead(out)\n\n#learning error rates\nerrF &lt;- learnErrors(filtFs, multithread = TRUE)\nerrR &lt;- learnErrors(filtRs, multithread = TRUE)\n#plotting errors\n#plotErrors(errF, nominalQ=TRUE)\n#plotErrors(errR, nominalQ=TRUE)\n\n## ----plot_errF------------------------------\np3 &lt;- plotErrors(errF, nominalQ = TRUE)\nggsave(\"figures/BCS_26_plot_errorF_1.png\", p3, width = 7, height = 5)\nggsave(\"figures/BCS_26_plot_errorF_2.png\", p3)\n## ----plot_errR------------------------------\np4 &lt;- plotErrors(errR, nominalQ = TRUE)\nggsave(\"figures/BCS_26_plot_errorR_1.png\", p4, width = 7, height = 5)\nggsave(\"figures/BCS_26_plot_errorR_2.png\", p4)\n\n##Dereplicating reads\nsam.names &lt;- sapply(strsplit(basename(filtFs), \"_F_\"), `[`, 1)\nderepFs &lt;- derepFastq(filtFs)\nnames(derepFs) &lt;- sam.names\nderepRs &lt;- derepFastq(filtRs)\nnames(derepRs) &lt;- sam.names\n\n##Infering Sequence Variants\ndadaFs &lt;- dada(derepFs, err = errF, pool = \"pseudo\", multithread = TRUE)\ndadaFs[[1]]\ndadaRs &lt;- dada(derepRs, err = errR, pool = \"pseudo\", multithread = TRUE)\ndadaRs[[1]]\n\n##Merging paired ends\nmergers &lt;- mergePairs(dadaFs, derepFs, dadaRs, derepRs)\nBCS_26 &lt;- makeSequenceTable(mergers)\ndim(BCS_26)\n# [1]   384 29481\ntable(nchar(getSequences(BCS_26)))\n\n#exporting files to use in the next part of the workflow\nsaveRDS(BCS_26, \"BCS_26/BCS_26.rds\")\n\n#tracking changes through each step\ngetN &lt;- function(x) sum(getUniques(x))\ntrack &lt;-    cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN),\n                  sapply(mergers, getN))\ncolnames(track) &lt;- c(\"input\", \"filtered\", \"denoisedF\", \"denoisedR\",\n                        \"merged\")\nrownames(track) &lt;- sam.names\nwrite.table(track, \"BCS_26/BCS_26_read_changes.txt\", sep = \"\\t\", quote = FALSE,\n            col.names=NA)\n\nread_length &lt;-  data.frame(nchar(getSequences(BCS_26)))\n\ncolnames(read_length) &lt;- \"length\"\n\nplot_BCS_26 &lt;- qplot(length, data = read_length, \n                     geom = \"histogram\", binwidth = 1, \n                     xlab = \"read length\", \n                     ylab = \"total variants\", \n                     xlim = c(225,275)) \nggsave(\"figures/read_length_before_pseudo_BCS_26.png\", plot_BCS_26, width = 7, height = 3)\n\nsave.image(\"BCS_26.rdata\")\n\n\n\n\n\nProcessing script for run BCS_28################################################################\n#####           TRANSISTHMIAN SHRIMP MICROBIOME            #####\n#####                 MISEQ RUN: BCS_28                    #####\n#####               PLATES ISTHMO S3, S4                   #####\n################################################################\n#\n# SCRIPT prepared by Matthieu Leray\n# last modified January 28th 2023\n#\n# READ PROCESSING\n#\n# Modified by Jarrod Scott\n# last modified September 11 2024\n################################################################\n\nlibrary(dada2)\nlibrary(tidyverse)\nlibrary(ff)\nlibrary(phyloseq)\nlibrary(gridExtra)\nlibrary(dplyr)\nlibrary(decontam)\n\n##Creating filepaths to data \npath &lt;- \"RAW_DATA/BCS_28\"\nhead(list.files(path)) #eventually to check if the path works\n\n##File preparation\n#extracting Forward (fnFs) and Reverse (fnRs) reads from files\nfnFs &lt;- sort(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\nfnRs &lt;- sort(list.files(path, pattern = \"_R2_001.trimmed.fastq\"))\nsample.names &lt;- sapply(strsplit(fnFs, \"_R1_\"), `[`, 1)\nfnFs &lt;-file.path(path, fnFs)\nfnRs &lt;-file.path(path, fnRs)\n\nx &lt;- length(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\n\nqprofile_fwd &lt;- plotQualityProfile(fnFs[1:x], aggregate = TRUE)\nqprofile_rev &lt;- plotQualityProfile(fnRs[1:x], aggregate = TRUE)\n\nqprofile &lt;- grid.arrange(qprofile_fwd, qprofile_rev, nrow = 1)\nggsave(\"figures/BCS_28_filt_plot_qscores.png\", qprofile, width = 7, height = 3)\n\n#placing filtered files in a new filtered subdirectory\nfiltFs &lt;- file.path(path, \"filtered\", paste0(sample.names, \"_F_filt.fastq\"))\nfiltRs &lt;- file.path(path, \"filtered\", paste0(sample.names, \"_R_filt.fastq\"))\nnames(filtFs) &lt;- sample.names\nnames(filtRs) &lt;- sample.names\n\n# filtering and trimming, here truncation at 220 (Fwd) and 180 (Rev) bp, \n# 2expected errors max (N discarded automatically)\n\nout &lt;- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(220,180),\n                        maxN=0, maxEE=2, truncQ=2, rm.phix=TRUE,\n                        compress=TRUE, multithread=20)\n\nhead(out)\n\n#learning error rates\nerrF &lt;- learnErrors(filtFs, multithread = TRUE)\nerrR &lt;- learnErrors(filtRs, multithread = TRUE)\n\n## ----plot_errF------------------------------\np3 &lt;- plotErrors(errF, nominalQ = TRUE)\nggsave(\"figures/BCS_28_plot_errorF_1.png\", p3, width = 7, height = 5)\nggsave(\"figures/BCS_28_plot_errorF_2.png\", p3)\n## ----plot_errR------------------------------\np4 &lt;- plotErrors(errR, nominalQ = TRUE)\nggsave(\"figures/BCS_28_plot_errorR_1.png\", p4, width = 7, height = 5)\nggsave(\"figures/BCS_28_plot_errorR_2.png\", p4)\n\n##Dereplicating reads\nsam.names &lt;- sapply(strsplit(basename(filtFs), \"_F_\"), `[`, 1)\nderepFs &lt;- derepFastq(filtFs)\nnames(derepFs) &lt;- sam.names\nderepRs &lt;- derepFastq(filtRs)\nnames(derepRs) &lt;- sam.names\n\n##Infering Sequence Variants\ndadaFs &lt;- dada(derepFs, err = errF, pool = \"pseudo\", multithread = TRUE)\ndadaFs[[1]]\ndadaRs &lt;- dada(derepRs, err = errR, pool = \"pseudo\", multithread = TRUE)\ndadaRs[[1]]\n\n##Merging paired ends\nmergers &lt;- mergePairs(dadaFs, derepFs, dadaRs, derepRs)\nBCS_28 &lt;- makeSequenceTable(mergers)\ndim(BCS_28)\n# [1]   384 29481\ntable(nchar(getSequences(BCS_28)))\n\n#exporting files to use in the next part of the workflow\nsaveRDS(BCS_28, \"BCS_28/BCS_28.rds\")\n\n#tracking changes through each step\ngetN &lt;- function(x) sum(getUniques(x))\ntrack &lt;-    cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN),\n                  sapply(mergers, getN))\ncolnames(track) &lt;- c(\"input\", \"filtered\", \"denoisedF\", \"denoisedR\",\n                        \"merged\")\nrownames(track) &lt;- sam.names\nwrite.table(track, \"BCS_28/BCS_28_read_changes.txt\", sep = \"\\t\", quote = FALSE,\n            col.names=NA)\n\nread_length &lt;-  data.frame(nchar(getSequences(BCS_28)))\n\ncolnames(read_length) &lt;- \"length\"\n\nplot_BCS_28 &lt;- qplot(length, data = read_length, \n                     geom = \"histogram\", binwidth = 1, \n                     xlab = \"read length\", \n                     ylab = \"total variants\", \n                     xlim = c(225,275)) \nggsave(\"figures/read_length_before_pseudo_BCS_28.png\", plot_BCS_28, width = 7, height = 3)\n\nsave.image(\"BCS_28.rdata\")\n\n\n\n\n\nProcessing script for run BCS_29################################################################\n#####           TRANSISTHMIAN SHRIMP MICROBIOME            #####\n#####                 MISEQ RUN: BCS_29                    #####\n#####          PLATES ISTHMO S13, S14, S15, S16            #####\n################################################################\n#\n# SCRIPT prepared by Matthieu Leray\n# last modified January 28th 2023\n#\n# READ PROCESSING\n#\n# Modified by Jarrod Scott\n# last modified September 11 2024\n################################################################\n\nlibrary(dada2)\nlibrary(tidyverse)\nlibrary(ff)\nlibrary(phyloseq)\nlibrary(gridExtra)\nlibrary(dplyr)\nlibrary(decontam)\n\n##Creating filepaths to data \npath &lt;- \"RAW_DATA/BCS_29\"\nhead(list.files(path)) #eventually to check if the path works\n\n##File preparation\n#extracting Forward (fnFs) and Reverse (fnRs) reads from files\nfnFs &lt;- sort(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\nfnRs &lt;- sort(list.files(path, pattern = \"_R2_001.trimmed.fastq\"))\nsample.names &lt;- sapply(strsplit(fnFs, \"_R1_\"), `[`, 1)\nfnFs &lt;-file.path(path, fnFs)\nfnRs &lt;-file.path(path, fnRs)\n\nx &lt;- length(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\n\nqprofile_fwd &lt;- plotQualityProfile(fnFs[1:x], aggregate = TRUE)\nqprofile_rev &lt;- plotQualityProfile(fnRs[1:x], aggregate = TRUE)\n\nqprofile &lt;- grid.arrange(qprofile_fwd, qprofile_rev, nrow = 1)\nggsave(\"figures/BCS_29_filt_plot_qscores.png\", qprofile, width = 7, height = 3)\n\n#placing filtered files in a new filtered subdirectory\nfiltFs &lt;- file.path(path, \"filtered\", paste0(sample.names, \"_F_filt.fastq\"))\nfiltRs &lt;- file.path(path, \"filtered\", paste0(sample.names, \"_R_filt.fastq\"))\nnames(filtFs) &lt;- sample.names\nnames(filtRs) &lt;- sample.names\n\n#filtering and trimming, here truncation at 220 (Fwd) and 180 (Rev) bp, \n#2expected errors max (N discarded automatically)\n\nout &lt;- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(220,180),\n                        maxN=0, maxEE=2, truncQ=2, rm.phix=TRUE,\n                        compress=TRUE, multithread=20)\n\nhead(out)\n\n#learning error rates\nerrF &lt;- learnErrors(filtFs, multithread = TRUE)\nerrR &lt;- learnErrors(filtRs, multithread = TRUE)\n\n## ----plot_errF------------------------------\np3 &lt;- plotErrors(errF, nominalQ = TRUE)\nggsave(\"figures/BCS_29_plot_errorF_1.png\", p3, width = 7, height = 5)\nggsave(\"figures/BCS_29_plot_errorF_2.png\", p3)\n## ----plot_errR------------------------------\np4 &lt;- plotErrors(errR, nominalQ = TRUE)\nggsave(\"figures/BCS_29_plot_errorR_1.png\", p4, width = 7, height = 5)\nggsave(\"figures/BCS_29_plot_errorR_2.png\", p4)\n\n##Dereplicating reads\nsam.names &lt;- sapply(strsplit(basename(filtFs), \"_F_\"), `[`, 1)\nderepFs &lt;- derepFastq(filtFs)\nnames(derepFs) &lt;- sam.names\nderepRs &lt;- derepFastq(filtRs)\nnames(derepRs) &lt;- sam.names\n\n##Infering Sequence Variants\ndadaFs &lt;- dada(derepFs, err = errF, pool = \"pseudo\", multithread = TRUE)\ndadaFs[[1]]\ndadaRs &lt;- dada(derepRs, err = errR, pool = \"pseudo\", multithread = TRUE)\ndadaRs[[1]]\n\n##Merging paired ends\nmergers &lt;- mergePairs(dadaFs, derepFs, dadaRs, derepRs)\nBCS_29 &lt;- makeSequenceTable(mergers)\ndim(BCS_29)\n# [1]   384 29481\ntable(nchar(getSequences(BCS_29)))\n\n#exporting files to use in the next part of the workflow\nsaveRDS(BCS_29, \"BCS_29/BCS_29.rds\")\n\n#tracking changes through each step\ngetN &lt;- function(x) sum(getUniques(x))\ntrack &lt;-    cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN),\n                  sapply(mergers, getN))\ncolnames(track) &lt;- c(\"input\", \"filtered\", \"denoisedF\", \"denoisedR\",\n                        \"merged\")\nrownames(track) &lt;- sam.names\nwrite.table(track, \"BCS_29/BCS_29_read_changes.txt\", sep = \"\\t\", quote = FALSE,\n            col.names=NA)\n\nread_length &lt;-  data.frame(nchar(getSequences(BCS_29)))\n\ncolnames(read_length) &lt;- \"length\"\n\nplot_BCS_29 &lt;- qplot(length, data = read_length, \n                     geom = \"histogram\", binwidth = 1, \n                     xlab = \"read length\", \n                     ylab = \"total variants\", \n                     xlim = c(225,275)) \nggsave(\"figures/read_length_before_pseudo_BCS_29.png\", plot_BCS_29, width = 7, height = 3)\n\nsave.image(\"BCS_29.rdata\")\n\n\n\n\n\nProcessing script for run BCS_30################################################################\n#####           TRANSISTHMIAN SHRIMP MICROBIOME            #####\n#####                 MISEQ RUN: BCS_30                    #####\n#####          PLATES ISTHMO S17, S18, S19, S20            #####\n################################################################\n#\n# SCRIPT prepared by Matthieu Leray\n# last modified January 28th 2023\n#\n# READ PROCESSING\n#\n# Modified by Jarrod Scott\n# last modified September 11 2024\n################################################################\n\nlibrary(dada2)\nlibrary(tidyverse)\nlibrary(ff)\nlibrary(phyloseq)\nlibrary(gridExtra)\nlibrary(dplyr)\nlibrary(decontam)\n\n##Creating filepaths to data \npath &lt;- \"RAW_DATA/BCS_30\"\nhead(list.files(path)) #eventually to check if the path works\n\n##File preparation\n#extracting Forward (fnFs) and Reverse (fnRs) reads from files\nfnFs &lt;- sort(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\nfnRs &lt;- sort(list.files(path, pattern = \"_R2_001.trimmed.fastq\"))\nsample.names &lt;- sapply(strsplit(fnFs, \"_R1_\"), `[`, 1)\nfnFs &lt;-file.path(path, fnFs)\nfnRs &lt;-file.path(path, fnRs)\n\nx &lt;- length(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\n\nqprofile_fwd &lt;- plotQualityProfile(fnFs[1:x], aggregate = TRUE, n = 20000)\nqprofile_rev &lt;- plotQualityProfile(fnRs[1:x], aggregate = TRUE, n = 20000)\n\nqprofile &lt;- grid.arrange(qprofile_fwd, qprofile_rev, nrow = 1)\nggsave(\"figures/BCS_30_filt_plot_qscores.png\", qprofile, width = 7, height = 3)\n\n#placing filtered files in a new filtered subdirectory\nfiltFs &lt;- file.path(path, \"filtered\", paste0(sample.names, \"_F_filt.fastq\"))\nfiltRs &lt;- file.path(path, \"filtered\", paste0(sample.names, \"_R_filt.fastq\"))\nnames(filtFs) &lt;- sample.names\nnames(filtRs) &lt;- sample.names\n\n#filtering and trimming, here truncation at 220 (Fwd) and 180 (Rev) bp, \n#2expected errors max (N discarded automatically)\n\nout &lt;- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(220,180),\n                        maxN=0, maxEE=2, truncQ=2, rm.phix=TRUE,\n                        compress=TRUE, multithread=20)\n\nhead(out)\n\n#learning error rates\nerrF &lt;- learnErrors(filtFs, multithread = TRUE)\nerrR &lt;- learnErrors(filtRs, multithread = TRUE)\n\n## ----plot_errF------------------------------\np3 &lt;- plotErrors(errF, nominalQ = TRUE)\nggsave(\"figures/BCS_30_plot_errorF_1.png\", p3, width = 7, height = 5)\nggsave(\"figures/BCS_30_plot_errorF_2.png\", p3)\n## ----plot_errR------------------------------\np4 &lt;- plotErrors(errR, nominalQ = TRUE)\nggsave(\"figures/BCS_30_plot_errorR_1.png\", p4, width = 7, height = 5)\nggsave(\"figures/BCS_30_plot_errorR_2.png\", p4)\n\n##Dereplicating reads\nsam.names &lt;- sapply(strsplit(basename(filtFs), \"_F_\"), `[`, 1)\nderepFs &lt;- derepFastq(filtFs)\nnames(derepFs) &lt;- sam.names\nderepRs &lt;- derepFastq(filtRs)\nnames(derepRs) &lt;- sam.names\n\n##Infering Sequence Variants\ndadaFs &lt;- dada(derepFs, err = errF, pool = \"pseudo\", multithread = TRUE)\ndadaFs[[1]]\ndadaRs &lt;- dada(derepRs, err = errR, pool = \"pseudo\", multithread = TRUE)\ndadaRs[[1]]\n\n##Merging paired ends\nmergers &lt;- mergePairs(dadaFs, derepFs, dadaRs, derepRs)\nBCS_30 &lt;- makeSequenceTable(mergers)\ndim(BCS_30)\n# [1]   384 29481\ntable(nchar(getSequences(BCS_30)))\n\n#exporting files to use in the next part of the workflow\nsaveRDS(BCS_30, \"BCS_30/BCS_30.rds\")\n\n#tracking changes through each step\ngetN &lt;- function(x) sum(getUniques(x))\ntrack &lt;-    cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN),\n                  sapply(mergers, getN))\ncolnames(track) &lt;- c(\"input\", \"filtered\", \"denoisedF\", \"denoisedR\",\n                        \"merged\")\nrownames(track) &lt;- sam.names\nwrite.table(track, \"BCS_30/BCS_30_read_changes.txt\", sep = \"\\t\", quote = FALSE,\n            col.names=NA)\n\nread_length &lt;-  data.frame(nchar(getSequences(BCS_30)))\n\ncolnames(read_length) &lt;- \"length\"\n\nplot_BCS_30 &lt;- qplot(length, data = read_length, \n                     geom = \"histogram\", binwidth = 1, \n                     xlab = \"read length\", \n                     ylab = \"total variants\", \n                     xlim = c(225,275)) \nggsave(\"figures/read_length_before_pseudo_BCS_30.png\", plot_BCS_30, width = 7, height = 3)\n\nsave.image(\"BCS_30.rdata\")\n\n\n\n\n\nProcessing script for run BCS_34################################################################\n#####           TRANSISTHMIAN SHRIMP MICROBIOME            #####\n#####                 MISEQ RUN: BCS_34                    #####\n#####               PLATES ISTHMO S01, S02                 #####\n################################################################\n#\n# SCRIPT prepared by Matthieu Leray\n# last modified January 28th 2023\n#\n# READ PROCESSING\n#\n# Modified by Jarrod Scott\n# last modified September 11 2024\n################################################################\n\nlibrary(dada2)\nlibrary(tidyverse)\nlibrary(ff)\nlibrary(phyloseq)\nlibrary(gridExtra)\nlibrary(dplyr)\nlibrary(decontam)\n\n##Creating filepaths to data \npath &lt;- \"RAW_DATA/BCS_34\"\nhead(list.files(path)) #eventually to check if the path works\n\n##File preparation\n#extracting Forward (fnFs) and Reverse (fnRs) reads from files\nfnFs &lt;- sort(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\nfnRs &lt;- sort(list.files(path, pattern = \"_R2_001.trimmed.fastq\"))\nsample.names &lt;- sapply(strsplit(fnFs, \"_R1_\"), `[`, 1)\nfnFs &lt;-file.path(path, fnFs)\nfnRs &lt;-file.path(path, fnRs)\n\nx &lt;- length(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\n\nqprofile_fwd &lt;- plotQualityProfile(fnFs[1:x], aggregate = TRUE)\nqprofile_rev &lt;- plotQualityProfile(fnRs[1:x], aggregate = TRUE)\n\nqprofile &lt;- grid.arrange(qprofile_fwd, qprofile_rev, nrow = 1)\nggsave(\"figures/BCS_34_filt_plot_qscores.png\", qprofile, width = 7, height = 3)\n\n#placing filtered files in a new filtered subdirectory\nfiltFs &lt;- file.path(path, \"filtered\", paste0(sample.names, \"_F_filt.fastq\"))\nfiltRs &lt;- file.path(path, \"filtered\", paste0(sample.names, \"_R_filt.fastq\"))\nnames(filtFs) &lt;- sample.names\nnames(filtRs) &lt;- sample.names\n\n#filtering and trimming, here truncation at 220 (Fwd) and 180 (Rev) bp, \n#2expected errors max (N discarded automatically)\n\nout &lt;- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(220,180),\n                        maxN=0, maxEE=2, truncQ=2, rm.phix=TRUE,\n                        compress=TRUE, multithread=20)\n\nhead(out)\n\n#learning error rates\nerrF &lt;- learnErrors(filtFs, multithread = TRUE)\nerrR &lt;- learnErrors(filtRs, multithread = TRUE)\n\n## ----plot_errF------------------------------\np3 &lt;- plotErrors(errF, nominalQ = TRUE)\nggsave(\"figures/BCS_34_plot_errorF_1.png\", p3, width = 7, height = 5)\nggsave(\"figures/BCS_34_plot_errorF_2.png\", p3)\n## ----plot_errR------------------------------\np4 &lt;- plotErrors(errR, nominalQ = TRUE)\nggsave(\"figures/BCS_34_plot_errorR_1.png\", p4, width = 7, height = 5)\nggsave(\"figures/BCS_34_plot_errorR_2.png\", p4)\n\n##Dereplicating reads\nsam.names &lt;- sapply(strsplit(basename(filtFs), \"_F_\"), `[`, 1)\nderepFs &lt;- derepFastq(filtFs)\nnames(derepFs) &lt;- sam.names\nderepRs &lt;- derepFastq(filtRs)\nnames(derepRs) &lt;- sam.names\n\n##Infering Sequence Variants\ndadaFs &lt;- dada(derepFs, err = errF, pool = \"pseudo\", multithread = TRUE)\ndadaFs[[1]]\ndadaRs &lt;- dada(derepRs, err = errR, pool = \"pseudo\", multithread = TRUE)\ndadaRs[[1]]\n\n##Merging paired ends\nmergers &lt;- mergePairs(dadaFs, derepFs, dadaRs, derepRs)\nBCS_34 &lt;- makeSequenceTable(mergers)\ndim(BCS_34)\n# [1]   384 29481\ntable(nchar(getSequences(BCS_34)))\n\n#exporting files to use in the next part of the workflow\nsaveRDS(BCS_34, \"BCS_34/BCS_34.rds\")\n\n#tracking changes through each step\ngetN &lt;- function(x) sum(getUniques(x))\ntrack &lt;-    cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN),\n                  sapply(mergers, getN))\ncolnames(track) &lt;- c(\"input\", \"filtered\", \"denoisedF\", \"denoisedR\",\n                        \"merged\")\nrownames(track) &lt;- sam.names\nwrite.table(track, \"BCS_34/BCS_34_read_changes.txt\", sep = \"\\t\", quote = FALSE,\n            col.names=NA)\n\nread_length &lt;-  data.frame(nchar(getSequences(BCS_34)))\n\ncolnames(read_length) &lt;- \"length\"\n\nplot_BCS_34 &lt;- qplot(length, data = read_length, \n                      geom = \"histogram\", binwidth = 1, \n                      xlab = \"read length\", \n                      ylab = \"total variants\", \n                      xlim = c(225,275)) \nggsave(\"figures/read_length_before_pseudo_BCS_34.png\", plot_BCS_34, width = 7, height = 3)\n\nsave.image(\"BCS_34.rdata\")\n\n\n\n\n\nProcessing script for run BCS_35################################################################\n#####           TRANSISTHMIAN SHRIMP MICROBIOME            #####\n#####                 MISEQ RUN: BCS_35                    #####\n#####           PLATES ISTHMO S9, S10, S11, S12            #####\n################################################################\n#\n# SCRIPT prepared by Matthieu Leray\n# last modified January 28th 2023\n#\n# READ PROCESSING\n#\n# Modified by Jarrod Scott\n# last modified September 11 2024\n################################################################\n\nlibrary(dada2)\nlibrary(tidyverse)\nlibrary(ff)\nlibrary(phyloseq)\nlibrary(gridExtra)\nlibrary(dplyr)\nlibrary(decontam)\n\n##Creating filepaths to data \npath &lt;- \"RAW_DATA/BCS_35\"\nhead(list.files(path)) #eventually to check if the path works\n\n##File preparation\n#extracting Forward (fnFs) and Reverse (fnRs) reads from files\nfnFs &lt;- sort(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\nfnRs &lt;- sort(list.files(path, pattern = \"_R2_001.trimmed.fastq\"))\nsample.names &lt;- sapply(strsplit(fnFs, \"_R1_\"), `[`, 1)\nfnFs &lt;-file.path(path, fnFs)\nfnRs &lt;-file.path(path, fnRs)\n\nx &lt;- length(list.files(path, pattern = \"_R1_001.trimmed.fastq\"))\n\nqprofile_fwd &lt;- plotQualityProfile(fnFs[1:x], aggregate = TRUE)\nqprofile_rev &lt;- plotQualityProfile(fnRs[1:x], aggregate = TRUE)\n\nqprofile &lt;- grid.arrange(qprofile_fwd, qprofile_rev, nrow = 1)\nggsave(\"figures/BCS_35_filt_plot_qscores.png\", qprofile, width = 7, height = 3)\n\n#placing filtered files in a new filtered subdirectory\nfiltFs &lt;- file.path(path, \"filtered\", paste0(sample.names, \"_F_filt.fastq\"))\nfiltRs &lt;- file.path(path, \"filtered\", paste0(sample.names, \"_R_filt.fastq\"))\nnames(filtFs) &lt;- sample.names\nnames(filtRs) &lt;- sample.names\n\n#filtering and trimming, here truncation at 220 (Fwd) and 180 (Rev) bp, \n#2expected errors max (N discarded automatically)\n\nout &lt;- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(220,180),\n                        maxN=0, maxEE=2, truncQ=2, rm.phix=TRUE,\n                        compress=TRUE, multithread=20)\n\nhead(out)\n\n#learning error rates\nerrF &lt;- learnErrors(filtFs, multithread = TRUE)\nerrR &lt;- learnErrors(filtRs, multithread = TRUE)\n\n## ----plot_errF------------------------------\np3 &lt;- plotErrors(errF, nominalQ = TRUE)\nggsave(\"figures/BCS_35_plot_errorF_1.png\", p3, width = 7, height = 5)\nggsave(\"figures/BCS_35_plot_errorF_2.png\", p3)\n## ----plot_errR------------------------------\np4 &lt;- plotErrors(errR, nominalQ = TRUE)\nggsave(\"figures/BCS_35_plot_errorR_1.png\", p4, width = 7, height = 5)\nggsave(\"figures/BCS_35_plot_errorR_2.png\", p4)\n\n##Dereplicating reads\nsam.names &lt;- sapply(strsplit(basename(filtFs), \"_F_\"), `[`, 1)\nderepFs &lt;- derepFastq(filtFs)\nnames(derepFs) &lt;- sam.names\nderepRs &lt;- derepFastq(filtRs)\nnames(derepRs) &lt;- sam.names\n\n##Infering Sequence Variants\ndadaFs &lt;- dada(derepFs, err = errF, pool = \"pseudo\", multithread = TRUE)\ndadaFs[[1]]\ndadaRs &lt;- dada(derepRs, err = errR, pool = \"pseudo\", multithread = TRUE)\ndadaRs[[1]]\n\n##Merging paired ends\nmergers &lt;- mergePairs(dadaFs, derepFs, dadaRs, derepRs)\nBCS_35 &lt;- makeSequenceTable(mergers)\ndim(BCS_35)\n# [1]   384 29481\ntable(nchar(getSequences(BCS_35)))\n\n#exporting files to use in the next part of the workflow\nsaveRDS(BCS_35, \"BCS_35/BCS_35.rds\")\n\n#tracking changes through each step\ngetN &lt;- function(x) sum(getUniques(x))\ntrack &lt;-    cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN),\n                  sapply(mergers, getN))\ncolnames(track) &lt;- c(\"input\", \"filtered\", \"denoisedF\", \"denoisedR\",\n                        \"merged\")\nrownames(track) &lt;- sam.names\nwrite.table(track, \"BCS_35/BCS_35_read_changes.txt\", sep = \"\\t\", quote = FALSE,\n            col.names=NA)\n\nread_length &lt;-  data.frame(nchar(getSequences(BCS_35)))\n\ncolnames(read_length) &lt;- \"length\"\n\nplot_BCS_35 &lt;- qplot(length, data = read_length, \n                     geom = \"histogram\", binwidth = 1, \n                     xlab = \"read length\", \n                     ylab = \"total variants\", \n                     xlim = c(225,275)) \nggsave(\"figures/read_length_before_pseudo_BCS_35.png\", plot_BCS_35, width = 7, height = 3)\n\nsave.image(\"BCS_35.rdata\")\n\n\n\n\n\n\nconda activate R\nRscript BCS_26.R\nRscript BCS_28.R\nRscript BCS_29.R\nRscript BCS_30.R\nRscript BCS_34.R\nRscript BCS_35.R\n\nMerged Runs\nOnce these workflows finish, we then merge the 6 sequence tables together and proceed with chimera removal and taxonomic classification.\n\nProcessing script for run merged runs#!/usr/bin/env Rscript\nset.seed(919191)\nlibrary(dada2); packageVersion(\"dada2\")\nlibrary(ggplot2)\nlibrary(ff)\nlibrary(phyloseq)\nlibrary(gridExtra)\nlibrary(dplyr)\nlibrary(decontam)\nlibrary(grid)\nlibrary(ShortRead); packageVersion(\"ShortRead\")\nlibrary(Biostrings); packageVersion(\"Biostrings\")\nlibrary(DECIPHER); packageVersion(\"DECIPHER\")\n\n########################################\n#\n# 2. MERGE ALL SEQ TABS\n#\n########################################\n\n############################################\n# PROBLEM: Duplicated sample names \n# detected in the sequence table row names: \n# 7512-G, 7512-H, 7512-M, 7512-S\n# BCS_30 and BCS_35\n# FOR BCS_30 changed name to 7512A...\n# FOR BCS_35 changed name to 7512B...\n############################################\n\nBCS_26 &lt;- readRDS(\"BCS_26/BCS_26.rds\")\nBCS_28 &lt;- readRDS(\"BCS_28/BCS_28.rds\")\nBCS_29 &lt;- readRDS(\"BCS_29/BCS_29.rds\")\nBCS_30 &lt;- readRDS(\"BCS_30/BCS_30.rds\")\nBCS_34 &lt;- readRDS(\"BCS_34/BCS_34.rds\")\nBCS_35 &lt;- readRDS(\"BCS_35/BCS_35.rds\")\n\nseqtab.merge &lt;- mergeSequenceTables(BCS_26, BCS_28, BCS_29, \n                                    BCS_30, BCS_34, BCS_35\n                                    )\ndim(seqtab.merge)\ntable(nchar(getSequences(seqtab.merge)))\n\nread_length_all &lt;-  data.frame(nchar(getSequences(seqtab.merge)))\ncolnames(read_length_all) &lt;- \"length\"\nplot_all &lt;- qplot(length, data = read_length_all, \n                  geom = \"histogram\", binwidth = 1, \n                  xlab = \"read length\", \n                  ylab = \"total variants\", \n                  xlim = c(200,400)) \nggsave(\"figures/read_length_before_collapse.png\", \n        plot_all, width = 7, height = 3)\nsaveRDS(seqtab.merge, \"2.seqtab.merge.rds\")\n\nsave.image(\"rdata/2.merge.seqtabs.rdata\")\n\n########################################\n#\n# collapseNoMismatch\n# TESTED, only minor differences in\n# 13 samples. Takes long time to run\n#\n########################################\n\n# seqtab_to_collapse &lt;- collapseNoMismatch(st_all, minOverlap = 20, orderBy = \"abundance\",\n#   identicalOnly = FALSE, vec = TRUE, band = -1, verbose = TRUE)\n\n# dim(seqtab_to_collapse)\n# table(nchar(getSequences(seqtab_to_collapse)))\n\n# read_length_all_collapse &lt;-  data.frame(nchar(getSequences(seqtab_to_collapse)))\n# colnames(read_length_all_collapse) &lt;- \"length\"\n# plot_all_collapse &lt;- qplot(length, data = read_length_all_collapse, geom = \"histogram\", binwidth = 1, xlab = \"read length\", ylab = \"total variants\", xlim = c(200,400)) \n# ggsave(\"figures/read_length_after_collapse.png\", plot_all_collapse, width = 7, height = 3)\n# saveRDS(seqtab_to_collapse, \"seqtab_after_collapse.rds\")\n\n# save.image(\"rdata/2_merge_seqtabs_collapsed.rdata\")\n\n########################################\n#\n# 3. REMOVING CHIMERAS\n#\n########################################\n\n############################################\n# PROBLEM: Duplicated sample names \n# detected in the sequence table row names: \n# 7512-G, 7512-H, 7512-M, 7512-S\n# BCS_30 and BCS_35\n# FOR BCS_30 changed name to 7512A...\n# FOR BCS_35 changed name to 7512B...\n############################################\n\n## REMVOE OUTLIER READ LENGTHS\nseqtab &lt;- seqtab.merge\n#seqtab.merge &lt;- readRDS(\"seqtab_before_collapse.rds\")\ntable(nchar(getSequences(seqtab)))\n\n################################################################################# \n## \n## 220   221   222   223   224   225   226   227   228   229   230   231   232\n##   125    67    14    36    20    13    10    25     9     6     4    27     2\n##   234   235   236   237   238   239   240   241   242   243   244   245   246\n##     9     8  1373   151    46     6    99   407   298   452    31    14    13\n##   247   248   249   250   251   252   253   254   255   256   257   258   259\n##    26    23    19    49   159  3587 84485  3772   319   123    96    20    10\n##   260   261   262   263   264   265   266   267   268   269   270   271   272\n##     8    16     9     4     2     1     1     1     4     2     9     8     4\n##   273   274   275   276   277   278   279   280   281   282   284   285   286\n##     7     3     2     5     1     7     4     1     1     2     1     4     4\n##   288   289   290   291   292   293   294   295   296   297   298   300   303\n##     1     3     1     2     4     8     7     2     3     2     3     2     3\n##   304   305   307   308   309   310   311   312   313   315   316   317   318\n##     1     5     2     3     2     1     3     1     3     1     4     1     3\n##   319   320   321   322   323   324   325   326   328   329   330   332   333\n##     3     1     2     3     2     1     3     1     3     3     2     1     3\n##   334   335   336   337   338   339   340   341   342   343   344   345   346\n##    13     6     7    18     5    25    16    70    52     8     7     8     4\n##   347   348   349   350   351   352   353   354   355   356   357   358   359\n##    25    17    21    10     2    11     1     1     7     6    31     6    15\n##   360   361   362   363   364   365   366   367   368   369   370   371   372\n##    21   161   188    43   141   108    19     9    26     5     3     3     8\n##   373   374   376   377   378   379   380   384   385   386   387   388\n##    11     2     3     5     2     3     1     1     1     1     2     1\n##\n#################################################################################\n\n#######################################################\n## ----REMOVE OUTLIER READ LENGTHS------------------ ##\n#######################################################\n\nseqtab.trim &lt;- seqtab[,nchar(colnames(seqtab)) %in% seq(252, 254)]\ndim(seqtab.trim)\ntable(nchar(getSequences(seqtab.trim)))\n\n#####################\n##   252   253   254\n##  3587 84485  3772\n#####################\n\n#######################################################\n## ----chimera pooled------------------------------- ##\n#######################################################\n\nseqtab.trim.nochim.pool &lt;- \n          removeBimeraDenovo(seqtab.trim, \n                             method = \"pooled\", \n                             multithread = 20, \n                             verbose = TRUE)\ndim(seqtab.trim.nochim.pool)\nsum(seqtab.trim.nochim.pool)/sum(seqtab.trim)\n\nsaveRDS(seqtab.trim.nochim.pool, \"3.seqtab.trim.nochim.pool.rds\")\n\ntable(nchar(getSequences(seqtab.trim.nochim.pool)))\n\ntable(colSums(seqtab.trim.nochim.pool &gt; 0))\ntable(rowSums(seqtab.trim.nochim.pool &gt; 0))\n\n##########################################################\n## ----chimera consensus------------------------------- ##\n##########################################################\n\nseqtab.trim.nochim.consensus &lt;- \n           removeBimeraDenovo(seqtab.trim, \n                              method = \"consensus\", \n                              multithread = 20, \n                              verbose = TRUE)\ndim(seqtab.trim.nochim.consensus)\nsum(seqtab.trim.nochim.consensus)/sum(seqtab.trim)\n\nsaveRDS(seqtab.trim.nochim.consensus, \"3.seqtab.trim.nochim.consensus.rds\")\n\ntable(nchar(getSequences(seqtab.trim.nochim.consensus)))\n\ntable(colSums(seqtab.trim.nochim.consensus &gt; 0))\ntable(rowSums(seqtab.trim.nochim.consensus &gt; 0))\n\n##########################################################\n## ----tracking changes-------------------------------- ##\n##########################################################\n\ngetN &lt;- function(x) sum(getUniques(x))\ntrack &lt;- cbind(rowSums(seqtab), \n               rowSums(seqtab.trim), \n               rowSums(seqtab.trim.nochim.pool), \n               rowSums(seqtab.trim.nochim.consensus))\n\ncolnames(track) &lt;- c(\"merged\", \"trim\", \"chimera_pool\", \"chimera_concensus\")\nwrite.table(track, \"3.chimera_read_changes_pipeline.txt\", \n            sep = \"\\t\", quote = FALSE, col.names=NA)\n\nsave.image(\"rdata/3.trim.chimera.rdata\")\n\n########################################\n#\n# 4. ASSIGNING TAXONOMY\n#\n########################################\n\n###########################################################\n# reference datasets formatted for DADA2 can be found here: \n# https://benjjneb.github.io/dada2/training.html\n###########################################################\n\n########################################\n#\n# TAXONOMY chimera = pooled\n#\n########################################\n\n# seqtab &lt;- readRDS(\"3.seqtab.trim.nochim.pool.rds\")\n\n########################################\n# TAXONOMY = silva\n########################################\nseqtab.pool &lt;- seqtab.trim.nochim.pool\n\ntax_silva_v138.pool &lt;- \n              assignTaxonomy(seqtab.pool, \n              \"TAXONOMY_FILES/silva_nr99_v138.1_train_set.fa.gz\", \n              multithread = TRUE, verbose = TRUE)\nsaveRDS(tax_silva_v138.pool, \"4.tax_silva_v138.pool.rds\")\n\ntax_silva_v132.pool &lt;- \n              assignTaxonomy(seqtab.pool, \n              \"TAXONOMY_FILES/silva_nr_v132_train_set.fa.gz\", \n              multithread = TRUE, verbose = TRUE)\nsaveRDS(tax_silva_v132.pool, \"4.tax_silva_v132.pool.rds\")\n\n########################################\n# TAXONOMY = RDP\n########################################\n\ntax_rdp_v138.pool &lt;- \n             assignTaxonomy(seqtab.pool, \n             \"TAXONOMY_FILES/rdp_train_set_18.fa.gz\", \n             multithread = TRUE, verbose = TRUE)\nsaveRDS(tax_rdp_v138.pool, \"4.tax_rdp_v138.pool.rds\")\n\n########################################\n#\n# TAXONOMY chimera = consensus\n#\n########################################\n\n#remove(list = ls())\n#seqtab &lt;- readRDS(\"3.seqtab.trim.nochim.consensus.rds\")\n#objects()\n\n########################################\n# TAXONOMY = silva\n########################################\nseqtab.consensus &lt;- seqtab.trim.nochim.consensus\n\ntax_silva_v138.consensus &lt;- \n              assignTaxonomy(seqtab.consensus, \n              \"TAXONOMY_FILES/silva_nr99_v138.1_train_set.fa.gz\", \n              multithread = TRUE, verbose = TRUE)\nsaveRDS(tax_silva_v138.consensus, \"4.tax_silva_v138.consensus.rds\")\n\ntax_silva_v132.consensus &lt;- \n              assignTaxonomy(seqtab.consensus, \n              \"TAXONOMY_FILES/silva_nr_v132_train_set.fa.gz\", \n              multithread = TRUE, verbose = TRUE)\nsaveRDS(tax_silva_v132.consensus, \"4.tax_silva_v132.consensus.rds\")\n\n########################################\n# TAXONOMY = RDP\n########################################\n\ntax_rdp_v138.consensus &lt;- \n              assignTaxonomy(seqtab.consensus, \n              \"TAXONOMY_FILES/rdp_train_set_18.fa.gz\", \n              multithread = TRUE, verbose = TRUE)\nsaveRDS(tax_rdp_v138.consensus, \"4.tax_rdp_v138.consensus.rds\")\n\n########################################\n# TAXONOMY = ITGDB\n########################################\n\ntax_itgdb.consensus &lt;- \n              assignTaxonomy(seqtab.consensus, \n              \"TAXONOMY_FILES/itgdb_dada2.fa\", \n              multithread = TRUE, verbose = TRUE)\nsaveRDS(tax_itgdb.consensus, \"4.tax_itgdb.consensus.rds\")\n\n########################################\n# TAXONOMY = GSRDB\n########################################\n\ntax_gsrdb.consensus &lt;- \n              assignTaxonomy(seqtab.consensus, \n              \"TAXONOMY_FILES/gsrdb_dada2.fa\", \n              multithread = TRUE, verbose = TRUE)\nsaveRDS(tax_gsrdb.consensus, \"4.tax_gsrdb.consensus.rds\")\n\nsave.image(\"rdata/4.dada2.pipeline.rdata\")\n\nsessionInfo()\ndevtools::session_info()\n\nquit()\n\n\n\nconda activate R\nRscript 2.dada2_pipeline.R\n\nIn the resources listed above, we include a table that summarizes read changes for each sample through the pipeline."
  },
  {
    "objectID": "docs/data/data-mg.html",
    "href": "docs/data/data-mg.html",
    "title": "Data & Scripts",
    "section": "",
    "text": "Quick access to pipeline processing scripts and raw data. With these scripts and associated data you can run the processing steps for each analysis.\nAll sequence data is linked to projects on the European Nucleotide Archive (ENA). The scripts and associated files can be downloaded as .zip archives by clicking the links. Or if you prefer, you can copy the scripts directly from the code blocks below."
  },
  {
    "objectID": "docs/data/data-mg.html#metagenomic-data-and-scripts",
    "href": "docs/data/data-mg.html#metagenomic-data-and-scripts",
    "title": "Data & Scripts",
    "section": "Metagenomic Data and Scripts",
    "text": "Metagenomic Data and Scripts\n\n\n\n\nName\n\n\nDescription\n\n\n\n\n\nSequence data\n\n\nRaw metagenomic reads (forward and reverse) for 58 samples. \n\n\n\n\nSample data\n\n\n16S rRNA sample data \n\n\n\n\nAnvi’o config file\n\n\nJSON-formatted configuration file for metagenomic workflow \n\n\n\n\nSample mapping file\n\n\nFile needed for workflow that maps fastq files sample names and co-assembly group \n\n\n\n\nHydra job scripts\n\n\nHydra job scripts \n\n\n\n\nQC report\n\n\nTable showing the results of quality control filtering for each sample. \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "docs/data/data-mg.html#metagenomic-processing",
    "href": "docs/data/data-mg.html#metagenomic-processing",
    "title": "Data & Scripts",
    "section": "Metagenomic Processing",
    "text": "Metagenomic Processing\nFor metagenomic analysis we used the anvi’o platform, which brings together many aspects of cutting-edge computational strategies of data-enabled microbiology, including genomics, metagenomics, metatranscriptomics, pangenomics, metapangenomics, phylogenomics, and microbial population genetics in an integrated and easy-to-use fashion through extensive interactive visualization capabilities. It is basically heavy metal.\nHere we run the anvi’o metagenomic workflow. For this pipeline we need the following:\nA. JSON-formatted config file, which we can get by running the following:\n\nanvi-run-workflow -w metagenomics --get-default-config default_mg.json\n\nB. Modify the file to suite your needs. Here is our final config file.\n\njson formatted config file for anvi’o metagenomic workflow.{\n    \"fasta_txt\": \"\",\n    \"anvi_gen_contigs_database\": {\n        \"--project-name\": \"{group}\",\n        \"--description\": \"\",\n        \"--skip-gene-calling\": \"\",\n        \"--ignore-internal-stop-codons\": \"\",\n        \"--skip-mindful-splitting\": \"\",\n        \"--contigs-fasta\": \"\",\n        \"--split-length\": \"\",\n        \"--kmer-size\": \"\",\n        \"--skip-predict-frame\": \"\",\n        \"--prodigal-translation-table\": \"\",\n        \"threads\": 14\n    },\n    \"centrifuge\": {\n        \"threads\": 14,\n        \"run\": true,\n        \"db\": \"/pool/genomics/stri_istmobiome/dbs/centrifuge_dbs/p+h+v\"\n    },\n    \"anvi_run_hmms\": {\n        \"run\": true,\n        \"threads\": 14,\n        \"--also-scan-trnas\": true,\n        \"--installed-hmm-profile\": \"\",\n        \"--hmm-profile-dir\": \"\",\n        \"--add-to-functions-table\": \"\"\n    },\n    \"anvi_run_kegg_kofams\": {\n        \"run\": true,\n        \"threads\": 14,\n        \"--kegg-data-dir\": \"/pool/genomics/stri_istmobiome/dbs/kegg_kofam/\",\n        \"--hmmer-program\": \"\",\n        \"--keep-all-hits\": \"\",\n        \"--log-bitscores\": \"\",\n        \"--just-do-it\": \"\"\n    },\n    \"anvi_run_ncbi_cogs\": {\n        \"run\": true,\n        \"threads\": 14,\n        \"--cog-data-dir\": \"/pool/genomics/stri_istmobiome/dbs/cog_db/\",\n        \"--temporary-dir-path\": \"/pool/genomics/stri_istmobiome/dbs/cog_db/tmp_mega/\",\n        \"--search-with\": \"\"\n    },\n    \"anvi_run_scg_taxonomy\": {\n        \"run\": true,\n        \"threads\": 14,\n        \"--scgs-taxonomy-data-dir\": \"\"\n    },\n    \"anvi_run_trna_scan\": {\n        \"run\": true,\n        \"threads\": 14,\n        \"--trna-cutoff-score\": \"\"\n    },\n    \"anvi_script_reformat_fasta\": {\n        \"run\": true,\n        \"--prefix\": \"{group}\",\n        \"--simplify-names\": true,\n        \"--keep-ids\": \"\",\n        \"--exclude-ids\": \"\",\n        \"--min-len\": \"1000\",\n        \"--seq-type\": \"\",\n        \"threads\": 7\n    },\n    \"emapper\": {\n        \"--database\": \"bact\",\n        \"--usemem\": true,\n        \"--override\": true,\n        \"path_to_emapper_dir\": \"\",\n        \"threads\": \"\"\n    },\n    \"anvi_script_run_eggnog_mapper\": {\n        \"--use-version\": \"0.12.6\",\n        \"run\": \"\",\n        \"--cog-data-dir\": \"\",\n        \"--drop-previous-annotations\": \"\",\n        \"threads\": \"\"\n    },\n    \"samples_txt\": \"samples.txt\",\n    \"metaspades\": {\n        \"additional_params\": \"--only-assembler\",\n        \"threads\": 7,\n        \"run\": \"\",\n        \"use_scaffolds\": \"\"\n    },\n    \"megahit\": {\n        \"--min-contig-len\": 1000,\n        \"--memory\": 0.9,\n        \"threads\": 14,\n        \"run\": true,\n        \"--min-count\": \"\",\n        \"--k-min\": \"\",\n        \"--k-max\": \"\",\n        \"--k-step\": \"\",\n        \"--k-list\": \"\",\n        \"--no-mercy\": \"\",\n        \"--no-bubble\": \"\",\n        \"--merge-level\": \"\",\n        \"--prune-level\": \"\",\n        \"--prune-depth\": \"\",\n        \"--low-local-ratio\": \"\",\n        \"--max-tip-len\": \"\",\n        \"--no-local\": \"\",\n        \"--kmin-1pass\": \"\",\n        \"--presets\": \"meta-sensitive\",\n        \"--mem-flag\": \"\",\n        \"--use-gpu\": \"\",\n        \"--gpu-mem\": \"\",\n        \"--keep-tmp-files\": \"\",\n        \"--tmp-dir\": \"\",\n        \"--continue\": true,\n        \"--verbose\": \"\"\n    },\n    \"idba_ud\": {\n        \"--min_contig\": 1000,\n        \"threads\": 7,\n        \"run\": \"\",\n        \"--mink\": \"\",\n        \"--maxk\": \"\",\n        \"--step\": \"\",\n        \"--inner_mink\": \"\",\n        \"--inner_step\": \"\",\n        \"--prefix\": \"\",\n        \"--min_count\": \"\",\n        \"--min_support\": \"\",\n        \"--seed_kmer\": \"\",\n        \"--similar\": \"\",\n        \"--max_mismatch\": \"\",\n        \"--min_pairs\": \"\",\n        \"--no_bubble\": \"\",\n        \"--no_local\": \"\",\n        \"--no_coverage\": \"\",\n        \"--no_correct\": \"\",\n        \"--pre_correction\": \"\",\n        \"use_scaffolds\": \"\"\n    },\n    \"iu_filter_quality_minoche\": {\n        \"run\": true,\n        \"--ignore-deflines\": true,\n        \"--visualize-quality-curves\": \"\",\n        \"--limit-num-pairs\": \"\",\n        \"--print-qual-scores\": \"\",\n        \"--store-read-fate\": \"\",\n        \"threads\": 7\n    },\n    \"gzip_fastqs\": {\n        \"run\": true,\n        \"threads\": 7\n    },\n    \"bowtie\": {\n        \"additional_params\": \"--no-unal\",\n        \"threads\": 7\n    },\n    \"samtools_view\": {\n        \"additional_params\": \"-F 4\",\n        \"threads\": 7\n    },\n    \"anvi_profile\": {\n        \"threads\": 7,\n        \"--sample-name\": \"{sample}\",\n        \"--overwrite-output-destinations\": true,\n        \"--report-variability-full\": \"\",\n        \"--skip-SNV-profiling\": \"\",\n        \"--profile-SCVs\": true,\n        \"--description\": \"\",\n        \"--skip-hierarchical-clustering\": \"\",\n        \"--distance\": \"\",\n        \"--linkage\": \"\",\n        \"--min-contig-length\": \"\",\n        \"--min-mean-coverage\": \"\",\n        \"--min-coverage-for-variability\": \"\",\n        \"--cluster-contigs\": \"\",\n        \"--contigs-of-interest\": \"\",\n        \"--queue-size\": \"\",\n        \"--write-buffer-size-per-thread\": \"\",\n        \"--fetch-filter\": \"\",\n        \"--min-percent-identity\": \"\",\n        \"--max-contig-length\": \"\"\n    },\n    \"anvi_merge\": {\n        \"--sample-name\": \"{group}\",\n        \"--overwrite-output-destinations\": true,\n        \"--description\": \"\",\n        \"--skip-hierarchical-clustering\": \"\",\n        \"--enforce-hierarchical-clustering\": \"\",\n        \"--distance\": \"\",\n        \"--linkage\": \"\",\n        \"threads\": 14\n    },\n    \"import_percent_of_reads_mapped\": {\n        \"run\": true,\n        \"threads\": 7\n    },\n    \"krakenuniq\": {\n        \"threads\": 3,\n        \"--gzip-compressed\": true,\n        \"additional_params\": \"\",\n        \"run\": \"\",\n        \"--db\": \"\"\n    },\n    \"remove_short_reads_based_on_references\": {\n        \"delimiter-for-iu-remove-ids-from-fastq\": \" \",\n        \"dont_remove_just_map\": \"\",\n        \"references_for_removal_txt\": \"\",\n        \"threads\": \"\"\n    },\n    \"anvi_cluster_contigs\": {\n        \"--collection-name\": \"{driver}\",\n        \"run\": \"\",\n        \"--driver\": \"\",\n        \"--just-do-it\": \"\",\n        \"--additional-params-concoct\": \"\",\n        \"--additional-params-metabat2\": \"\",\n        \"--additional-params-maxbin2\": \"\",\n        \"--additional-params-dastool\": \"\",\n        \"--additional-params-binsanity\": \"\",\n        \"threads\": \"\"\n    },\n    \"gen_external_genome_file\": {\n        \"threads\": \"\"\n    },\n    \"export_gene_calls_for_centrifuge\": {\n        \"threads\": \"\"\n    },\n    \"anvi_import_taxonomy_for_genes\": {\n        \"threads\": \"\"\n    },\n    \"annotate_contigs_database\": {\n        \"threads\": \"\"\n    },\n    \"anvi_get_sequences_for_gene_calls\": {\n        \"threads\": \"\"\n    },\n    \"gunzip_fasta\": {\n        \"threads\": \"\"\n    },\n    \"reformat_external_gene_calls_table\": {\n        \"threads\": \"\"\n    },\n    \"reformat_external_functions\": {\n        \"threads\": \"\"\n    },\n    \"import_external_functions\": {\n        \"threads\": \"\"\n    },\n    \"anvi_run_pfams\": {\n        \"run\": true,\n        \"--pfam-data-dir\": \"/pool/genomics/stri_istmobiome/dbs/pfam_db\",\n        \"threads\": 14\n    },\n    \"iu_gen_configs\": {\n        \"--r1-prefix\": \"\",\n        \"--r2-prefix\": \"\",\n        \"threads\": 14\n    },\n    \"gen_qc_report\": {\n        \"threads\": \"\"\n    },\n    \"merge_fastqs_for_co_assembly\": {\n        \"threads\": \"\"\n    },\n    \"merge_fastas_for_co_assembly\": {\n        \"threads\": \"\"\n    },\n    \"bowtie_build\": {\n        \"additional_params\": \"\",\n        \"threads\": \"\"\n    },\n    \"anvi_init_bam\": {\n        \"threads\": \"\"\n    },\n    \"krakenuniq_mpa_report\": {\n        \"threads\": \"\"\n    },\n    \"import_krakenuniq_taxonomy\": {\n        \"--min-abundance\": \"\",\n        \"threads\": \"\"\n    },\n    \"anvi_summarize\": {\n        \"additional_params\": \"\",\n        \"run\": \"\",\n        \"threads\": \"\"\n    },\n    \"anvi_split\": {\n        \"additional_params\": \"\",\n        \"run\": \"\",\n        \"threads\": \"\"\n    },\n    \"references_mode\": \"\",\n    \"all_against_all\": \"\",\n    \"kraken_txt\": \"\",\n    \"collections_txt\": \"\",\n    \"output_dirs\": {\n        \"FASTA_DIR\": \"02_FASTA\",\n        \"CONTIGS_DIR\": \"03_CONTIGS\",\n        \"QC_DIR\": \"01_QC\",\n        \"MAPPING_DIR\": \"04_MAPPING\",\n        \"PROFILE_DIR\": \"05_ANVIO_PROFILE\",\n        \"MERGE_DIR\": \"06_MERGED\",\n        \"TAXONOMY_DIR\": \"07_TAXONOMY\",\n        \"SUMMARY_DIR\": \"08_SUMMARY\",\n        \"SPLIT_PROFILES_DIR\": \"09_SPLIT_PROFILES\",\n        \"LOGS_DIR\": \"00_LOGS\"\n    },\n    \"max_threads\": \"\",\n    \"config_version\": \"3\",\n    \"workflow_name\": \"metagenomics\"\n}\n\n\nC. A four-column tab-delimited text file where each row is a sample. The column headers are as follows:\n\n\nsample: the sample name\n\ngroup: group for co-assembly\n\nr1: sample forward reads\n\nr2: sample reverse reads\n\nAnd here is a little mock example. The file we used (samples.txt) is linked in the table above.\nsample  group   r1  r2\nEP_ALPH_AREN_GL_P01 EP  /path/to/files/sample-1_R1_001.fastq.gz /path/to/files/sample-1_R2_001.fastq.gz\nEP_ALPH_AREN_HP_P01 EP  /path/to/files/sample-2_R1_001.fastq.gz /path/to/files/sample-2_R2_001.fastq.gz\nWA_ALPH_WEBS_HP_P01 WA  /path/to/files/sample_3_R1_001.fastq.gz /path/to/files/sample-3_R2_001.fastq.gz\nWA_ALPH_WEBS_MG_P01 WA  /path/to/files/sample-4_R1_001.fastq.gz /path/to/files/sample-4_R2_001.fastq.gz\nD. And of course a bunch of fastq files (linked in the table above).\nOnce all of these pieces are in place, let er rip by running this command:\n\nanvi-run-workflow --workflow metagenomics \\\n                  --get-default-config default_mg.json  \\\n                  --list-dependencies\n\nThe entire pipeline can take several days to run depending on the size and complexity of your dataset.\nIn the resources listed above, we include a table that summarizes the initial QC screening for each sample."
  },
  {
    "objectID": "docs/mg-workflows/aa-results/index.html",
    "href": "docs/mg-workflows/aa-results/index.html",
    "title": "3. Assembly & Annotation Results",
    "section": "",
    "text": "Click here for libraries and setup information.knitr::opts_chunk$set(echo = TRUE, eval = FALSE)\nset.seed(919191)\nlibrary(ggplot2); packageVersion(\"ggplot2\")\nlibrary(Biostrings); packageVersion(\"Biostrings\")\npacman::p_load(tidyverse, gridExtra, grid,\n               formatR, reactable, gdata, patchwork,\n               kableExtra, magrittr, \n               install = FALSE, update = FALSE)\n\noptions(scipen = 999)\nknitr::opts_current$get(c(\n  \"cache\",\n  \"cache.path\",\n  \"cache.rebuild\",\n  \"dependson\",\n  \"autodep\"\n))\n#root &lt;- find_root(has_file(\"_quarto.yml\"))\n#source(file.path(root, \"assets\", \"functions.R\"))",
    "crumbs": [
      "Metagenomics",
      "3. Assembly & Annotation Results"
    ]
  },
  {
    "objectID": "docs/mg-workflows/aa-results/index.html#qc-results",
    "href": "docs/mg-workflows/aa-results/index.html#qc-results",
    "title": "3. Assembly & Annotation Results",
    "section": "QC Results",
    "text": "QC Results\nThe first thing we should do is look at the results of the initial QC step. For each sample, anvi’o spits out individual quality control reports. Thankfully anvi’o also concatenates those files into one table. This table contains information like the number of pairs analyzed, the total pairs passed, etc.\n Download QC data.",
    "crumbs": [
      "Metagenomics",
      "3. Assembly & Annotation Results"
    ]
  },
  {
    "objectID": "docs/mg-workflows/aa-results/index.html#assembly-results",
    "href": "docs/mg-workflows/aa-results/index.html#assembly-results",
    "title": "3. Assembly & Annotation Results",
    "section": "Assembly Results",
    "text": "Assembly Results\nNext we can look at the results of the co-assembly, the number of HMM hits, and the estimated number of predicted genomes. These data not only give us a general idea of assembly quality but will also help us decide parameters for automatic clustering down the road.\nWe can use anvi’o to generate a simple table of contig stats for this assembly.\n\nanvi-display-contigs-stats PAN-contigs.db \\\n                           --output-file contig-stats.txt \\\n                           --report-as-text",
    "crumbs": [
      "Metagenomics",
      "3. Assembly & Annotation Results"
    ]
  },
  {
    "objectID": "docs/mg-workflows/aa-results/index.html#short-read-taxonomy",
    "href": "docs/mg-workflows/aa-results/index.html#short-read-taxonomy",
    "title": "3. Assembly & Annotation Results",
    "section": "Short-read Taxonomy",
    "text": "Short-read Taxonomy\nSince the Kraken2 classification was performed BEFORE the assembly we can look at the Krona plots for each individual sample. Here samples are separated by site.\n\nanvi-export-table PROFILE.db --table layer_additional_data \\\n                             --output-file  layer_additional_data.txt \\\n                             --index item_name \\\n                             --columns data_key \\\n                             --values data_value \\\n                             --matrix-format",
    "crumbs": [
      "Metagenomics",
      "3. Assembly & Annotation Results"
    ]
  },
  {
    "objectID": "docs/mg-workflows/setup/index.html",
    "href": "docs/mg-workflows/setup/index.html",
    "title": "1. Annotation Databases",
    "section": "",
    "text": "Click here for libraries and setup information.knitr::opts_chunk$set(echo = TRUE, eval = FALSE)\nset.seed(919191)\nlibrary(ggplot2); packageVersion(\"ggplot2\")\nlibrary(Biostrings); packageVersion(\"Biostrings\")\npacman::p_load(tidyverse, gridExtra, grid,\n               formatR, reactable, gdata, patchwork,\n               kableExtra, magrittr, \n               install = FALSE, update = FALSE)\n\noptions(scipen = 999)\nknitr::opts_current$get(c(\n  \"cache\",\n  \"cache.path\",\n  \"cache.rebuild\",\n  \"dependson\",\n  \"autodep\"\n))\n#root &lt;- find_root(has_file(\"_quarto.yml\"))\n#source(file.path(root, \"assets\", \"functions.R\"))\nThere are two main types of annotations we are interested in for this metagenomic project—taxonomic and functional—and there are many, many ways to accomplish both of these goals. This next section involves building the databases and installing any additional tools we need for annotation.\nLet’s start with the tools and databases for taxonomic classification.",
    "crumbs": [
      "Metagenomics",
      "1. Annotation Databases"
    ]
  },
  {
    "objectID": "docs/mg-workflows/setup/index.html#kaiju",
    "href": "docs/mg-workflows/setup/index.html#kaiju",
    "title": "1. Annotation Databases",
    "section": "Kaiju",
    "text": "Kaiju\nKaiju (Menzel, Ng, and Krogh 2016)… finds maximum (in-)exact matches on the protein-level using the Burrows–Wheeler transform. Kaiju is a program for the taxonomic classification… of metagenomic DNA. Reads are directly assigned to taxa using the NCBI taxonomy and a reference database of protein sequences from microbial and viral genomes.\nFirst we need to install Kaiju. Again, I will run kaiju in a separate conda environment. Now I can either install kaiju from the source code or as conda package. The latter is easier but often conda packages may lag behind the source code versions. I usually compare the release dates of the conda package with the source code and look at the number of downloads. In this case, the conda version of kaiju looks fine.\n\n# create generic environment\nconda create -n kaiju\nconda activate kaiji\nconda install -c bioconda kaiju\n\nAfter kaiju is installed, the next thing to do is generate the database. You can find a description of kaiju databases in the section on dreating the Kaiju index. I downloaded and formatted the progenomes database, whic contains representative sets of genomes from the proGenomes database and viruses from the NCBI RefSeq database.\nSimply run the kaiju-makedb command and specify a database.\n\nsource activate kaiju\nmkdir kaiju\ncd kaiju\nkaiju-makedb -s progenomes\n\nThe progenomes database is 95GB.\n\n\n\n\n\n\nExpand for the KAIJU_DB_BUILD Hydra script\n\n\n\n\n\n# /bin/sh\n# ----------------Parameters---------------------- #\n#$ -S /bin/sh\n#$ -pe mthread 15\n#$ -q sThM.q\n#$ -l mres=225G,h_data=15G,h_vmem=15G,himem\n#$ -cwd\n#$ -j y\n#$ -N job_build_anvio_dbs\n#$ -o hydra_logs/job_build_kaiju.log\n#\n# ----------------Modules------------------------- #\nmodule load gcc/4.9.2\n#\n# ----------------Your Commands------------------- #\n#\necho + `date` job $JOB_NAME started in $QUEUE with jobID=$JOB_ID on $HOSTNAME\necho + NSLOTS = $NSLOTS\n#\n# ----------------CALLING ANVIO------------------- #\nexport PATH=/home/scottjj/miniconda3:$PATH\nexport PATH=/home/scottjj/miniconda3/bin:$PATH\n#\nsource activate kaiju\nmkdir kaiju\ncd kaiju\nkaiju-makedb -s progenomes",
    "crumbs": [
      "Metagenomics",
      "1. Annotation Databases"
    ]
  },
  {
    "objectID": "docs/mg-workflows/setup/index.html#kraken2",
    "href": "docs/mg-workflows/setup/index.html#kraken2",
    "title": "1. Annotation Databases",
    "section": "Kraken2",
    "text": "Kraken2\nKraken2 (Wood, Lu, and Langmead 2019) is a novel metagenomics classifier that combines the fast k-mer-based classification of Kraken with an efficient algorithm for assessing the coverage of unique k-mers found in each species in a dataset.\nInstalled in separate conda environment\n\nconda create -n kraken2\nconda install kraken2\nconda activate kraken2\n\nThe standard way of installing a Kraken2 database is to run kraken2-build and calling the --standard flag, which will construct a database containing Refeq archaea, bacteria, viral, plasmid, human data plus data from UniVec_Core. Here is the command.\n\nkraken2-build --standard --db kraken2_db --threads $NSLOTS\n\nThe first problem arises because of issues with the NCBI servers and kraken2-build use of rsync. So we add the flag --use-ftp. Like so…\n\nkraken2-build --standard --db kraken2_db --threads $NSLOTS --use-ftp\n\nBut this did not work either–it failed repeatedly. If you look at the Kraken2 GitHub issues page you will see that this is a common issue. So instead we tried using one of the prebuilt database, however none of these contained files we needed for downstream analysis. Strike 2. So we tried a different approach.\nFirst, we followed this suggestion to change line 16 of the file PATH_to_KRAKEN2/download_taxonomy.sh from this…\n\nFTP_SERVER=\"ftp://$NCBI_SERVER\"\n\n…to this\n\nTO: FTP_SERVER=\"https://$NCBI_SERVER\"\n\nSupposedly this is to help with the timeout issues from the NCBI servers. Next, we tried building our own database. The first step was to download the NCBI taxonomy.\n\nkraken2-build --download-taxonomy --db kraken2_db --use-ftp\n\nOnce this step was complete, we downloaded individual libraries in order to build a custom Kraken2 database.\n\nkraken2-build --download-library archaea --db kraken2_db --threads $NSLOTS --use-ftp \nkraken2-build --download-library bacteria --db kraken2_db --threads $NSLOTS --use-ftp \nkraken2-build --download-library plasmid --db kraken2_db --threads $NSLOTS --use-ftp \nkraken2-build --download-library viral --db kraken2_db --threads $NSLOTS --use-ftp \nkraken2-build --download-library fungi --db kraken2_db --threads $NSLOTS --use-ftp \nkraken2-build --download-library protozoa --db kraken2_db --threads $NSLOTS --use-ftp \nkraken2-build --download-library UniVec_Core --db kraken2_db --threads $NSLOTS --use-ftp \n\nAnd finally constructed the database.\n\nkraken2-build --build --db kraken2_db\n\nAnd it worked. The whole process took about 8 hours and the final DB is ~90GB.\n\n\n\n\n\n\nExpand for the KRAKEN2_DB_BUILD Hydra script\n\n\n\n\n\n# /bin/sh\n# ----------------Parameters---------------------- #\n#$ -S /bin/sh\n#$ -pe mthread 30\n#$ -q mThM.q\n#$ -l mres=900G,h_data=30G,h_vmem=30G,himem\n#$ -cwd\n#$ -j y\n#$ -N job_00_kraken2-build\n#$ -o job_00_kraken2-build4.job\n#\n# ----------------Modules------------------------- #\nmodule load bioinformatics/blast\n#\n# ----------------Load Envs------------------- #\n#\necho + `date` job $JOB_NAME started in $QUEUE with jobID=$JOB_ID on $HOSTNAME\necho + NSLOTS = $NSLOTS\n#\nexport PATH=/home/scottjj/miniconda3/bin:$PATH\nexport PATH=/home/scottjj/miniconda3/envs/kraken2/bin:$PATH\n\n######################################################################\n# Changed line 16 of this file:\n# FROM: FTP_SERVER=\"ftp://$NCBI_SERVER\"\n# TO: FTP_SERVER=\"https://$NCBI_SERVER\"\n# /home/scottjj/miniconda3/envs/kraken2/share/kraken2-2.1.3-1/libexec/download_taxonomy.sh\n# per https://github.com/DerrickWood/kraken2/issues/515#issuecomment-949354093\n######################################################################\n\nsource activate kraken2\n\n######################################################################\n## STANDARD BUILD-FAILED\n######################################################################\n###kraken2-build --standard --db kraken2_db --threads $NSLOTS --use-ftp\n\n######################################################################\n## CUSTOM BUILD\n######################################################################\n\n## TAXONOMY\nkraken2-build --download-taxonomy --db kraken2_db --use-ftp\n\n## DATABASES\nkraken2-build --download-library archaea --db kraken2_db --threads $NSLOTS --use-ftp \nkraken2-build --download-library bacteria --db kraken2_db --threads $NSLOTS --use-ftp \nkraken2-build --download-library plasmid --db kraken2_db --threads $NSLOTS --use-ftp \nkraken2-build --download-library viral --db kraken2_db --threads $NSLOTS --use-ftp \nkraken2-build --download-library fungi --db kraken2_db --threads $NSLOTS --use-ftp \nkraken2-build --download-library protozoa --db kraken2_db --threads $NSLOTS --use-ftp \nkraken2-build --download-library UniVec_Core --db kraken2_db --threads $NSLOTS --use-ftp \n\n## BUILD\nkraken2-build --build --db kraken2_db\n\nconda deactivate\n\necho = `date` job $JOB_NAME done",
    "crumbs": [
      "Metagenomics",
      "1. Annotation Databases"
    ]
  },
  {
    "objectID": "docs/mg-workflows/setup/index.html#virsorter2",
    "href": "docs/mg-workflows/setup/index.html#virsorter2",
    "title": "1. Annotation Databases",
    "section": "VirSorter2",
    "text": "VirSorter2\nVirSorter (Guo et al. 2021), applies a multi-classifier, expert-guided approach to detect diverse DNA and RNA virus genomes. It has made major updates to its previous version.\nI followed this recipe for installing VirSorter2. Piece of cake.\n\nmamba create -n virsorter2 -c conda-forge -c bioconda virsorter=2\nmamba activate virsorter2\n\nGood to go? Now time to build the VirSorter2 database. Pretty straightforward actually. The general instructions can be found here.\n\nsource activate virsorter2\nvirsorter setup --db-dir /pool/genomics/stri_istmobiome/dbs/virsorter2/ -j $NSLOTS\n\nAnd then a quick test to make sure the database is ok.\n\nmkdir TEST_virsorter2\ncd TEST_virsorter2\nwget -O test.fa https://raw.githubusercontent.com/jiarong/VirSorter2/master/test/8seq.fa\nvirsorter run -w test.out -i test.fa -j $NSLOTS --db-dir dbs/virsorter2/ --tmpdir TEST_virsorter2/tmp_vir --rm-tmpdir --min-length 1500 all\n\nThe uncompressed database is a little over 10GB.\n\n\n\n\n\n\nExpand for the VIRSORTER2_DB_BUILD Hydra script\n\n\n\n\n\n# /bin/sh\n# ----------------Parameters---------------------- #\n#$ -S /bin/sh\n#$ -pe mthread 15\n#$ -q sThM.q\n#$ -l mres=225G,h_data=15G,h_vmem=15G,himem\n#$ -cwd\n#$ -j y\n#$ -N job_build_anvio_dbs\n#$ -o hydra_logs/job_build_virsorter2.log\n#\n# ----------------Modules------------------------- #\nmodule load gcc/4.9.2\n#\n# ----------------Your Commands------------------- #\n#\necho + `date` job $JOB_NAME started in $QUEUE with jobID=$JOB_ID on $HOSTNAME\necho + NSLOTS = $NSLOTS\n#\n# ----------------CALLING ANVIO------------------- #\nexport PATH=/home/scottjj/miniconda3:$PATH\nexport PATH=/home/scottjj/miniconda3/bin:$PATH\n#\n\nsource activate virsorter2\nvirsorter setup --db-dir /pool/genomics/stri_istmobiome/dbs/virsorter2/ -j $NSLOTS\n\n############### TEST VIRSORTER 2 ###############\n\nmkdir TEST_virsorter2\ncd TEST_virsorter2\nwget -O test.fa https://raw.githubusercontent.com/jiarong/VirSorter2/master/test/8seq.fa\nvirsorter run -w test.out -i test.fa -j $NSLOTS --db-dir /pool/genomics/stri_istmobiome/dbs/virsorter2/ --tmpdir TEST_virsorter2/tmp_vir --rm-tmpdir --min-length 1500 all\n\nconda deactivate\n\necho = `date` job $JOB_NAME done",
    "crumbs": [
      "Metagenomics",
      "1. Annotation Databases"
    ]
  },
  {
    "objectID": "docs/mg-workflows/setup/index.html#single-copy-gene-scg-taxonomy",
    "href": "docs/mg-workflows/setup/index.html#single-copy-gene-scg-taxonomy",
    "title": "1. Annotation Databases",
    "section": "Single Copy Gene (SCG) taxonomy",
    "text": "Single Copy Gene (SCG) taxonomy\nAnvi’o has native support for building tRNA and Single Copy Gene (SCG) taxonomy from the Genome Taxonomy Database (GTDB).\n\nanvi-setup-scg-taxonomy -T $NSLOTS\nanvi-setup-trna-taxonomy -T $NSLOTS",
    "crumbs": [
      "Metagenomics",
      "1. Annotation Databases"
    ]
  },
  {
    "objectID": "docs/ssu-workflows/compare/index.html",
    "href": "docs/ssu-workflows/compare/index.html",
    "title": "3. OTU Workflow",
    "section": "",
    "text": "Click here for page build libraries and setup information.knitr::opts_chunk$set(echo = TRUE, eval = FALSE)\nset.seed(919191)\nlibrary(dada2); packageVersion(\"dada2\")\nlibrary(ShortRead); packageVersion(\"ShortRead\")\nlibrary(ggplot2); packageVersion(\"ggplot2\")\nlibrary(Biostrings); packageVersion(\"Biostrings\")\npacman::p_load(tidyverse, gridExtra, grid, miaViz,\n               formatR, reactable, gdata, patchwork,\n               kableExtra, microeco, magrittr, \n               rprojroot,\n               tidySummarizedExperiment, scater,\n               install = FALSE, update = FALSE)\n\noptions(scipen = 999)\nknitr::opts_current$get(c(\n  \"cache\",\n  \"cache.path\",\n  \"cache.rebuild\",\n  \"dependson\",\n  \"autodep\"\n))\n#root &lt;- find_root(has_file(\"_quarto.yml\"))\n#source(file.path(root, \"assets\", \"functions.R\"))\nlibrary(Biostrings)\nlibrary(Biostrings)\nlibrary(magrittr)\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(readr)\n################################################################################\n########### READ IN THE MAPPING FILE FROM mothur2oligo #########################\n################################################################################\nid_map &lt;- readr::read_delim(\"../pipelineFiles_med/BACTERIA_ONLY/intermediate2\", \n                            delim = \"\\t\", \n                            col_names = FALSE)\nid_map &lt;- id_map %&gt;% dplyr::rename(\"MOTHUR_ID\" = 1) %&gt;% \n                     dplyr::rename(., \"MED_ID\" = 2)\nid_map$MOTHUR_ID_RED &lt;- id_map$MOTHUR_ID\nid_map &lt;- id_map %&gt;%\n          mutate(across(\"MOTHUR_ID_RED\",~ gsub(\"_\\\\d+$\",\"\", .)))\nid_map$MOTHUR_UID &lt;- id_map$MOTHUR_ID\nid_map &lt;- id_map %&gt;%\n          mutate(across(\"MOTHUR_UID\",~ gsub(\".*_\",\"\", .)))\n################################################################################\n########### READ IN THE OTU LIST FROM MOTHUR ###################################\n########### get.otulist(list=final.opti_mcc.list, sort=name) ###################\n################################################################################\notu_list &lt;- readr::read_delim(\"final.opti_mcc.0.03.otu\", \n                              delim = \"\\t\", col_names = FALSE)\notu_list &lt;- otu_list %&gt;% dplyr::rename(\"MOTHUR_ID\" = 1) %&gt;% \n                         dplyr::rename(., \"MOTHUR_OTU\" = 2)\n################################################################################\n########### FUNCTION TO READ IN FASTA FILES  ###################################\n################################################################################\nget_fasta_headers &lt;- function(file_path) {\n    # Read the FASTA file\n    fasta &lt;- readDNAStringSet(file_path)\n    \n    # Extract and return the headers\n    headers &lt;- names(fasta)\n    return(headers)\n}\n################################################################################\nfa_files &lt;- list.files(\"NODES/\", pattern = \".fa\", full.names = FALSE)\nseqmatch &lt;- data.frame()\ntotal_files &lt;- length(fa_files)\ndir.create(file.path(\"ALL_RESULTS\"))  \n################################################################################\nfor (j in seq_along(fa_files)) {\n  tmp_fasta_file &lt;- paste0(\"NODES/\", fa_files[j])\n  #tmp_fasta_file &lt;- paste0(\"NODES/\", j)\n  cat(sprintf(\"Processing file %d of %d: %s\\n\", j, total_files, tmp_fasta_file))\n  tmp_input_ids &lt;- data.frame(get_fasta_headers(tmp_fasta_file))\n  tmp_input_ids &lt;- tmp_input_ids %&gt;% dplyr::rename(\"MED_ID\" = 1)  \n  tmp_input_ids &lt;- tibble::as_tibble(tmp_input_ids)\n  tmp_1 &lt;- dplyr::left_join(tmp_input_ids, id_map, by = \"MED_ID\") \n  tmp_2 &lt;- dplyr::left_join(as.data.frame(tmp_1), \n                            as.data.frame(otu_list), \n                            by = c(\"MOTHUR_ID_RED\" = \"MOTHUR_ID\" ), \n                            keep = FALSE)\n  tmp_summ &lt;- data.frame(table(tmp_2$MOTHUR_OTU, useNA = \"always\"))\n  tmp_summ$file_name &lt;- str_replace(basename(tmp_fasta_file), \".fa\", \"\")\n  tmp_base_name &lt;- str_replace(basename(tmp_fasta_file), \".fa\", \"\")\n  write_delim(tmp_2, \n              paste(\"ALL_RESULTS/\", tmp_base_name, \".txt\", sep = \"\"), \n              delim = \"\\t\")\n  seqmatch &lt;- rbind(seqmatch, tmp_summ) \n  rm(list = ls(pattern = \"tmp_\"))\n}\n\nseqmatch &lt;- seqmatch %&gt;% dplyr::rename(\"MOTHUR_OTU\" = 1) %&gt;% \n                     dplyr::rename(., \"COUNT\" = 2) %&gt;% \n                     dplyr::rename(., \"MED_NODE\" = 3) \nseqmatch$MOTHUR_OTU &lt;- as.character(seqmatch$MOTHUR_OTU)\nseqmatch$MOTHUR_OTU[is.na(seqmatch$MOTHUR_OTU)] &lt;- \"NO_OTU\"\n\nwrite_delim(seqmatch, \"seqmatch.txt\", delim = \"\\t\")\nsaveRDS(seqmatch, \"seqmatch.rds\")\n#save.image(\"seqmatch_workflow.rdata\")\nseqmatch &lt;- readRDS(\"files/rdata/seqmatch.rds\")\nseqmatch$MED_NODE &lt;- paste(\"MED\", seqmatch$MED_NODE, sep =\"\")\n#seqmatch$UNIQUE_ID &lt;- paste0(seqmatch$MED_NODE, \"-\", seqmatch$MOTHUR_OTU)\nseqmatch\n# Load necessary packages\n\ncalculate_summary &lt;- function(data) {\n  max_val &lt;- max(data$COUNT)\n  no_otu_val &lt;- data$COUNT[data$MOTHUR_OTU == \"NO_OTU\"]\n  \n  sum_non_max &lt;- sum(data$COUNT[data$COUNT != max_val])\n  adjusted_sum_non_max &lt;- sum_non_max - ifelse(length(no_otu_val) &gt; 0, no_otu_val, 0)\n  \n  data.frame(\n    max_value = max_val,\n    combined_others = paste(data$MOTHUR_OTU[order(-data$COUNT)], collapse = \", \"),\n    no_otu_value = ifelse(length(no_otu_val) &gt; 0, no_otu_val, NA),\n    sum_non_max_values = sum_non_max,\n    adjusted_sum_non_max = adjusted_sum_non_max\n  )\n}\n\n# Apply the function to each group and combine results\nresult &lt;- seqmatch %&gt;%\n  group_by(MED_NODE) %&gt;%\n  do(calculate_summary(.)) %&gt;%\n  ungroup()\n\nprint(result)\nseqmatch\nCodeme_ds &lt;- c(\"me_asv_tmp\", \"me_otu_tmp\", \"me_med_tmp\")\nfor (j in me_ds) {\n  tmp_get &lt;- get(j)\n  tmp_tax &lt;- tmp_get$tax_table\n  tmp_samp &lt;- tmp_get$sample_table\n  tmp_st &lt;- tmp_get$otu_table\n  tmp_fasta &lt;- tmp_get$rep_fasta\n\n   for (i in 1:nrow(tmp_tax)){\n       if (tmp_tax[i,2] == \"p__Proteobacteria\" & tmp_tax[i,3] == \"c__Alphaproteobacteria\"){\n           phylum &lt;- base::paste(\"p__Alphaproteobacteria\")\n           tmp_tax[i, 2] &lt;- phylum\n   }   else if (tmp_tax[i,2] == \"p__Proteobacteria\" & tmp_tax[i,3] == \"c__Gammaproteobacteria\"){\n           phylum &lt;- base::paste(\"p__Gammaproteobacteria\")\n           tmp_tax[i, 2] &lt;- phylum\n   }   else if (tmp_tax[i,2] == \"p__Proteobacteria\" & tmp_tax[i,3] == \"c__Epsilonproteobacteria\"){\n           phylum &lt;- base::paste(\"p__Epsilonproteobacteria\")\n           tmp_tax[i, 2] &lt;- phylum\n   }   else if (tmp_tax[i,2] == \"p__Proteobacteria\" & tmp_tax[i,3] == \"c__Zetaproteobacteria\"){\n              phylum &lt;- base::paste(\"p__Zetaproteobacteria\")\n           tmp_tax[i, 2] &lt;- phylum\n   }   else if (tmp_tax[i,2] == \"p__Proteobacteria\" & tmp_tax[i,3] == \"c__\"){\n           phylum &lt;- base::paste(\"p__Proteobacteria\")\n           tmp_tax[i, 2] &lt;- phylum\n       }\n   }\n  tmp_name &lt;- str_replace(j, \"_tmp\", \"_pb\")\n  tmp_me &lt;- microtable$new(sample_table = tmp_samp, \n                         otu_table = tmp_st, \n                         tax_table = tmp_tax)\n                         \n  tmp_me$rep_fasta &lt;- tmp_fasta\n  tmp_me$tidy_dataset()\n  assign(tmp_name, tmp_me)\n  rm(list = ls(pattern = \"tmp_\"))\n}\nrm(class, order, phylum)\nrm(list = ls(pattern = \"_tmp\"))\nobjects()\nrm(i,j)\nCodeme_ds &lt;- c(\"me_asv\", \"me_otu\", \"me_med\", \n           \"me_asv_pb\", \"me_otu_pb\", \"me_med_pb\")\nfor (j in me_ds) {\n  tmp_get &lt;- get(j)\n  if (str_detect(j, \"_asv\") == TRUE) {\n    tmp_get$add_rownames2taxonomy(use_name = \"ASV\")\n  }\n  else if (str_detect(j, \"_otu\") == TRUE) {\n    tmp_get$add_rownames2taxonomy(use_name = \"OTU\")\n  }\nelse if (str_detect(j, \"_med\") == TRUE) {\n    tmp_get$add_rownames2taxonomy(use_name = \"MED\")\n}\n  rm(list = ls(pattern = \"tmp_\"))\n}\nfor (j in me_ds) {\n  tmp_get &lt;- get(j)\n  tmp_get$auto_tidy = TRUE\n  tmp_path &lt;- file.path(\"files/rdata/\")\n  saveRDS(tmp_get, paste(tmp_path, j, \".rds\", sep = \"\"))\n  rm(list = ls(pattern = \"tmp_\"))\n}\nme_asv_pb$tax_table\nme_ds &lt;- c(\"me_asv\", \"me_otu\", \"me_med\")\nfor (j in me_ds) {\n  tmp_get &lt;- get(j)\n  tmp_ds &lt;- microeco::clone(tmp_get)\n  tmp_ds_df &lt;- tmp_ds$otu_table\n\n  tmp_type &lt;- str_replace(rownames(tmp_ds_df)[1], \"\\\\d+\", \"\")\n  tmp_type &lt;- paste(tmp_type, \"_ID\", sep = \"\")\n\n  tmp_ds_df &lt;- tmp_ds_df %&gt;% \n    mutate(total_reads = rowSums(.), \n           .before = 1)\n  tmp_ds_df &lt;- tmp_ds_df  %&gt;% \n    mutate(total_reads_ES = rowSums(select(., contains(\"E_SAMP\"))), \n           .after = \"total_reads\")\n  tmp_ds_df &lt;- dplyr::select(tmp_ds_df, -contains(\"E_SAMP\"))\n  tmp_ds_df &lt;- tmp_ds_df %&gt;% \n    dplyr::mutate(total_reads_SS = rowSums(.[3:ncol(tmp_ds_df)]), \n                  .after = \"total_reads_ES\")\n\n  tmp_ds_df &lt;- tmp_ds_df %&gt;% tibble::rownames_to_column(tmp_type)\n\n  tmp_ds_df[, 5:ncol(tmp_ds_df)] &lt;- list(NULL)\n  tmp_ds_df &lt;- tmp_ds_df %&gt;% \n    dplyr::mutate(perc_reads_in_ES = 100*(\n    total_reads_ES / (total_reads_ES + total_reads_SS)),\n                .after = \"total_reads_SS\")\n  tmp_ds_df$perc_reads_in_ES &lt;- round(tmp_ds_df$perc_reads_in_ES, digits = 6)\n  tmp_name &lt;- purrr::map_chr(j, ~ paste0(., \"_df\"))\n  #tmp_name &lt;- str_replace(j, \"_tmp\", \"_df\")\n  assign(tmp_name, tmp_ds_df)\n  rm(list = ls(pattern = \"tmp_\"))\n}\nobjects(pattern = \"_df\")\nme_ds &lt;- c(\"me_asv\", \"me_otu\", \"me_med\")\nfor (j in me_ds) {\n  tmp_get &lt;- get(j)\n  tmp_1 &lt;- data.frame(rowSums(tmp_get$otu_table != 0))\n  tmp_type &lt;- str_replace(rownames(tmp_1)[1], \"\\\\d+\", \"\")\n  tmp_type &lt;- paste(tmp_type, \"_ID\", sep = \"\")\n  tmp_1 &lt;- tmp_1 %&gt;% tibble::rownames_to_column(tmp_type)\n  tmp_1 &lt;- tmp_1 %&gt;% dplyr::rename(\"total_samples\" = 2)  \n\n  tmp_2 &lt;- dplyr::select(tmp_get$otu_table, contains(\"E_SAMP\"))\n  tmp_2$num_samp_ES &lt;- rowSums(tmp_2 != 0)\n  tmp_2 &lt;- dplyr::select(tmp_2, contains(\"num_samp_ES\"))\n  tmp_2 &lt;- tmp_2 %&gt;% tibble::rownames_to_column(tmp_type)\n\n  tmp_3 &lt;- dplyr::select(tmp_get$otu_table, -contains(\"E_SAMP\"))\n  tmp_3$num_samp_SS &lt;- rowSums(tmp_3 != 0)\n  tmp_3 &lt;- dplyr::select(tmp_3, contains(\"num_samp_SS\"))\n  tmp_3 &lt;- tmp_3 %&gt;% tibble::rownames_to_column(tmp_type)\n  #tmp_merge &lt;- get(str_replace(j, \"_tmp\", \"_df\"))\n  tmp_merge &lt;- get(purrr::map_chr(j, ~ paste0(., \"_df\")))\n  tmp_ds_df &lt;- dplyr::left_join(tmp_merge, tmp_1) %&gt;%\n                 dplyr::left_join(., tmp_2) %&gt;%\n                 dplyr::left_join(., tmp_3)\n\n  tmp_ds_df &lt;- tmp_ds_df %&gt;%\n    dplyr::mutate(perc_samp_in_ES = 100*( num_samp_ES / (num_samp_ES + num_samp_SS)),\n                .after = \"num_samp_SS\")\n  tmp_name &lt;- purrr::map_chr(j, ~ paste0(., \"_df\"))\n  #tmp_name &lt;- str_replace(j, \"_tmp\", \"_df\")\n  assign(tmp_name, tmp_ds_df)\n  rm(list = ls(pattern = \"tmp_\"))\n}\nobjects()\nme_ds &lt;- c(\"me_asv_df\", \"me_otu_df\", \"me_med_df\")\nfor (j in me_ds) {\n  tmp_get &lt;- get(j)\n  tmp_ds_df &lt;- tmp_get %&gt;% filter(perc_reads_in_ES &gt; 1 | perc_samp_in_ES &gt; 1)\n  tmp_name_df &lt;- str_replace(j, \"_df\", \"_env\")\n  assign(tmp_name_df, tmp_ds_df)\n  \n  tmp_rem &lt;- tmp_ds_df[,1] %&gt;%  unlist(strsplit(., split = \", \")) \n  tmp_name &lt;- str_replace(j, \"_df\", \"_env_rem\")\n  assign(tmp_name, tmp_rem)\n  rm(list = ls(pattern = \"tmp_\"))\n}\nobjects()\n\nme_ds &lt;- c(\"me_asv\", \"me_otu\", \"me_med\")\nfor (j in me_ds) {\n  tmp_get &lt;- get(j)\n  tmp_es &lt;- microeco::clone(tmp_get)\n  tmp_rem &lt;- get(purrr::map_chr(j, ~ paste0(., \"_env_rem\")))\n  tmp_es$otu_table &lt;- tmp_es$otu_table %&gt;% \n        filter(row.names(tmp_es$otu_table)  tmp_rem)\n  tmp_es$tidy_dataset()\n  tmp_name &lt;- purrr::map_chr(j, ~ paste0(., \"_es\"))\n  assign(tmp_name, tmp_es)\n  rm(list = ls(pattern = \"tmp_\"))\n}\n\nobjects(pattern = \"_es\")\n\ntmp_asv &lt;- me_asv_es$merge_samples(\"TAXON\")\ntmp_asv$otu_table\ntmp_med &lt;- me_med_es$merge_samples(\"TAXON\")\ntmp_med$tax_table\ntmp_otu &lt;- me_otu_es$merge_samples(\"TAXON\")\ntmp_otu$otu_table\nme_asv_env\nme_ds &lt;- clone(me_med_pb)\n\nt1 &lt;- trans_alpha$new(dataset = me_ds, group = \"TISSUE\")\n# return t1$data_stat\nhead(t1$data_stat)\n\nt1$cal_diff(method = \"KW\")\n# return t1$res_diff\nhead(t1$res_diff)\n\nt1$cal_diff(method = \"KW_dunn\")\n# return t1$res_diff\nhead(t1$res_diff)\n\n# more options\nt1$cal_diff(method = \"KW_dunn\", KW_dunn_letter = FALSE)\nhead(t1$res_diff)\nt1$cal_diff(method = \"wilcox\")\nhead(t1$res_diff)\nt1$cal_diff(method = \"t.test\")\n\n\nt1$cal_diff(method = \"anova\")\n# return t1$res_diff\nhead(t1$res_diff)\n\nt1 &lt;- trans_alpha$new(dataset = me_ds, group = \"TISSUE\")\nt1$cal_diff(method = \"anova\", formula = \"TISSUE+OCEAN\")\nhead(t1$res_diff)\n# see the help document for the usage of formula\n\nt1$cal_diff(method = \"anova\")\n# y_increase can adjust the distance from the letters to the highest point\nt1$plot_alpha(measure = \"Chao1\", y_increase = 0.3)\nt1$plot_alpha(measure = \"Chao1\", y_increase = 0.1)\n# add_sig_text_size: letter size adjustment\nt1$plot_alpha(measure = \"Chao1\", add_sig_text_size = 6, add = \"jitter\", order_x_mean = TRUE)\n\nt1$cal_diff(method = \"wilcox\")\nt1$plot_alpha(measure = \"Chao1\", shape = \"TISSUE\")\n# y_start: starting height for the first label\n# y_increase: increased height for each label\nt1$plot_alpha(measure = \"Chao1\", shape = \"TISSUE\", add = \"jitter\", y_start = 0.1, y_increase = 0.1)\n\nt1$res_diff %&lt;&gt;% base::subset(Significance != \"ns\")\nt1$plot_alpha(measure = \"Chao1\", add = \"dotplot\", xtext_size = 15)\n\nt1 &lt;- trans_alpha$new(dataset = me_ds, group = \"OCEAN\", by_group = \"TISSUE\")\nt1$cal_diff(method = \"wilcox\")\nt1$plot_alpha(measure = \"Shannon\")\nrm(t1)\nme_ds$cal_betadiv(unifrac = FALSE)\nunique(me_ds$sample_table$PAIR)\n\n\nt1 &lt;- trans_beta$new(dataset = me_ds, group = \"TISSUE\", measure = \"bray\")\n\nt1$cal_ordination(method = \"PCoA\")\n# t1$res_ordination is the ordination result list\nclass(t1$res_ordination)\n# plot the PCoA result with confidence ellipse\nt1$plot_ordination(plot_color = \"TISSUE\", plot_shape = \"OCEAN\", plot_type = c(\"point\", \"ellipse\"))\nMERGE SAMPLES ANALYSIS\nme_ds &lt;- clone(me_med_pb)\nt1 &lt;- trans_abund$new(dataset = me_ds, taxrank = \"Phylum\", ntaxa = 10, groupmean = \"TISSUE\")\ng1 &lt;- t1$plot_bar(others_color = \"grey70\", legend_text_italic = FALSE)\ng1 + theme_classic() + theme(axis.title.y = element_text(size = 18))\n\n# show 40 taxa at Genus level\nt1 &lt;- trans_abund$new(dataset = me_ds, taxrank = \"Phylum\", ntaxa = 40)\ng1 &lt;- t1$plot_heatmap(facet = \"TISSUE\", xtext_keep = FALSE, withmargin = FALSE, plot_breaks = c(0.01, 0.1, 1, 10))\ng1\ng1 + theme(axis.text.y = element_text(face = 'italic'))\n\nt1 &lt;- trans_abund$new(dataset = me_ds, taxrank = \"Phylum\", ntaxa = 6, groupmean = \"TISSUE\")\n# all pie chart in one row\nt1$plot_pie(facet_nrow = 3)\nt1$plot_pie(facet_nrow = 3, add_label = TRUE)\n\nt1 &lt;- trans_abund$new(dataset = me_ds, taxrank = \"Phylum\", ntaxa = 8, groupmean = \"TISSUE\")\nt1$plot_donut(label = FALSE, facet_nrow = 3)\nt1$plot_donut(label = TRUE, facet_nrow = 3)\n\nt1 &lt;- trans_abund$new(dataset = me_ds, taxrank = \"Phylum\", ntaxa = 10, groupmean = \"SPECIES\")\ng1 &lt;- t1$plot_bar(coord_flip = TRUE)\ng1 &lt;- g1 + theme_classic() + theme(axis.title.x = element_text(size = 16), axis.ticks.y = element_blank(), axis.line.y = element_blank())\ng1\ng1 &lt;- t1$plot_bar(clustering_plot = TRUE)\n# In this case, g1 (aplot object) is the combination of different ggplot objects\n# to adjust the main plot, please select g1[[1]]\ng1[[1]] &lt;- g1[[1]] + theme_classic() + theme(axis.title.x = element_text(size = 16), axis.ticks.y = element_blank(), axis.line.y = element_blank())\ng1\n\n\ntmp &lt;- me_ds$merge_samples(\"TISSUE\")\nt1 &lt;- trans_venn$new(dataset = tmp)\n# only show some sets with large intersection numbers\nt1$data_summary %&lt;&gt;% .[.[, 1] &gt; 20, ]\ng1 &lt;- t1$plot_bar(left_plot = TRUE, bottom_height = 0.5, left_width = 0.15, up_bar_fill = \"grey50\", left_bar_fill = \"grey50\", bottom_point_color = \"black\")\ng1\n# g1 is aplot class and can be saved with ggplot2::ggsave, aplot::ggsave or cowplot::save_plot function\n# as g1 is comprised of several sub-plots, please adjust the details for each sub-plot\ng1[[1]]\ng1[[2]]\n\ndataset1 &lt;- me_ds$merge_samples(\"TISSUE\")\nt1 &lt;- trans_venn$new(dataset1)\n# transform venn results to the sample-species table, here do not consider abundance, only use presence/absence.\nt2 &lt;- t1$trans_comm(use_frequency = TRUE)\n# t2 is a new microtable class, each part is considered a sample\nclass(t2)\n# calculate taxa abundance, that is, the frequency\nt2$cal_abund()\nt2$taxa_abund\n# transform and plot\nt3 &lt;- trans_abund$new(dataset = t2, taxrank = \"Phylum\", ntaxa = 8)\nunique(t3$data_abund$Sample)\nt3$plot_bar(bar_full = FALSE, legend_text_italic = T, xtext_angle = 30, color_values = RColorBrewer::brewer.pal(8, \"Set2\"),\n    order_x = c(\"IW\", \"CW\", \"TW\", \"IW&CW\", \"IW&TW\", \"CW&TW\", \"IW&CW&TW\")) + ylab(\"Frequency (%)\")\nunique(t3$data_abund$Sample)\nme_ds &lt;- c(\"me_med\", \"me_asv\", \"me_otu\")\nfor (j in me_ds) {\n  tmp_get &lt;- get(j)\n  tmp_get_tab &lt;- data.frame(tmp_get$otu_table)\n  tmp_get_tab &lt;- tmp_get_tab %&gt;% tibble::rownames_to_column(\"ID\")\n  tmp_get_tab &lt;- jamba::mixedSortDF(tmp_get_tab, decreasing = FALSE, \n                                        useRownames = FALSE, byCols = 1)\n  tmp_get_tab &lt;- tmp_get_tab %&gt;% tibble::remove_rownames() \n  tmp_get_tab &lt;- tmp_get_tab %&gt;% tibble::column_to_rownames(\"ID\")\n  tmp_get_tab &lt;- data.frame(t(tmp_get_tab))\n\n  tmp_get_tab_ord &lt;- data.frame(tmp_get$otu_table)\n  tmp_get_tab_ord &lt;- tmp_get_tab_ord %&gt;% tibble::rownames_to_column(\"ID\")\n  tmp_get_tab_ord &lt;- jamba::mixedSortDF(tmp_get_tab_ord, decreasing = TRUE, \n                                        useRownames = FALSE, byCols = 1)\n  tmp_get_tab_ord &lt;- tmp_get_tab_ord %&gt;% tibble::remove_rownames() \n  tmp_get_tab_ord &lt;- tmp_get_tab_ord %&gt;% tibble::column_to_rownames(\"ID\")\n  tmp_get_tab_ord &lt;- data.frame(t(tmp_get_tab_ord))\n\n  tmp_tab_name &lt;- purrr::map_chr(j, ~ paste0(., \"_perfect\"))\n  assign(tmp_tab_name, tmp_get_tab)\n  \n  tmp_tab_ord_name &lt;- purrr::map_chr(j, ~ paste0(., \"_ord_perfect\"))\n  assign(tmp_tab_ord_name, tmp_get_tab_ord)\n  rm(list = ls(pattern = \"tmp_\"))\n  \n}\nobjects(pattern = \"_perfect\")\ndim(me_med_perfect)\ndim(me_med_ord_perfect)\ndim(me_asv_perfect)\ndim(me_asv_ord_perfect)\ndim(me_otu_perfect)\ndim(me_otu_ord_perfect)\n#me_ds &lt;- c(\"me_med\", \"me_asv\", \"me_otu\")\nme_ds &lt;- c(\"me_med\")\n\nlibrary(PERFect)\nper_pval &lt;- 0.05\nfor (j in me_ds) {\n  tmp_path &lt;- file.path(\"files/rdata/\")\n  tmp_get &lt;- get(purrr::map_chr(j, ~ paste0(., \"_perfect\")))\n  tmp_pval &lt;- per_pval\n  tmp_get_ord &lt;- get(purrr::map_chr(j, ~ paste0(., \"_ord_perfect\")))\n\n  print(\"Running PERFect_sim on default\")\n  tmp_sim &lt;- PERFect_sim(X = tmp_get, alpha = tmp_pval, Order = \"NP\", center = FALSE)\n  print(\"Finished running PERFect_sim on default\")\n  dim(tmp_sim$filtX)\n  tmp_sim_name &lt;- purrr::map_chr(j, ~ paste0(., \"_perfect_sim\"))\n  assign(tmp_sim_name, tmp_sim)\n  saveRDS(tmp_sim, paste(tmp_path, tmp_sim_name, \".rds\", sep = \"\"))\n  \n  print(\"Running PERFect_sim on Ordered\")\n  tmp_sim_ord &lt;- PERFect_sim(X = tmp_get_ord, alpha = tmp_pval, Order = \"NP\", center = FALSE)\n  dim(tmp_sim_ord$filtX)\n  tmp_sim_ord_name &lt;- purrr::map_chr(j, ~ paste0(., \"_ord_perfect_sim\"))\n  print(\"Finished running PERFect_sim on Ordered\")\n  assign(tmp_sim_ord_name, tmp_sim_ord)\n  saveRDS(tmp_sim_ord, paste(tmp_path, tmp_sim_ord_name, \".rds\", sep = \"\"))\n\n  rm(list = ls(pattern = \"tmp_\"))\n  \n}\nobjects(pattern = \"_sim\")\ndim(me_med_ord_perfect_sim$filtX)\ndim(me_med_perfect_sim$filtX)[2]\ndata.frame(me_med_perfect_sim$pvals)\ndata.frame(me_med_ord_perfect_sim$pvals)\ncat(\"Total 16S rRNA ASVs with p-value less than\", ssu_per_pval[1], \"\\n\")\ntmp_df &lt;- ssu_default_pvals\ntmp_df &lt;- data.frame(tmp_df)\npval_asv &lt;- tmp_df %&gt;% dplyr::summarise(count = sum(tmp_df &lt;= ssu_per_pval))\n\nprint(paste(\"default order: ASVs before checking p value was\", \n            ssu_default_num_asvs, \n            \"and after was\", \n            pval_asv$count[1]), \n      quote = FALSE)\n\nprint(\"--------------------------------------\", quote = FALSE)\n\ntmp_df &lt;- ssu_ord_pvals\ntmp_df &lt;- data.frame(tmp_df)\npval_asv_ord &lt;- tmp_df %&gt;% dplyr::summarise(count = sum(tmp_df &lt;= ssu_per_pval))\n\nprint(paste(\"decreasing order: ASVs before checking p value was\", \n            ssu_ord_num_asvs, \"and after was\", \n            pval_asv_ord$count[1]), \n      quote = FALSE)\ntmp_a %&gt;% \n   filter(if_any(everything(), ~ str_detect(.x, 'OTU_132')))\n#prep for https://env-med.shinyapps.io/microbiem/\ntmp_tax &lt;- me_asv$tax_table\n\ntmp_tax &lt;- tmp_tax %&gt;% mutate_all(funs(str_replace_all(., \"[a-z]__\", \"\")))\ntmp_tax &lt;- tmp_tax %&gt;% unite(\"Taxonomy\", 1:6, remove = TRUE, sep = \";\")\ntmp_tax &lt;- tibble::rownames_to_column(tmp_tax, \"OTU_ID\")\ntmp_tax &lt;- data.frame(sapply(tmp_tax, \n                             gsub, \n                             pattern = \";+$\", \n                             replacement = \"\"))\ntmp_tax\n\ntmp_asv &lt;- me_asv$otu_table\ntmp_asv &lt;- tibble::rownames_to_column(tmp_asv, \"OTU_ID\")\n\nasv_miem_feature &lt;- dplyr::left_join(tmp_asv, tmp_tax, by = \"OTU_ID\")\nwrite_delim(asv_miem_feature, \"asv_miem_feature.txt\", delim = \"\\t\")\n\ntmp_samp &lt;- me_asv$sample_table\ntmp_samp &lt;- tibble::rownames_to_column(tmp_samp, \"Sample_ID\")\ntmp_samp$SampleID &lt;- NULL\ntmp_samp$Sample_type &lt;- \"SAMPLE\"\nasv_miem_sample &lt;- tmp_samp %&gt;% dplyr::relocate(Sample_type, .after = \"Sample_ID\")\nwrite_delim(asv_miem_sample, \"asv_miem_sample.txt\", delim = \"\\t\")\n#prep for https://env-med.shinyapps.io/microbiem/\ntmp_tax &lt;- me_asv$tax_table\n\ntmp_tax &lt;- tmp_tax %&gt;% mutate_all(funs(str_replace_all(., \"[a-z]__\", \"\")))\ntmp_tax &lt;- tmp_tax %&gt;% unite(\"Taxonomy\", 1:6, remove = TRUE, sep = \";\")\ntmp_tax &lt;- tibble::rownames_to_column(tmp_tax, \"OTU_ID\")\ntmp_tax &lt;- data.frame(sapply(tmp_tax, \n                             gsub, \n                             pattern = \";+$\", \n                             replacement = \"\"))\ntmp_tax\n\ntmp_asv &lt;- me_asv$otu_table\ntmp_asv &lt;- tibble::rownames_to_column(tmp_asv, \"OTU_ID\")\n\nasv_miem_feature &lt;- dplyr::left_join(tmp_asv, tmp_tax, by = \"OTU_ID\")\nwrite_delim(asv_miem_feature, \"asv_miem_feature.txt\", delim = \"\\t\")\n\ntmp_samp &lt;- me_asv$sample_table\ntmp_samp &lt;- tibble::rownames_to_column(tmp_samp, \"Sample_ID\")\ntmp_samp$SampleID &lt;- NULL\ntmp_samp$Sample_type &lt;- \"SAMPLE\"\nasv_miem_sample &lt;- tmp_samp %&gt;% dplyr::relocate(Sample_type, .after = \"Sample_ID\")\nwrite_delim(asv_miem_sample, \"asv_miem_sample.txt\", delim = \"\\t\")\n#prep for https://env-med.shinyapps.io/microbiem/\ntmp_tax &lt;- me_med$tax_table\n\ntmp_tax &lt;- tmp_tax %&gt;% mutate_all(funs(str_replace_all(., \"[a-z]__\", \"\")))\ntmp_tax &lt;- tmp_tax %&gt;% unite(\"Taxonomy\", 1:6, remove = TRUE, sep = \";\")\ntmp_tax &lt;- tibble::rownames_to_column(tmp_tax, \"OTU_ID\")\ntmp_tax &lt;- data.frame(sapply(tmp_tax, \n                             gsub, \n                             pattern = \";+$\", \n                             replacement = \"\"))\ntmp_tax\n\ntmp_asv &lt;- me_med$otu_table\ntmp_asv &lt;- tibble::rownames_to_column(tmp_asv, \"OTU_ID\")\n\nmed_miem_feature &lt;- dplyr::left_join(tmp_asv, tmp_tax, by = \"OTU_ID\")\nwrite_delim(med_miem_feature, \"med_miem_feature.txt\", delim = \"\\t\")\n\ntmp_samp &lt;- me_med$sample_table\ntmp_samp &lt;- tibble::rownames_to_column(tmp_samp, \"Sample_ID\")\ntmp_samp$SampleID &lt;- NULL\ntmp_samp$Sample_type &lt;- tmp_samp$Sample_ID\nmed_miem_sample &lt;- tmp_samp %&gt;% dplyr::relocate(Sample_type, .after = \"Sample_ID\")\nmed_miem_sample &lt;- med_miem_sample %&gt;%\n  mutate(across(\"Sample_type\", str_replace, \"^.*E_SAMP.*\", \"POS1\")) %&gt;%\n  mutate(across(\"Sample_type\", str_replace, \"^.*A_.*\", \"SAMPLE\")) %&gt;%\n  mutate(across(\"Sample_type\", str_replace, \"^.*UNKN.*\", \"SAMPLE\"))\nwrite_delim(med_miem_sample, \"med_miem_sample.txt\", delim = \"\\t\")\ncitation(\"microeco\")\ncitation(\"mia\")"
  },
  {
    "objectID": "docs/ssu-workflows/compare/index.html#rename-na-taxonomic-ranks",
    "href": "docs/ssu-workflows/compare/index.html#rename-na-taxonomic-ranks",
    "title": "3. OTU Workflow",
    "section": "Rename NA taxonomic ranks",
    "text": "Rename NA taxonomic ranks\nPhyloseq has an odd way of dealing with taxonomic ranks that have no value—in other words, NA in the tax table. The first thing we are going to do before moving forward is to change all of the NAs to have a value of the next highest classified rank. For example, ASV26 is not classified at the Genus level but is at Family level (Xanthobacteraceae). So we change the Genus name to Family_Xanthobacteraceae. The code for comes from these two posts on the phyloseq GitHub, both by MSMortensen: issue #850 and issue #990.\n\nOne thing this code does is reassign the functions class and order to taxonomic ranks. This can cause issues if you need these functions.\n\nSo you need to run something like this rm(class, order, phylum, kingdom) at the end of the code to remove these as variables. For now, I have not come up with a better solution.\n\nCodeps_moth &lt;- ps_moth_nc\ntax.clean &lt;- data.frame(tax_table(ps_moth))\nfor (i in 1:6){ tax.clean[,i] &lt;- as.character(tax.clean[,i])}\ntax.clean[is.na(tax.clean)] &lt;- \"\"\n\nfor (i in 1:nrow(tax.clean)){\n    if (tax.clean[i,2] == \"\"){\n        kingdom &lt;- base::paste(\"k_\", tax.clean[i,1], sep = \"\")\n        tax.clean[i, 2:6] &lt;- kingdom\n    } else if (tax.clean[i,3] == \"\"){\n        phylum &lt;- base::paste(\"p_\", tax.clean[i,2], sep = \"\")\n        tax.clean[i, 3:6] &lt;- phylum\n    } else if (tax.clean[i,4] == \"\"){\n        class &lt;- base::paste(\"c_\", tax.clean[i,3], sep = \"\")\n        tax.clean[i, 4:6] &lt;- class\n    } else if (tax.clean[i,5] == \"\"){\n        order &lt;- base::paste(\"o_\", tax.clean[i,4], sep = \"\")\n        tax.clean[i, 5:6] &lt;- order\n    } else if (tax.clean[i,6] == \"\"){\n        tax.clean$Genus[i] &lt;- base::paste(\"f\",tax.clean$Family[i], sep = \"_\")\n        }\n}\ntax_table(ps_moth) &lt;- as.matrix(tax.clean)\nrank_names(ps_moth)\nrm(class, order, phylum, kingdom, i)\n\n\nStill the same ranks. That’s good. What about the new groups? Let’s take a peak at some families.\n\nhead(get_taxa_unique(ps_moth, \"Family\"), 16)"
  },
  {
    "objectID": "docs/ssu-workflows/sampledata/index.html",
    "href": "docs/ssu-workflows/sampledata/index.html",
    "title": "1. Sample Metadata",
    "section": "",
    "text": "Click here for page build libraries and setup information.knitr::opts_chunk$set(echo = TRUE, eval = FALSE)\nset.seed(919191)\nlibrary(dada2); packageVersion(\"dada2\")\nlibrary(ShortRead); packageVersion(\"ShortRead\")\nlibrary(ggplot2); packageVersion(\"ggplot2\")\nlibrary(Biostrings); packageVersion(\"Biostrings\")\npacman::p_load(tidyverse, gridExtra, grid, miaViz,\n               formatR, reactable, gdata, patchwork,\n               kableExtra, microeco, magrittr, \n               rprojroot,\n               tidySummarizedExperiment, scater,\n               install = FALSE, update = FALSE)\n\noptions(scipen = 999)\nknitr::opts_current$get(c(\n  \"cache\",\n  \"cache.path\",\n  \"cache.rebuild\",\n  \"dependson\",\n  \"autodep\"\n))\n#root &lt;- find_root(has_file(\"_quarto.yml\"))\n#source(file.path(root, \"assets\", \"functions.R\"))",
    "crumbs": [
      "16S rRNA Processing",
      "1. Sample Metadata"
    ]
  },
  {
    "objectID": "docs/ssu-workflows/sampledata/index.html#overview",
    "href": "docs/ssu-workflows/sampledata/index.html#overview",
    "title": "1. Sample Metadata",
    "section": "Overview",
    "text": "Overview",
    "crumbs": [
      "16S rRNA Processing",
      "1. Sample Metadata"
    ]
  },
  {
    "objectID": "docs/ssu-workflows/sampledata/index.html#wrangle-metadata",
    "href": "docs/ssu-workflows/sampledata/index.html#wrangle-metadata",
    "title": "1. Sample Metadata",
    "section": "Wrangle Metadata",
    "text": "Wrangle Metadata\nThe first step in to load all metadata files (one for each sequencing run) and combine the files.\n\nfile_paths &lt;- list.files(path = \"files/tables/metadata\", \n                         pattern = \"\\\\.csv$\", full.names = TRUE)\ntmp_meta &lt;- vroom::vroom(file_paths)\ncolnames(tmp_meta)[1] &lt;- \"Original_name\"\n\nNext we remove any duplicate entires. We have duplicates because some samples were sequecned more than once due to low read count from the initial run.\n\n\n\n\n\nCode chunk coloring by language\n\nColor\nLanguage\n\n\n\nblue\nR\n\n\nvermillion\nbash\n\n\nbluish green\nshell\n\n\nreddish purple\nmothur\n\n\n\n\n\nWe make use of many different coding languages in these workflows. Code chucks are colored by language.\n\ndups_in_md &lt;- tmp_meta[duplicated(tmp_meta[,1:1]),]\ntmp_meta &lt;- tmp_meta[!duplicated(tmp_meta[,1:1]),]\n\nIn the code below we remove several samples due to low initial read count.\n\ntmp_meta &lt;- tmp_meta[!grepl(\"7512\", tmp_meta$Original_name),]\ntmp_meta &lt;- tmp_meta[!grepl(\"8978-S\", tmp_meta$Original_name),]\ntmp_meta &lt;- tmp_meta[!grepl(\"8978-H\", tmp_meta$Original_name),]\ntmp_meta &lt;- tmp_meta[!grepl(\"7640-S\", tmp_meta$Original_name),]\n\nAnd finally do a little maintenance on column names.\n\ncolnames(tmp_meta) &lt;- gsub(\"-| |\\\\.\", \"_\", colnames(tmp_meta))\ncolnames(tmp_meta)[12] &lt;- \"Species_pair\"\ncolnames(tmp_meta) &lt;- gsub(\"_Original\", \"\", colnames(tmp_meta))\n\nOnce this is finished we can go ahead and select columns in the metadata that are most important for downstream analysis.\n\ntmp_meta &lt;- tmp_meta %&gt;% \n  dplyr::relocate(c(\"Ocean\", \"Morphospecies\", \"Tissue\", \n                    \"Habitat\", \"Site\", \"Site_2\", \"Taxon\", \n                    \"Length\", \"Station_no\", \"Species_pair\", \n                    \"Species_group\", \"Species_complex\", \n                    \"Plate\", \"Row\", \"Column\"), \n                  .after = \"Original_name\")\ntmp_meta[17:ncol(tmp_meta)] &lt;- NULL\n\nAt this point what we now need to do is standardize some of the values in the metadata catagories. This includes things like replacing spaces with underscores (_). The goal here is to eliminate anything that may cause issues later in the workflows.\n\ntmp_meta &lt;- tmp_meta %&gt;%\n  mutate(Ocean = str_replace_all(Ocean, \"Pacific\", \"Eastern_Pacific\")) %&gt;%\n  mutate(Ocean = str_replace_all(Ocean, \"Western Atlantic\", \"Western_Atlantic\")) %&gt;%\n  mutate(Taxon = str_replace_all(Taxon, \"Snapping shrimp\", \"Snapping_shrimp\")) %&gt;%\n  mutate(Morphospecies = str_replace_all(Morphospecies, \" +\", \"_\")) %&gt;%\n  mutate(Species_pair = str_replace_all(Species_pair, \"\\\\. +\", \"_\")) %&gt;%\n  mutate(Species_pair = str_replace_all(Species_pair, \" / \", \"-\")) %&gt;%\n  mutate(Species_pair = str_replace_all(Species_pair, \" +\", \"_\")) %&gt;%\n  mutate(Species_group = str_replace_all(Species_group, \"\\\\. \", \"_\")) %&gt;%\n  mutate(Species_group = str_replace_all(Species_group, \" \", \"_\")) %&gt;%\n  mutate(Species_complex = str_replace_all(Species_complex, \"\\\\. \", \"_\"))  %&gt;%\n  mutate(Plate = str_replace_all(Plate, \" \", \"_\"))\ntmp_meta &lt;- tmp_meta %&gt;%\n  mutate(Taxon = str_replace_all(Taxon, c(\n    \"Rubble\" = \"Environmental\", \n    \"Sediment\" = \"Environmental\", \n    \"Mud\" = \"Environmental\",\n    \"Water\" = \"Environmental\"\n    )\n  )\n)\ntmp_meta$Morphospecies[is.na(tmp_meta$Morphospecies)] &lt;- \"Environmental\"",
    "crumbs": [
      "16S rRNA Processing",
      "1. Sample Metadata"
    ]
  },
  {
    "objectID": "docs/ssu-workflows/sampledata/index.html#sample-shortcodes",
    "href": "docs/ssu-workflows/sampledata/index.html#sample-shortcodes",
    "title": "1. Sample Metadata",
    "section": "Sample Shortcodes",
    "text": "Sample Shortcodes\nIn order to create more meaningful sample names, we want to generate some shortcodes, specifically for the sampling ocean, the species name, and the tissue type, which we will combine with the unique individual ID number to get the new name. For example, something like EP_A_HEBE_GL_8937 is the gill tissue (GL) from an Alpheus hebes (A-HEBE), individual ID 8937, collected in the Eastern Pacific (EP). We use a similar convention for environmental samples, but instead of species name, we use E_SAMP to delineate environmental samples. For example, WA_E_SAMP_WT_3075 is a water sample (WT) collected from the Western Atlantic (WA), unique id 3075.\n\nClick to see the the shortcodes for each variable.# Replacement list (old = new)\ntmp_ocean &lt;- c(\n  \"Control\" = \"CON\",\n  \"Eastern_Pacific\" = \"EP\",\n  \"Western_Atlantic\" = \"WA\"\n)\n\ntmp_tissue &lt;- c(\n  \"Control\" = \"CON\", \n  \"Egg\" = \"EG\", \n  \"Gill\" = \"GL\", \n  \"Hepatopancreas\" = \"HP\", \n  \"Midgut\" = \"MG\", \n  \"Stomach\" = \"ST\", \n  \"Mud\" = \"MD\", \n  \"Rubble\" = \"RB\", \n  \"Sediment\" = \"SD\", \n  \"Water\" = \"WT\"\n)\n\ntmp_species &lt;- c(\n  \"Control\" = \"CON\", \n  \"Alpheus_arenensis\" = \"A_AREN\", \n  \"Alpheus_bahamensis\" = \"A_BAHA\", \n  \"Alpheus_bouvieri\" = \"A_BOUV\", \n  \"Alpheus_cristulifrons\" = \"A_CRIS\", \n  \"Alpheus_fasciatus\" = \"A_FASC\", \n  \"Alpheus_floridanus\" = \"A_FLOR\", \n  \"Alpheus_formosus\" = \"A_FORM\", \n  \"Alpheus_galapagensis\" = \"A_GALA\", \n  \"Alpheus_hebes\" = \"A_HEBE\", \n  \"Alpheus_hephaestus\" = \"A_HEPH\", \n  \"Alpheus_hyeyoungae\" = \"A_HYEY\", \n  \"Alpheus_javieri\" = \"A_JAVI\", \n  \"Alpheus_millsae\" = \"A_MILL\", \n  \"Alpheus_nuttingi\" = \"A_NUTT\", \n  \"Alpheus_panamensis\" = \"A_PANA\", \n  \"Alpheus_paracrinitus_no_spot\" = \"A_PCNS\", \n  \"Alpheus_paracrinitus_with_spot\" = \"A_PCWS\", \n  \"Alpheus_paraformosus\" = \"A_PARA\", \n  \"Alpheus_platycheirus\" = \"A_PLAT\", \n  \"Alpheus_rostratus\" = \"A_ROST\", \n  \"Alpheus_saxidomus\" = \"A_SAXI\", \n  \"Alpheus_simus\" = \"A_SIMU\", \n  \"Alpheus_thomasi\" = \"A_THOM\", \n  \"Alpheus_umbo\" = \"A_UMBO\", \n  \"Alpheus_utriensis\" = \"A_UTRI\", \n  \"Alpheus_verrilli\" = \"A_VERR\", \n  \"Alpheus_websteri\" = \"A_WEBS\", \n  \"Environmental\" = \"E_SAMP\", \n  \"Unknown\" = \"UNKN\"\n)\n\n\nWith these shortcodes in hand we can match each original sample name (from the fastq file names) to the shortocodes.\n\ntmp_shortcode &lt;- tmp_meta\ntmp_shortcode &lt;- tmp_shortcode %&gt;% select(1:4, 8)\n\ntmp_shortcode$Ocean_code &lt;- tmp_shortcode$Ocean\ntmp_shortcode &lt;- tmp_shortcode %&gt;%\n  mutate(Ocean_code = str_replace_all(Ocean_code, tmp_ocean)) \n\ntmp_shortcode$Species_code &lt;- tmp_shortcode$Morphospecies\ntmp_shortcode &lt;- tmp_shortcode %&gt;%\n  mutate(Species_code = str_replace_all(Species_code, tmp_species)) \n\ntmp_shortcode$Tissue_code &lt;- tmp_shortcode$Tissue\ntmp_shortcode &lt;- tmp_shortcode %&gt;%\n  mutate(Tissue_code = str_replace_all(Tissue_code, tmp_tissue)) \ntmp_shortcode[2:4] &lt;- NULL\n\ntmp_shortcode &lt;- tmp_shortcode %&gt;% \n  dplyr::relocate(\"Taxon\", .after = \"Tissue_code\")\nhead(tmp_shortcode)\n\n\n\n# A tibble: 6 × 5\n  Original_name Ocean_code Species_code Tissue_code Taxon          \n  &lt;chr&gt;         &lt;chr&gt;      &lt;chr&gt;        &lt;chr&gt;       &lt;chr&gt;          \n1 7322-M        EP         A_PANA       MG          Snapping_shrimp\n2 7322-H        EP         A_PANA       HP          Snapping_shrimp\n3 7322-S        EP         A_PANA       ST          Snapping_shrimp\n4 7322-G        EP         A_PANA       GL          Snapping_shrimp\n5 7326-M        EP         A_PANA       MG          Snapping_shrimp\n6 7326-G        EP         A_PANA       GL          Snapping_shrimp\n\n\nOk, still with me? Now, even though we have the pieces in place to standardize the sample names, we cannot do that yet because different sample types have different initial naming conventions. For examples, Control samples are named like Control_1, Control_2, etc., while the snapping shrimp are named like 7322-M, 7322-H, etc. Environmental samples have totally different names, for example Machete_scrap2, ML2670, etc. This makes it a litle difficult to parse out meaningful information (specifically ID numbers) from the original sample names and apply our short codes to generate the new names.\nWhen we generated the short codes above we also included a column called Taxon. These data tell us whether a the entry is a snapping shrimp, a control, or an environmental sample.\n\nunique(tmp_shortcode$Taxon)\n\n[1] \"Snapping_shrimp\" \"Control\"  \"Environmental\"  \nWe can use the base R command split to separate the dataset based on the Taxon type so that we can process each category separately.\n\ntmp_split_dfs &lt;- split(tmp_shortcode, tmp_shortcode$Taxon)\n\nGenerating three separate data frames.\ntmp_split_dfs$Control\n\ntmp_split_dfs$Environmental\n\ntmp_split_dfs$Snapping_shrimp\nControl samples\n\ntmp_control &lt;- tmp_split_dfs$Control\ntmp_control$tmp &lt;- tmp_control$Original_name\n\ntmp_control &lt;- tmp_control %&gt;%\n  separate_wider_delim(tmp, delim = \"-\", names = c(\"tmp1\", \"ID\"))\ntmp_control$tmp1 &lt;- NULL\n\nEnvironmental samples\n\ntmp_envr &lt;- tmp_split_dfs$Environmental\n\ntmp_envr$tmp &lt;- tmp_envr$Original_name\n\ntmp_envr &lt;- tmp_envr %&gt;%\n  separate_wider_delim(tmp,\n                       delim = stringr::regex(\"(_)|(ML)\"),\n                       too_few = \"align_start\",\n                       names_sep = \"\",\n                       names_repair = ~ sub(\"value\", \"X\", .x))\n\ntmp_envr$tmp2 &lt;- str_replace(tmp_envr$tmp2, \"(\\\\d+).*\", \"\\\\1\")\ntmp_envr$tmp2 &lt;- str_replace(tmp_envr$tmp2, \"Sed\", \"sed\")\ntmp_envr$tmp1 &lt;- NULL\ntmp_envr &lt;- tmp_envr %&gt;% rename(ID = tmp2)\n\nShrimp samples\n\ntmp_shrmp &lt;- tmp_split_dfs$Snapping_shrimp\n\ntmp_shrmp$tmp &lt;- tmp_shrmp$Original_name\ntmp_shrmp %&gt;%\n  filter(Original_name == \"7332-H-M\")\ntmp_shrmp$tmp &lt;- gsub(\"7332-H-M\", \"7332M-H\", tmp_shrmp$tmp)\n\ntmp_shrmp &lt;- tmp_shrmp %&gt;%\n  separate_wider_delim(tmp,\n                       delim = \"-\",\n                       names = c(\"tmp1\", \"tmp2\")\n                       )\ntmp_shrmp %&gt;%\n  filter(Original_name == \"7332-H-M\")\ntmp_shrmp$tmp2 &lt;- NULL\ntmp_shrmp &lt;- tmp_shrmp %&gt;% rename(ID = tmp1)\n\nSweet. At this point we can generate the new unique name for each sample based on the criteria listed above.\n\ntmp_control$SampleID &lt;- tmp_control$Original_name\ntmp_control$SampleID &lt;- gsub(\"-\", \"_\", tmp_control$SampleID)\n\ntmp_envr &lt;- tmp_envr %&gt;%\n  mutate(SampleID = paste(Ocean_code, \n                          Species_code, \n                          Tissue_code, \n                          ID, \n                          sep = \"_\")\n         )\ntmp_shrmp &lt;- tmp_shrmp %&gt;%\n  mutate(SampleID = paste(Ocean_code, \n                          Species_code, \n                          Tissue_code, \n                          ID, \n                          sep = \"_\")\n         )\n\nAnd combine the three modified data frames.\n\ntmp_combo &lt;- rbind(tmp_control, tmp_envr, tmp_shrmp)\n\nFor posterity, we can save the shortcode data frame.\n\ntmp_combo$Taxon &lt;- NULL\nshortcodes &lt;- tmp_combo\nwrite_delim(shortcodes, \"files/tables/shortcodes.txt\",\n    delim = \"\\t\")\n\nAnd generate a modified metadata file containing the new sample ID plus the original and modified fastq file names.\n\ntmp_combo[2:5] &lt;- NULL\ntmp_fastq &lt;- read_delim(\"files/tables/fastq_info.txt\")\n\nall_metadata &lt;- dplyr::left_join(tmp_meta, tmp_combo, by = \"Original_name\") %&gt;%\n  dplyr::left_join(., tmp_fastq, by = \"Original_name\")\n\nall_metadata &lt;- all_metadata %&gt;% dplyr::relocate(\"SampleID\", \n                                         .before = \"Original_name\")\nall_metadata &lt;- all_metadata %&gt;% \n  dplyr::relocate(\"Run\", .before = \"Plate\")\n\nwrite_delim(all_metadata, \"files/tables/all_metadata.txt\",\n    delim = \"\\t\")",
    "crumbs": [
      "16S rRNA Processing",
      "1. Sample Metadata"
    ]
  },
  {
    "objectID": "docs/ssu-workflows/sampledata/index.html#modified-metadata",
    "href": "docs/ssu-workflows/sampledata/index.html#modified-metadata",
    "title": "1. Sample Metadata",
    "section": "Modified metadata",
    "text": "Modified metadata\nAnd here is the modifed metadata.\n\n\n\n\n\n\nExpand to column descriptions for sample table\n\n\n\n\n\n\n\n\n\n\n\nHeader\nDescription\n\n\n\nSampleID\nSample ID based on Ocean, species, tissue, & unique ID\n\n\nOriginal_name\nOriginal sample ID\n\n\nOcean\nSampling ocean\n\n\nMorphospecies\nHost shrimp species\n\n\nTissue\nShrimp tissue type\n\n\nHabitat\nSampling habitat\n\n\nSite\nSampling Site 1\n\n\nSite_2\nSampling Site 2\n\n\nTaxon\nShrimp, environmental samples, or Controls\n\n\nLength\nLength of individual\n\n\nStation_no\nASK MATT\n\n\nSpecies_pair\nASK MATT\n\n\nSpecies_group\nASK MATT\n\n\nSpecies_complex\nASK MATT\n\n\nRun\nSequencing run ID\n\n\nPlate\nSequencing plate ID\n\n\nRow\nSequencing plate row number\n\n\nColumn\nSequencing plate column number\n\n\nFastq_ID_forward_original\nOriginal fastq ID (F)\n\n\nFastq_ID_forward_rename\nNew fastq ID (F)\n\n\nFastq_ID_reverse_original\nOriginal fastq ID (R)\n\n\nFastq_ID_reverse_rename\nNew fastq ID (R)\n\n\n\n\n\n\n\n\nSample data & associated sequencing information.\n\n\n\n\n Download sample sequencing data \nWe sequenced a total of 1909 samples, including 1797 shrimp samples, 52 environmental samples, and 60 control samples. 884 total samples came from the Eastern Pacific and 965 from the Western Atlantic.",
    "crumbs": [
      "16S rRNA Processing",
      "1. Sample Metadata"
    ]
  },
  {
    "objectID": "docs/ssu-workflows/sampledata/index.html#rename-fastq-files",
    "href": "docs/ssu-workflows/sampledata/index.html#rename-fastq-files",
    "title": "1. Sample Metadata",
    "section": "Rename FastQ Files",
    "text": "Rename FastQ Files\nUsing the modified metadata, specifically the new sample names, we can rename all fastq file prior to processing the data. To batch rename samples we created tab-delimited lookup tables, where the first column contains the original name of each fastq file while the second column corresponds to the new name. We did this for each sequencing run. Here is an example of the first few samples from the lookup table for run BCS_34, which we call BCS_34.rename.txt\n8937-G_R1_001.trimmed.fastq EP_A_HEBE_GL_8937_R1_001.trimmed.fastq\n8937-H_R1_001.trimmed.fastq EP_A_HEBE_HP_8937_R1_001.trimmed.fastq\n8937-M_R1_001.trimmed.fastq EP_A_HEBE_MG_8937_R1_001.trimmed.fastq\n8937-S_R1_001.trimmed.fastq EP_A_HEBE_ST_8937_R1_001.trimmed.fastq\nWe use the new metadata file and the base R command split to generate initial lookup tables for each of the six sequencing runs.\n\ntmp_split_dfs &lt;- all_metadata\ntmp_split_dfs &lt;- tmp_split_dfs %&gt;% \n  dplyr::select(Run, \n                Fastq_ID_forward_original, \n                Fastq_ID_forward_rename, \n                Fastq_ID_reverse_original, \n                Fastq_ID_reverse_rename)\nsplit_dfs &lt;- split(tmp_split_dfs, tmp_split_dfs$Run)\n\nAgain, an example from run BCS_34.\n\nhead(split_dfs$BCS_34)\n\n\n\n# A tibble: 6 × 5\n  Run    Fastq_ID_forward_original Fastq_ID_forward_ren…¹ Fastq_ID_reverse_ori…²\n  &lt;chr&gt;  &lt;chr&gt;                     &lt;chr&gt;                  &lt;chr&gt;                 \n1 BCS_34 9123-G_R1_001.trimmed.fa… EP_A_ROST_GL_9123_R1_… 9123-G_R2_001.trimmed…\n2 BCS_34 9123-H_R1_001.trimmed.fa… EP_A_ROST_HP_9123_R1_… 9123-H_R2_001.trimmed…\n3 BCS_34 9123-M_R1_001.trimmed.fa… EP_A_ROST_MG_9123_R1_… 9123-M_R2_001.trimmed…\n4 BCS_34 9123-S_R1_001.trimmed.fa… EP_A_ROST_ST_9123_R1_… 9123-S_R2_001.trimmed…\n5 BCS_34 8963-G_R1_001.trimmed.fa… EP_A_HYEY_GL_8963_R1_… 8963-G_R2_001.trimmed…\n6 BCS_34 8963-M_R1_001.trimmed.fa… EP_A_HYEY_MG_8963_R1_… 8963-M_R2_001.trimmed…\n# ℹ abbreviated names: ¹​Fastq_ID_forward_rename, ²​Fastq_ID_reverse_original\n# ℹ 1 more variable: Fastq_ID_reverse_rename &lt;chr&gt;\n\n\nFinally, a little wrangling and then save each lookup table.\n\nfor (i in split_dfs) {\n  tmp_ds &lt;- data.frame(i)\n  tmp_name &lt;- as.character(i[1,1])\n  tmp_runF &lt;- tmp_ds %&gt;% \n    dplyr::select(Run, \n                  Fastq_ID_forward_original, \n                  Fastq_ID_forward_rename)\n  tmp_runF$Run &lt;- NULL\n  tmp_runF &lt;- tmp_runF %&gt;% rename(\"X1\" = 1, \"X2\" = 2)\n  tmp_runR &lt;- tmp_ds %&gt;% \n    dplyr::select(Run,\n                  Fastq_ID_reverse_original, \n                  Fastq_ID_reverse_rename)\n  tmp_runR$Run &lt;- NULL\n  tmp_runR &lt;- tmp_runF %&gt;% rename(\"X1\" = 1, \"X2\" = 2)\n  tmp_run &lt;- rbind(tmp_runF, tmp_runR)\n  assign(tmp_name, tmp_run)  \n  write.table(tmp_run, \n              paste(\"include/fastq_rename_lookup/\", \n                    tmp_name, \".rename.txt\", sep = \"\"\n                    ), \n              sep = \"\\t\", quote = FALSE, \n              row.names = FALSE, col.names = FALSE)\n  rm(list = ls(pattern = \"tmp_\"))\n}\n\nThe fastq renaming lookup tables can be accessed on GitHub  by clicking this link.\n\nOnce we have lookup tables, we can run the following bash script. This code will take the lookup table and go through each fastq file in the run directory and assign the new name.\n\nwhile IFS=$'\\t' read -r orig new; do \n    rename -v \"$orig\" \"$new\" *.fastq; \ndone &lt; BCS_34.rename.txt",
    "crumbs": [
      "16S rRNA Processing",
      "1. Sample Metadata"
    ]
  },
  {
    "objectID": "docs/ssu-workflows/sampledata/index.html#defining-groups",
    "href": "docs/ssu-workflows/sampledata/index.html#defining-groups",
    "title": "1. Sample Metadata",
    "section": "Defining Groups",
    "text": "Defining Groups\nThe last thing to do is define a sample data frame that contains only the most relevant metadata. This table will be used in all downstream analyses to define samples.\n\ntmp_shortcodes &lt;- shortcodes[order(shortcodes$SampleID), ]\ntmp_metadata &lt;- all_metadata[order(all_metadata$SampleID), ]\nidentical(tmp_shortcodes$SampleID, tmp_metadata$SampleID)\n\nsamptab &lt;- dplyr::left_join(tmp_metadata, tmp_shortcodes, by = \"SampleID\")\n\n\nsamdf &lt;- data.frame(SampleID = samptab$SampleID,\n                    OCEAN = samptab$Ocean_code,\n                    SPECIES = samptab$Species_code,\n                    TISSUE = samptab$Tissue_code,\n                    ID = samptab$ID, \n                    SITE = samptab$Site, \n                    HABITAT = samptab$Habitat, \n                    TAXON = samptab$Taxon, \n                    PAIR = samptab$Species_pair, \n                    GROUP = samptab$Species_group, \n                    COMPLEX = samptab$Species_complex, \n                    RUN = samptab$Run, \n                    PLATE = samptab$Plate\n                    )\nsaveRDS(samdf, \"files/tables/samdf.rds\")\n\nMoving on.",
    "crumbs": [
      "16S rRNA Processing",
      "1. Sample Metadata"
    ]
  },
  {
    "objectID": "docs/ssu-workflows/med/index.html",
    "href": "docs/ssu-workflows/med/index.html",
    "title": "4. MED Workflow",
    "section": "",
    "text": "Click here for page build libraries and setup information.knitr::opts_chunk$set(echo = TRUE, eval = FALSE)\nset.seed(919191)\nlibrary(dada2); packageVersion(\"dada2\")\nlibrary(ShortRead); packageVersion(\"ShortRead\")\nlibrary(ggplot2); packageVersion(\"ggplot2\")\nlibrary(Biostrings); packageVersion(\"Biostrings\")\npacman::p_load(tidyverse, gridExtra, grid, miaViz,\n               formatR, reactable, gdata, patchwork,\n               kableExtra, microeco, magrittr, \n               rprojroot,\n               tidySummarizedExperiment, scater,\n               install = FALSE, update = FALSE)\n\noptions(scipen = 999)\nknitr::opts_current$get(c(\n  \"cache\",\n  \"cache.path\",\n  \"cache.rebuild\",\n  \"dependson\",\n  \"autodep\"\n))\n#root &lt;- find_root(has_file(\"_quarto.yml\"))\n#source(file.path(root, \"assets\", \"functions.R\"))",
    "crumbs": [
      "16S rRNA Processing",
      "4. MED Workflow"
    ]
  },
  {
    "objectID": "docs/ssu-workflows/med/index.html#overview",
    "href": "docs/ssu-workflows/med/index.html#overview",
    "title": "4. MED Workflow",
    "section": "Overview",
    "text": "Overview\nWith the mothur pipeline finished, we can turn our attention to Minimum Entropy Decomposition (MED) (Eren et al. 2015). MED is a novel, information theory-based clustering algorithm for sensitive partitioning of high-throughput marker gene sequences. Basically, MED needs redundant (identical sequences included) alignment file of read data. We again use mothur but this pipeline starts with the output of the align.seqs step from the mothur OTU pipeline.\nWe set up our run in the same way as the mothur pipeline.\n\n\n\n\n\nflowchart LR\n  A(Start with output&lt;/br&gt;from align.seqs)\n  A --&gt; B(Remove&lt;br/&gt;Negative&lt;br/&gt;Controls)\n  C(\"get.groups &lt;br/&gt; (NC Samples)\")\n  C --&gt; B\n  B --&gt; D(remove.seqs)\n  B --&gt; E(remove.groups)\n  D --&gt; F\n  E --&gt; F(chimera.vsearch)\n  F --&gt; G(classify.seqs)\n  G --&gt; H(remove.lineage)\n\n\n\n\n\n\n\n\n\n\n\n\nExpand to see environment variables\n\n\n\n\n\n$ export DATA=01_TRIMMED_DATA/\n$ export TYPE=fastq\n$ export PROC=30\n\n$ export REF_LOC=reference_dbs\n$ export TAXREF_FASTA=gsrdb.fasta\n$ export TAXREF_TAX=gsrdb.tax\n$ export ALIGNREF=silva.v4.fasta\n\n$ export CONTAMINENTS=Chloroplast-Mitochondria-unknown-Eukaryota",
    "crumbs": [
      "16S rRNA Processing",
      "4. MED Workflow"
    ]
  },
  {
    "objectID": "docs/ssu-workflows/med/index.html#getting-started",
    "href": "docs/ssu-workflows/med/index.html#getting-started",
    "title": "4. MED Workflow",
    "section": "Getting Started",
    "text": "Getting Started\n\n\n\n\n\nCode chunk coloring by language\n\nColor\nLanguage\n\n\n\nblue\nR\n\n\nvermillion\nbash\n\n\nbluish green\nshell\n\n\nreddish purple\nmothur\n\n\n\n\n\nWe make use of many different coding languages in these workflows. Code chucks are colored by language.\nset.dir(output=pipelineFiles_med/)\nMothur's directories:\noutputDir=pipelineFiles_med/\nWe first copy the output of the align.seqs portion of the mothur workflow.\nsystem(cp pipelineFiles/shrimp.trim.contigs.good.unique.good.filter.unique.fasta pipelineFiles_med/)\nsystem(cp pipelineFiles/shrimp.trim.contigs.good.unique.good.filter.count_table pipelineFiles_med/)",
    "crumbs": [
      "16S rRNA Processing",
      "4. MED Workflow"
    ]
  },
  {
    "objectID": "docs/ssu-workflows/med/index.html#remove-negative-controls",
    "href": "docs/ssu-workflows/med/index.html#remove-negative-controls",
    "title": "4. MED Workflow",
    "section": "Remove Negative Controls",
    "text": "Remove Negative Controls\nAs before, we remove NC samples, but we skip the pre.cluster. We need to remove the NC samples and reads found in those sample. We first identified all reads that were present in at least one NC sample represented by at least 1 read. We did this by subsetting the NC samples from dataset.\nget.groups(fasta=shrimp.trim.contigs.good.unique.good.filter.unique.fasta, count=shrimp.trim.contigs.good.unique.good.filter.count_table, groups=Control_49-Control_23-Control_17-Control_24-Control_14-Control_44-Control_20-Control_33-Control_41-Control_29-Control_50-Control_22-Control_19-Control_18-Control_48-Control_13-Control_21-Control_16-Control_30-Control_5-Control_42-Control_25-Control_51-Control_40-Control_15-Control_36-Control_47-Control_27-Control_32-Control_8-Control_3-Control_4-Control_6-Control_45-Control_26-Control_46-Control_53-Control_7-Control_12-Control_10-Control_9-Control_35-Control_54-Control_2-Control_43-Control_1-Control_11-Control_52-Control_38-Control_34-Control_56-Control_37-Control_28-Control_57-Control_31-Control_39-Control_59-Control_55-Control_60-Control_58)\nSelected 193014 sequences from your count file.\nSelected 34302 sequences from your fasta file.\n\nOutput File names:\nshrimp.trim.contigs.good.unique.good.filter.pick.count_table\nshrimp.trim.contigs.good.unique.good.filter.unique.pick.fasta\nrename.file(input=current, new=neg_control.fasta)\nrename.file(input=current, new=neg_control.count_table)\nsummary.seqs(fasta=neg_control.fasta, count=neg_control.count_table, processors=$PROC)\n\n\n\n\n\n\nExpand to see negative control summary\n\n\n\n\n\n\n\n\nStart\nEnd\nNBases\nAmbigs\nPolymer\nNumSeqs\n\n\n\nMinimum\n1\n735\n251\n0\n3\n1\n\n\n2.5%-tile:\n1\n736\n252\n0\n4\n4826\n\n\n25%-tile:\n1\n736\n253\n0\n4\n48254\n\n\nMedian:\n1\n736\n253\n0\n5\n96508\n\n\n75%-tile:\n1\n736\n253\n0\n5\n144761\n\n\n97.5%-tile:\n1\n736\n254\n0\n6\n188189\n\n\nMaximum:\n4\n736\n254\n0\n6\n193014\n\n\nMean:\n1\n735\n253\n0\n4\n\n\n\n\n# of unique seqs:   34302\ntotal # of seqs:    193014\n\nIt took 0 secs to summarize 193014 sequences.\n\nOutput File Names:\nneg_control.summary\n\n\n\nlist.seqs(count=neg_control.count_table)\nOutput File Names: \nneg_control.accnos\nThe next step is to use the *.accnos file from the previous step to remove any reads found in negative control (NC) samples. This seems reasonable enough except mothur will remove any read found in a NC sample. For example, let’s say we have two reads:\nread01 is found in most NC samples and not found in any remaining samples.read02 on the other hand is represented by one read in a single NC sample but very abundant in remaining samples.\nIt makes a lot of sense to remove read01 but not so for read02. So we need to make a custom *.accnos that only contains reads that are abundant in NC samples. To do this we will do a little data wrangling in R. For this we need two *.count_table files–one from the align.seqs section and the other from the step above. We got the idea on how best to do this from the mothur forum.\nEssentially, for each repseq, the code below calculates:\n\nThe total number of NC samples containing at least 1 read.\n\nThe total number of reads in NC samples.\n\nThe total number of non-NC samples containing at least 1 read.\n\nThe total number of reads in non-NC samples.\n\nThe percent of reads in the NC samples and the percent of NC samples containing reads.\n\nThe first command parses out the necessary data from the .count_table files.\n\ntmp_nc_reads &lt;- read_tsv(\n  \"files/tables/neg_control.count_table\",\n  col_names = TRUE,\n  col_types = NULL,\n  skip = 2,\n  col_select = c(\"Representative_Sequence\", \"total\")\n)\ntmp_non_nc_reads &lt;- read_tsv(\n  \"files/tables/shrimp.trim.contigs.good.unique.good.filter.count_table\",\n  col_names = TRUE,\n  col_types = NULL,\n  skip = 2,\n  col_select = c(\"Representative_Sequence\", \"total\"),\n)\ntmp_reads &lt;- dplyr::left_join(tmp_nc_reads, tmp_non_nc_reads,\n                              by = \"Representative_Sequence\")\ntmp_reads &lt;- tmp_reads %&gt;% dplyr::rename(c(\n                            \"total_reads_NC\" = \"total.x\", \n                            \"total_reads_samps\" = \"total.y\")\n                            ) \n\nAnd here is what the new dataframe looks like. Three columns where the first is the repseq name, the second the total number of reads in NC samples, and the third the total number of reads in the entire dataset.\n\n\n                       Representative_Sequence total_reads_NC total_reads_samps\n1 M06508_16_000000000-CMN9T_1_1113_21857_26306              1                 1\n2  M06508_19_000000000-CR8CR_1_2101_7451_16837              1                 1\n3 M06508_16_000000000-CMN9T_1_2103_11662_22768              1                 1\n4 M06508_18_000000000-CNPPR_1_2101_17675_20095              1                 1\n5 M06508_12_000000000-CJG44_1_2105_17229_20573              2                 2\n6  M06508_16_000000000-CMN9T_1_1114_23424_6835              2                 2\n\n\nWe identified 34,302 reads that were potential contaminants.\nNow we add in a column that calculates the percent of reads in the NC samples.\n\ntmp_nc_check &lt;- tmp_reads %&gt;%\n  dplyr::mutate(perc_in_neg = 100*(\n    total_reads_NC / (total_reads_NC + total_reads_samps)),\n                .after = \"total_reads_samps\")\ntmp_nc_check$perc_in_neg &lt;- round(tmp_nc_check$perc_in_neg, digits = 6)\n\nNow we use the count.seqs command in mothur to generate a file describing the distribution of NC reads across all samples. This information will tell us how many samples contain these reads.\ncount.seqs(count=neg_control.count_table, compress=f)\ncount.seqs(count=shrimp.trim.contigs.good.unique.good.filter.count_table, compress=f)\n\ntmp_nc_dist &lt;- read_tsv(\"neg_control.full.count_table\")\n\ntmp_all_dist &lt;- read_tsv(\"shrimp.trim.contigs.good.unique.good.filter.full.count_table\")\ntmp_all_dist &lt;- tmp_all_dist %&gt;% select(-starts_with(\"Control_\"))\n\ntmp_nc_dist$total &lt;- NULL\ntmp_all_dist$total &lt;- NULL\n\ntmp_nc_read_dist &lt;- dplyr::left_join(\n                    tmp_nc_dist, \n                    tmp_all_dist, \n                    by = \"Representative_Sequence\")\n\nAfter a little wrangling, the dataframe looks like this:\n\n\n# A tibble: 6 × 5\n  Representative_Sequence             Control_1 Control_10 Control_11 Control_12\n  &lt;chr&gt;                                   &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 M06508_16_000000000-CMN9T_1_1113_2…         0          0          0          0\n2 M06508_19_000000000-CR8CR_1_2101_7…         0          0          0          0\n3 M06508_16_000000000-CMN9T_1_2103_1…         0          0          0          0\n4 M06508_18_000000000-CNPPR_1_2101_1…         0          0          0          0\n5 M06508_12_000000000-CJG44_1_2105_1…         0          0          0          0\n6 M06508_16_000000000-CMN9T_1_1114_2…         0          0          0          0\n\n\nAnd then we calculate row sums to get the number of NC and non-NC samples containing these reads.\nAnd again calculate the percent of NC samples containing these reads.\n\ntmp_nc_check &lt;- cbind(tmp_nc_check, tmp_1, tmp_2, tmp_3)\ntmp_nc_check &lt;- tmp_nc_check %&gt;% dplyr::rename(\"total_samples\" = 5)  \ncolnames(tmp_nc_check)\ntmp_nc_check &lt;- tmp_nc_check %&gt;%\n  dplyr::mutate(perc_in_neg_samp = \n                  100*( num_samp_nc / (num_samp_nc + num_samp_no_nc)),\n                  .after = \"num_samp_no_nc\")\nnc_check &lt;- tmp_nc_check\n\n\n\nSummary of reads detected in Negative Control (NC) samples.\n\n\n\n\n Download summary of reads detected in Negative Control (NC) samples \nNow we remove any repseqs where:\n\nThe number of reads found in NC samples accounted for more than 10% of total reads OR\nThe percent of NC samples containing the ASV was greater than 10% of total samples.\n\n\n\n\n\n\n\n\n\n\n\nTotal rep seqs\nNC reads\nnon NC reads\n% NC reads\n\n\n\nRemoved\n32220\n162554\n339140\n32.401\n\n\nRetained\n2082\n30460\n10357755\n0.293\n\n\n\nWe identified a total of 34302 representative sequences (rep_seq) that were present in at least 1 NC sample by at least 1 read. We removed any rep_seq where more than 10% of total reads were found in NC samples OR any rep_seq found in more than 10% of NC samples. Based on these criteria we removed 32220 rep_seqs from the data set, which represented 162554 total reads in NC samples and 339140 total reads in non-NC samples. Of the total reads removed 32.401% came from NC samples. Of all rep_seqs identified in NC samples, 2082 were retained because they fell below the threshhold criteria. These rep_seqs accounted for 30460 reads in NC samples and 10357755 reads in non-NC samples. NC samples accounted for 0.293% of these reads.\nOK, now we can create a new neg_control.accnos containing only rep_seqs abundant in NC samples.\n\nwrite_delim(\n  data.frame(nc_remove$Representative_Sequence), \n  \"files/tables/neg_control_subset.accnos\", \n  col_names = FALSE)\n\nAnd then use this file in conjunction with the mothur command remove.seqs.\nremove.seqs(accnos=neg_control_subset.accnos, fasta=shrimp.trim.contigs.good.unique.good.filter.unique.fasta, count=shrimp.trim.contigs.good.unique.good.filter.count_table)\nRemoved 31958 sequences from shrimp.trim.contigs.good.unique.good.filter.unique.fasta.\nRemoved 344423 sequences from shrimp.trim.contigs.good.unique.good.filter.count_table.\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.good.filter.unique.pick.fasta\nshrimp.trim.contigs.good.unique.good.filter.pick.count_table\ncount.groups(count=current)\nSize of smallest group: 1.\n\nTotal seqs: 36032555.\n\nOutput File Names: \nshrimp.trim.contigs.good.unique.good.filter.pick.count.summary\nBefore we remove the NC samples we need to check whether some NC samples were already removed. When mothur runs the remove.seqs command it will automatically remove any samples where the read count has fallen to zero. If mothur did remove samples and we try to remove all NC samples, we will get an error. To check we can compare the count.summary files before and after the previous remove.seqs command.\n\ntmp_before &lt;- read_tsv(\n  \"files/tables/shrimp.trim.contigs.good.unique.good.filter.count.summary\",\n  col_names = FALSE,\n  col_select = 1\n)\ntmp_after &lt;- read_tsv(\n  \"files/tables/shrimp.trim.contigs.good.unique.good.filter.pick.count.summary\",\n  col_names = FALSE,\n  col_select = 1\n)\ntmp_nc_lost &lt;- anti_join(tmp_before, tmp_after)\ntmp_nc_lost$X1\n\nThese are the samples that were already removed when remove.seqs was run above. We need to remove these from our list of NC samples.\n[1] \"Control_18\" \"Control_5\" \n\nnc_to_remove &lt;- semi_join(tmp_before, tmp_after)\nnc_to_remove &lt;- nc_to_remove %&gt;%\n  dplyr::filter(\n    stringr::str_starts(X1, \"Control\")\n    )\n\nnc_to_remove &lt;- paste0(nc_to_remove$X1, collapse=\"-\")\n\n\nnc_to_remove\n\n[1] \"Control_1-Control_10-Control_11-Control_12-Control_13-Control_14-Control_15-Control_16-Control_17-Control_19-Control_2-Control_20-Control_21-Control_22-Control_23-Control_24-Control_25-Control_26-Control_27-Control_28-Control_29-Control_3-Control_30-Control_31-Control_32-Control_33-Control_34-Control_35-Control_36-Control_37-Control_38-Control_39-Control_4-Control_40-Control_41-Control_42-Control_43-Control_44-Control_45-Control_46-Control_47-Control_48-Control_49-Control_50-Control_51-Control_52-Control_53-Control_54-Control_55-Control_56-Control_57-Control_58-Control_59-Control_6-Control_60-Control_7-Control_8-Control_9\"\n\n\nremove.groups(count=shrimp.trim.contigs.good.unique.good.filter.pick.count_table, fasta=shrimp.trim.contigs.good.unique.good.filter.unique.pick.fasta, groups=Control_1-Control_10-Control_11-Control_12-Control_13-Control_14-Control_15-Control_16-Control_17-Control_19-Control_2-Control_20-Control_21-Control_22-Control_23-Control_24-Control_25-Control_26-Control_27-Control_28-Control_29-Control_3-Control_30-Control_31-Control_32-Control_33-Control_34-Control_35-Control_36-Control_37-Control_38-Control_39-Control_4-Control_40-Control_41-Control_42-Control_43-Control_44-Control_45-Control_46-Control_47-Control_48-Control_49-Control_50-Control_51-Control_52-Control_53-Control_54-Control_55-Control_56-Control_57-Control_58-Control_59-Control_6-Control_60-Control_7-Control_8-Control_9)\nRemoved 26944 sequences from your count file.\nRemoved 0 sequences from your fasta file.\n\nOutput File names: \nshrimp.trim.contigs.good.unique.good.filter.pick.pick.count_table\nshrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.fasta\nsummary.seqs(fasta=current, count=current, processors=$PROC)\n\n\n\n\n\n\nExpand to see data set summary\n\n\n\n\n\n\n\n\nStart\nEnd\nNBases\nAmbigs\nPolymer\nNumSeqs\n\n\n\nMinimum\n1\n735\n208\n0\n3\n1\n\n\n2.5%-tile:\n1\n736\n252\n0\n3\n900141\n\n\n25%-tile:\n1\n736\n253\n0\n4\n9001403\n\n\nMedian:\n1\n736\n253\n0\n4\n18002806\n\n\n75%-tile:\n1\n736\n253\n0\n5\n27004209\n\n\n97.5%-tile:\n1\n736\n253\n0\n6\n35105471\n\n\nMaximum:\n4\n736\n254\n0\n6\n36005611\n\n\nMean:\n1\n735\n252\n0\n4\n\n\n\n\n# of unique seqs:   4160331\ntotal # of seqs:    36005611\n\nIt took 74 secs to summarize 36005611 sequences.\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.summary\n\n\n\ncount.groups(count=current)\nSize of smallest group: 49.\n\nTotal seqs: 36005611.\n\nOutput File Names: \nshrimp.trim.contigs.good.unique.good.filter.pick.pick.count.summary",
    "crumbs": [
      "16S rRNA Processing",
      "4. MED Workflow"
    ]
  },
  {
    "objectID": "docs/ssu-workflows/med/index.html#remove-chimeras",
    "href": "docs/ssu-workflows/med/index.html#remove-chimeras",
    "title": "4. MED Workflow",
    "section": "Remove Chimeras",
    "text": "Remove Chimeras\nchimera.vsearch(fasta=current, count=current, dereplicate=t, processors=$PROC)\nUsing vsearch version v2.15.2.\nChecking sequences from shrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.fasta \n...\n\n/******************************************/\nSplitting by sample: \n\n...\n\nRemoving chimeras from your input files:\n/******************************************/\nRunning command: remove.seqs(fasta=shrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.fasta,\naccnos=shrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.accnos)\nRemoved 714610 sequences from shrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.fasta.\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.pick.fasta\n\n/******************************************/\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.good.filter.pick.pick.denovo.vsearch.count_table\nshrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.chimeras\nshrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.accnos\nshrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.fasta\nsummary.seqs(fasta=current, count=current, processors=$PROC)\n\n\n\n\n\n\nExpand to see data set summary\n\n\n\n\n\n\n\n\nStart\nEnd\nNBases\nAmbigs\nPolymer\nNumSeqs\n\n\n\nMinimum\n1\n735\n208\n0\n3\n1\n\n\n2.5%-tile:\n1\n736\n252\n0\n3\n867900\n\n\n25%-tile:\n1\n736\n253\n0\n4\n8678991\n\n\nMedian:\n1\n736\n253\n0\n4\n17357982\n\n\n75%-tile:\n1\n736\n253\n0\n5\n26036972\n\n\n97.5%-tile:\n1\n736\n253\n0\n6\n33848063\n\n\nMaximum:\n4\n736\n254\n0\n6\n34715962\n\n\nMean:\n1\n735\n252\n0\n4\n\n\n\n\n# of unique seqs:   3445721\ntotal # of seqs:    34715962\n\nIt took 64 secs to summarize 34715962 sequences.\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.summary\n\n\n\ncount.groups(count=current)\nSize of smallest group: 49.\n\nTotal seqs: 34715962.\n\nOutput File Names: \nshrimp.trim.contigs.good.unique.good.filter.pick.pick.denovo.vsearch.count.summary\nThe classify.seqs command requires properly formatted reference and taxonomy databases. For taxonomic assignment, we are using the GSR database (Molano, Vega-Abellaneda, and Manichanh 2024). The developers of mothur maintain formatted versions of popular databases, however the GSR-DB has not been formatted by the developers yet.\n\n\n\n\n\n\nNote\n\n\n\nYou can download an appropriate version of the GSR database here.\n\n\nTo create a mothur formatted version GSR-DB1, we perform the following steps.\nDownload a data base\nHere we are using the GSR V4 database.\n\nwget https://manichanh.vhir.org/gsrdb/GSR-DB_V4_cluster-1.tar.gz\ntar -xvzf GSR-DB_V4_cluster-1.tar.gz\n\nFirst (in the command line) we remove first line of the taxonomy file.\n\ncp GSR-DB_V4_cluster-1_taxa.txt tmp0.txt\nsed '1d' tmp0.txt &gt; tmp1.txt\n\nNext, delete species and remove leading [a-z]__ from taxa names\n\nsed -E 's/s__.*//g' tmp1.txt &gt; tmp2.txt\nsed -E 's/[a-zA-Z]__//g' tmp2.txt &gt; gsrdb.tax\ncp GSR-DB_V4_cluster-1_seqs.fasta gsrdb.fasta\n\nclassify.seqs(fasta=current, count=current, reference=$REF_LOC/$TAXREF_FASTA, taxonomy=$REF_LOC/$TAXREF_TAX, processors=$PROC)\nReading template taxonomy...     DONE.\nReading template probabilities...     DONE.\nIt took 4 seconds get probabilities.\nClassifying sequences from \nshrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.fasta ...\n[WARNING]: M06508_18_000000000-CNPPR_1_1112_24743_21202 could not be classified. \nYou can use the remove.lineage command with taxon=unknown; to remove such sequences.\n\n...\n\nIt took 839 secs to classify 3445721 sequences.\n\nIt took 1697 secs to create the summary file for 3445721 sequences.\n\nOutput File Names: \nshrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.pds.wang.taxonomy\nshrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.pds.wang.tax.summary",
    "crumbs": [
      "16S rRNA Processing",
      "4. MED Workflow"
    ]
  },
  {
    "objectID": "docs/ssu-workflows/med/index.html#remove-contaminants",
    "href": "docs/ssu-workflows/med/index.html#remove-contaminants",
    "title": "4. MED Workflow",
    "section": "Remove Contaminants",
    "text": "Remove Contaminants\nremove.lineage(fasta=current, count=current, taxonomy=current, taxon=$CONTAMINENTS)\nRunning command: \nremove.seqs(accnos=shrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.pds.wang.accnos,\ncount=shrimp.trim.contigs.good.unique.good.filter.pick.pick.denovo.vsearch.count_table,\nfasta=shrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.fasta)\n\nRemoved 2003 sequences from shrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.fasta.\nRemoved 8033 sequences from shrimp.trim.contigs.good.unique.good.filter.pick.pick.denovo.vsearch.count_table.\n\n/******************************************/\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.gsrdb.wang.pick.taxonomy\nshrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.gsrdb.wang.accnos\nshrimp.trim.contigs.good.unique.good.filter.pick.pick.denovo.vsearch.pick.count_table\nshrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.pick.fasta\nsummary.seqs(fasta=current, count=current, processors=$PROC)\n\n\n\n\n\n\nExpand to see data set summary\n\n\n\n\n\n\n\n\nStart\nEnd\nNBases\nAmbigs\nPolymer\nNumSeqs\n\n\n\nMinimum\n1\n735\n208\n0\n3\n1\n\n\n2.5%-tile:\n1\n736\n252\n0\n3\n867699\n\n\n25%-tile:\n1\n736\n253\n0\n4\n8676983\n\n\nMedian:\n1\n736\n253\n0\n4\n17353965\n\n\n75%-tile:\n1\n736\n253\n0\n5\n26030947\n\n\n97.5%-tile:\n1\n736\n253\n0\n6\n33840231\n\n\nMaximum:\n4\n736\n254\n0\n6\n34707929\n\n\nMean:\n1\n735\n252\n0\n4\n\n\n\n\n# of unique seqs:   3443718\ntotal # of seqs:    34707929\n\nIt took 62 secs to summarize 34707929 sequences.\n\nOutput File Names:\nshrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.pick.summary\n\n\n\nsummary.tax(taxonomy=current, count=current)\n[WARNING]: processors is not a valid parameter, ignoring.\nUsing shrimp.trim.contigs.good.unique.good.filter.pick.pick.denovo.vsearch.pick.count_table \nas input file for the count parameter.\nUsing shrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.pds.wang.pick.taxonomy \nas input file for the taxonomy parameter.\n\nIt took 1580 secs to create the summary file for 34011832 sequences.\n\nOutput File Names: \nshrimp.trim.contigs.good.unique.good.filter.unique.pick.pick.denovo.vsearch.pds.wang.pick.tax.summary\ncount.groups(count=current)\nSize of smallest group: 49.\n\nTotal seqs: 34707929\n\nOutput File Names: \nshrimp.trim.contigs.good.unique.good.filter.pick.pick.denovo.vsearch.pick.count.summary",
    "crumbs": [
      "16S rRNA Processing",
      "4. MED Workflow"
    ]
  },
  {
    "objectID": "docs/ssu-workflows/med/index.html#track-reads-through-workflow",
    "href": "docs/ssu-workflows/med/index.html#track-reads-through-workflow",
    "title": "4. MED Workflow",
    "section": "Track Reads through Workflow",
    "text": "Track Reads through Workflow\nAt this point we can look at the number of reads that made it through each step of the workflow for every sample.\n\nread_change &lt;- read_tsv(\n  \"include/tables/all_sample_med_read_changes.txt\",\n  col_names = TRUE\n)\n\n\n\nTracking read changes at each step of the mothur/med workflow.\n\n\n\n\n Download read changes for MED pipeline",
    "crumbs": [
      "16S rRNA Processing",
      "4. MED Workflow"
    ]
  },
  {
    "objectID": "docs/ssu-workflows/med/index.html#preparing-for-analysis",
    "href": "docs/ssu-workflows/med/index.html#preparing-for-analysis",
    "title": "4. MED Workflow",
    "section": "Preparing for analysis",
    "text": "Preparing for analysis\nrename.file(fasta=current, count=current, taxonomy=current, prefix=final_med)\nCurrent files saved by mothur:\nfasta=final.fasta\ntaxonomy=final.taxonomy\ncount=final.count_table",
    "crumbs": [
      "16S rRNA Processing",
      "4. MED Workflow"
    ]
  },
  {
    "objectID": "docs/ssu-workflows/med/index.html#assign-taxonomy",
    "href": "docs/ssu-workflows/med/index.html#assign-taxonomy",
    "title": "4. MED Workflow",
    "section": "Assign Taxonomy",
    "text": "Assign Taxonomy\nOur first step is to classify the node representatives from the MED output. The classify.seqs command requires properly formatted reference and taxonomy databases. For taxonomic assignment, we are using the GSR database (Molano, Vega-Abellaneda, and Manichanh 2024). The developers of mothur maintain formatted versions of popular databases, however the GSR-DB has not been formatted by the developers yet.\n\n\n\n\n\n\nNote\n\n\n\nHere you can download an appropriate version of the GSR database.\n\n\nTo create a mothur formatted version GSR-DB2, we perform the following steps.\nDownload a data base\nHere we are using the GSR V4 database.\n\nwget https://manichanh.vhir.org/gsrdb/GSR-DB_V4_cluster-1.tar.gz\ntar -xvzf GSR-DB_V4_cluster-1.tar.gz\n\nFirst (in the command line) we remove first line of the taxonomy file.\n\ncp GSR-DB_V4_cluster-1_taxa.txt tmp0.txt\nsed '1d' tmp0.txt &gt; tmp1.txt\n\nNext, delete species and remove leading [a-z]__ from taxa names\n\nsed -E 's/s__.*//g' tmp1.txt &gt; tmp2.txt\nsed -E 's/[a-zA-Z]__//g' tmp2.txt &gt; gsrdb.tax\ncp GSR-DB_V4_cluster-1_seqs.fasta gsrdb.fasta\n\nThe next thing we need to do is grab the node-representatives.fa.txt from the MED output so that we can classify these sequences. Of course, proper formatting is required.\n\nseqkit replace -p ^ -r MED node-representatives.fa.txt &gt; tmp1.fa\nseqkit replace -p \"\\|.*\" -r '' tmp1.fa &gt; med_nodes.fasta\nrm tmp1.fa\n\nGreat, the reference database is formatted. Now we need to make a few files that mimic normal mothur output files because the MED pipeline does not exactly genereate the files we need to generate a microtable object. First we use the matrix_counts.txt file from the MED analysis to create a mothur-styled count.table.\n\ntmp_med_counts &lt;- read_tsv(\n  \"include/results/matrix_counts.txt\",\n    col_names = TRUE\n)\ntmp_med_counts &lt;- tmp_med_counts %&gt;% \n  dplyr::rename_with( ~ paste0(\"MED\", .x)) \n\ntmp_med_counts &lt;- tmp_med_counts %&gt;%\n  tidyr::pivot_longer(cols = c(-1), names_to = \"tmp\") %&gt;%\n  tidyr::pivot_wider(names_from = c(1))\n\ntmp_med_counts &lt;- tibble::column_to_rownames(tmp_med_counts, \"tmp\")\n\ntmp_med_counts &lt;- tmp_med_counts %&gt;%\n                  mutate(total = rowSums(.), .before = 1)\n\ntmp_med_counts &lt;- tmp_med_counts %&gt;% \n     tibble::rownames_to_column(\"Representative_Sequence\")\nmed_counts &lt;- tmp_med_counts\n\nNow we can actually classify the representative sequences.\nclassify.seqs(fasta=med_nodes.fasta, count=med_nodes.count.table, reference=gsrdb.fasta, taxonomy=gsrdb.tax)\nNow we make a mothur styled shared file.\n\ntmp_med_counts &lt;- read_tsv(\n  \"include/results/matrix_counts.txt\",\n    col_names = TRUE\n)\ntmp_n_meds &lt;- ncol(tmp_med_counts) - 1\ntmp_med_counts &lt;- tmp_med_counts %&gt;% \n  dplyr::rename_with( ~ paste0(\"MED\", .x)) %&gt;% \n  dplyr::rename(\"Group\" = \"MEDsamples\")\n\ntmp_med_counts &lt;- tmp_med_counts %&gt;% \n  tibble::add_column(label = 0.03, .before = \"Group\") %&gt;% \n  tibble::add_column(numOtus = tmp_n_meds, .after = \"Group\")\nmed_shared &lt;- tmp_med_counts\n\nA. Taxonomy Table\nHere is what the taxonomy table looks like in the microeco mock data.\n\ntaxonomy_table_16S[1:6, 1:4]\n\n         Kingdom      Phylum            Class                 Order\nOTU_4272 k__Bacteria  p__Firmicutes     c__Bacilli            o__Bacillales\nOTU_236  k__Bacteria  p__Chloroflexi    c__                   o__\nOTU_399  k__Bacteria  p__Proteobacteria c__Betaproteobacteria o__Nitrosomonadales\nOTU_1556 k__Bacteria  p__Acidobacteria  c__Acidobacteria      o__Subgroup 17\nOTU_32   k__Archaea   p__Miscellaneous  c__                   o__\nOTU_706  k__Bacteria  p__Actinobacteria c__Actinobacteria     o__Frankiales\nOur taxonomy file (below) needs a little wrangling to be properly formatted.\n\ntmp_tax &lt;- read_delim(\"files/mothur/med_nodes.cons.taxonomy\", \n                      delim = \"\\t\")\nhead(tmp_tax)\n\nOTU           Size   Taxonomy\nMED000013400  928635 Bacteria(100);Proteobacteria(96);Gammaproteobacteria(87);Gammaproteobacteria_unclassified(87);Gammaproteobacteria_unclassified(87);Gammaproteobacteria_unclassified(87);\nMED000014292  842658 Bacteria(100);Proteobacteria(100);Gammaproteobacteria(100);Vibrionales(100);Vibrionaceae(100);Vibrio(100);\nMED000012327  643565 Bacteria(100);Bacteria_unclassified(100);Bacteria_unclassified(100);Bacteria_unclassified(100);Bacteria_unclassified(100);Bacteria_unclassified(100);\nMED000012097  630670 Bacteria(100);Bacteria_unclassified(100);Bacteria_unclassified(100);Bacteria_unclassified(100);Bacteria_unclassified(100);Bacteria_unclassified(100);\nMED000009320  571311 Bacteria(100);Bacteria_unclassified(100);Bacteria_unclassified(100);Bacteria_unclassified(100);Bacteria_unclassified(100);Bacteria_unclassified(100);\nMED000013103  536060 Bacteria(100);Bacteria_unclassified(100);Bacteria_unclassified(100);Bacteria_unclassified(100);Bacteria_unclassified(100);Bacteria_unclassified(100);\nSome fancy string manipulation…\n\ntmp_tax &lt;- data.frame(sapply(tmp_tax, \n                             gsub, \n                             pattern = \"\\\\(\\\\d+\\\\)\", \n                             replacement = \"\"))\ntmp_tax &lt;- data.frame(sapply(tmp_tax, \n                             gsub, \n                             pattern = \";$\", \n                             replacement = \"\"))\ntmp_tax &lt;- separate_wider_delim(tmp_tax, \n                              cols = Taxonomy, \n                              delim = \";\", names = c(\n                                \"Kingdom\", \"Phylum\", \n                                \"Class\", \"Order\", \n                                \"Family\", \"Genus\" \n                                )\n                              )\ntmp_tax &lt;- data.frame(sapply(tmp_tax, gsub, \n                           pattern = \"^.*_unclassified$\", \n                           replacement = \"\"))\ntmp_tax$Size &lt;- NULL\ntmp_tax &lt;- tibble::column_to_rownames(tmp_tax, \"OTU\")\n\nAnd we get this …\n              Kingdom   Phylum          Class                Order             Family             Genus\nMED000014526  Bacteria  Proteobacteria  Alphaproteobacteria  Rhodobacterales    \nMED000014072  Bacteria  Bacteroidetes   Bacteroidia          Marinilabiliales  Marinilabiliaceae  Saccharicrinis\nMED000013616  Bacteria  Proteobacteria  Alphaproteobacteria  Rhodobacterales    \nMED000011731  Bacteria  Proteobacteria  Alphaproteobacteria  Sphingomonadales  Erythrobacteraceae Qipengyuania\nMED000013191  Bacteria  Cyanobacteria   Unknown              Synechococcales   Prochlorococcaceae Prochlorococcus\nMED000012098  Bacteria          \n\ntmp_tax[is.na(tmp_tax)] &lt;- \"\"\ntmp_tax$Kingdom &lt;- paste(\"k\", tmp_tax$Kingdom, sep = \"__\")\ntmp_tax$Phylum &lt;- paste(\"p\", tmp_tax$Phylum, sep = \"__\")\ntmp_tax$Class &lt;- paste(\"c\", tmp_tax$Class, sep = \"__\")\ntmp_tax$Order &lt;- paste(\"o\", tmp_tax$Order, sep = \"__\")\ntmp_tax$Family &lt;- paste(\"f\", tmp_tax$Family, sep = \"__\")\ntmp_tax$Genus &lt;- paste(\"g\", tmp_tax$Genus, sep = \"__\")\n\ntmp_tax %&lt;&gt;% tidy_taxonomy\n\nAnd then this. Excatly like the mock data.\n              Kingdom      Phylum             Class                   Order               Family                Genus\nMED000014526  k__Bacteria  p__Proteobacteria  c__Alphaproteobacteria  o__Rhodobacterales  f__                   g__\nMED000014072  k__Bacteria  p__Bacteroidetes   c__Bacteroidia          o__Marinilabiliales f__Marinilabiliaceae  g__Saccharicrinis\nMED000013616  k__Bacteria  p__Proteobacteria  c__Alphaproteobacteria  o__Rhodobacterales  f__                   g__\nMED000011731  k__Bacteria  p__Proteobacteria  c__Alphaproteobacteria  o__Sphingomonadales f__Erythrobacteraceae g__Qipengyuania\nMED000013191  k__Bacteria  p__Cyanobacteria   c__                     o__Synechococcales  f__Prochlorococcaceae g__Prochlorococcus\nMED000012098  k__Bacteria  p__                c__                     o__                 f__                   g__\nB. Sequence Table\nHere is what the sequence table looks like in the mock data.\n\notu_table_16S[1:6, 1:11]\n\n         S1 S2 S3 S4  S5  S6  S7 S9 S10 S11 S12\nOTU_4272  1  0  1  1   0   0   1  1   0   1   1\nOTU_236   1  4  0  2  35   5  94  0 177  14  27\nOTU_399   9  2  2  4   4   0   3  6   0   1   2\nOTU_1556  5 18  7  3   2   9   2  6   1   2   1\nOTU_32   83  9 19  8 102 300 158 55 321  16  13\nOTU_706   0  1  0  1   0   0   1  0   0   0   1\n\n\nThese code block will return a properly formatted sequence table.\n\ntmp_st &lt;- readr::read_delim(\"files/mothur/med_nodes.shared\", \n                    delim = \"\\t\")\n\n\ntmp_st$numOtus &lt;- NULL\ntmp_st$label &lt;- NULL\ntmp_st &lt;- tmp_st %&gt;%\n  tidyr::pivot_longer(cols = c(-1), names_to = \"tmp\") %&gt;%\n  tidyr::pivot_wider(names_from = c(1))\n\ntmp_st &lt;- tibble::column_to_rownames(tmp_st, \"tmp\")\n\nC. Sample Table\nHere is what the sample table looks like in the mock data.\n\nhead(sample_info_16S)\n\n   SampleID Group Type          Saline\nS1       S1    IW   NE Non-saline soil\nS2       S2    IW   NE Non-saline soil\nS3       S3    IW   NE Non-saline soil\nS4       S4    IW   NE Non-saline soil\nS5       S5    IW   NE Non-saline soil\nS6       S6    IW   NE Non-saline soil\n\n\nNo problem.\n\nsamdf &lt;- read.table(\"../sampledata/files/tables/samdf.txt\",\n    header = TRUE, sep = \"\\t\")\n\nsamdf &lt;- samdf %&gt;% tibble::column_to_rownames(\"SampleID\")\nsamdf$SampleID &lt;- rownames(samdf)\nsamdf &lt;- samdf %&gt;% relocate(SampleID)\n\nsamdf &lt;- samdf %&gt;%\n  dplyr::filter(\n    stringr::str_starts(SampleID, \"Control\", negate = TRUE)\n    )",
    "crumbs": [
      "16S rRNA Processing",
      "4. MED Workflow"
    ]
  },
  {
    "objectID": "docs/ssu-workflows/med/index.html#create-a-microtable-object",
    "href": "docs/ssu-workflows/med/index.html#create-a-microtable-object",
    "title": "4. MED Workflow",
    "section": "Create a Microtable Object",
    "text": "Create a Microtable Object\nWith these three files in hand we are now ready to create a microtable object.\n\n\n\n\n\n\nNote\n\n\n\nA microtable object contains an OTU table (taxa abundances), sample metadata, and taxonomy table (mapping between OTUs and higher-level taxonomic classifications).\n\n\n\nsample_info &lt;- samdf\ntax_tab &lt;- tmp_tax\notu_tab &lt;- tmp_st\n\n\ntmp_me &lt;- microtable$new(sample_table = sample_info, \n                         otu_table = otu_tab, \n                         tax_table = tax_tab)\ntmp_me\n\nmicrotable-class object:\nsample_table have 1849 rows and 13 columns\notu_table have 119453 rows and 1849 columns\ntax_table have 119453 rows and 6 columns\nAdd Representative Sequence\nThe fasta file returned by MED needs a little T.L.C. For that we use a tool called SeqKit [Shen et al. (2016);shen2024seqkit2] for fasta defline manipulation.\n\nseqkit replace -p ^ -r MED node-representatives.fa.txt &gt; tmp1.fa\nseqkit replace -p \"\\|.*\" -r '' tmp1.fa &gt; tmp2.fa \nseqkit replace -p \"-\" -r '$1' -s -w 0 tmp2.fa &gt; med_nodes.fasta\nrm tmp1.fa\nrm tmp2.fa\n\n\nrep_fasta &lt;- Biostrings::readDNAStringSet(\"include/results/med_nodes.fasta\")\n\n\ntmp_me$rep_fasta &lt;- rep_fasta\ntmp_me$tidy_dataset()\ntmp_me",
    "crumbs": [
      "16S rRNA Processing",
      "4. MED Workflow"
    ]
  },
  {
    "objectID": "docs/ssu-workflows/med/index.html#curate-the-data-set",
    "href": "docs/ssu-workflows/med/index.html#curate-the-data-set",
    "title": "4. MED Workflow",
    "section": "Curate the Data Set",
    "text": "Curate the Data Set\nPretty much the last thing to do is remove low-count samples.\nRemove Low-Count Samples\n\ntmp_no_low &lt;- microeco::clone(me_med_raw)\ntmp_no_low$otu_table &lt;- me_med_raw$otu_table %&gt;%\n          dplyr::select(where(~ is.numeric(.) && sum(.) &gt;= 100))\ntmp_no_low$tidy_dataset()\ntmp_no_low\n\n\n\nmicrotable-class object:\nsample_table have 1848 rows and 13 columns\notu_table have 437 rows and 1848 columns\ntax_table have 437 rows and 6 columns\nrep_fasta have 437 sequences\n\n\n\nme_med &lt;- microeco::clone(tmp_no_low)",
    "crumbs": [
      "16S rRNA Processing",
      "4. MED Workflow"
    ]
  },
  {
    "objectID": "docs/ssu-workflows/med/index.html#summary",
    "href": "docs/ssu-workflows/med/index.html#summary",
    "title": "4. MED Workflow",
    "section": "Summary",
    "text": "Summary\nNow time to summarize the data. For this we use the R package miaverse (Ernst et al. 2024).\nFirst we do a little formatting to get our data compatible with mia.\n\n# https://github.com/microbiome/OMA/issues/202\ntmp_counts &lt;- as.matrix(me_med_raw$otu_table)\ntmp_assays &lt;-  SimpleList(counts = tmp_counts)\nmia_me_med_raw &lt;- TreeSummarizedExperiment(assays = tmp_assays,\n                                colData = DataFrame(me_med_raw$sample_table),\n                                rowData = DataFrame(me_med_raw$tax_table))\nrm(list = ls(pattern = \"tmp_\"))\ntmp_counts &lt;- as.matrix(me_med$otu_table)\ntmp_assays &lt;-  SimpleList(counts = tmp_counts)\nmia_me_med &lt;- TreeSummarizedExperiment(assays = tmp_assays,\n                                colData = DataFrame(me_med$sample_table),\n                                rowData = DataFrame(me_med$tax_table))\nmia_me_med_raw_summ &lt;- summary(mia_me_med_raw, assay.type = \"counts\")\nmia_me_med_summ &lt;- summary(mia_me_med, assay.type = \"counts\")\nrm(list = ls(pattern = \"tmp_\"))\nobjects()\n\n\n\n\n\n\n\n\nMetric\nStart\nEnd\n\n\n\nMin. number reads\n1\n102\n\n\nMax. number reads\n186556\n186556\n\n\nTotal number reads\n24851867\n24851866\n\n\nAvg number reads\n13441\n13448\n\n\nMedian number reads\n10363\n10370\n\n\nTotal MEDs\n437\n437\n\n\nNumber singleton MEDs\n0\n0\n\n\nAvg MEDs per sample.\n66\n66\n\n\n\nWe started off with 437 MEDs and 1849 samples. After removing low-count samples, there were 437 MEDs and 1848 samples remaining.\n\nhead(get_taxa_unique(ps_moth, \"Family\"), 16)",
    "crumbs": [
      "16S rRNA Processing",
      "4. MED Workflow"
    ]
  },
  {
    "objectID": "docs/ssu-workflows/med/index.html#footnotes",
    "href": "docs/ssu-workflows/med/index.html#footnotes",
    "title": "4. MED Workflow",
    "section": "Footnotes",
    "text": "Footnotes\n\nFrom the developers: GSR database (Greengenes, SILVA, and RDP database) is an integrated and manually curated database for bacterial and archaeal 16S amplicon taxonomy analysis. Unlike previous integration approaches, this database creation pipeline includes a taxonomy unification step to ensure consistency in taxonomical annotations. The database was validated with three mock communities and two real datasets and compared with existing 16S databases such as Greengenes, GTDB, ITGDB, SILVA, RDP, and MetaSquare. Results showed that the GSR database enhances taxonomical annotations of 16S sequences, outperforming current 16S databases at the species level. The GSR database is available for full-length 16S sequences and the most commonly used hypervariable regions: V4, V1-V3, V3-V4, and V3-V5.↩︎\nFrom the developers: GSR database (Greengenes, SILVA, and RDP database) is an integrated and manually curated database for bacterial and archaeal 16S amplicon taxonomy analysis. Unlike previous integration approaches, this database creation pipeline includes a taxonomy unification step to ensure consistency in taxonomical annotations. The database was validated with three mock communities and two real datasets and compared with existing 16S databases such as Greengenes, GTDB, ITGDB, SILVA, RDP, and MetaSquare. Results showed that the GSR database enhances taxonomical annotations of 16S sequences, outperforming current 16S databases at the species level. The GSR database is available for full-length 16S sequences and the most commonly used hypervariable regions: V4, V1-V3, V3-V4, and V3-V5.↩︎",
    "crumbs": [
      "16S rRNA Processing",
      "4. MED Workflow"
    ]
  }
]