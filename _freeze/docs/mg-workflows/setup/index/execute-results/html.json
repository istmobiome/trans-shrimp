{
  "hash": "aae86ef73bd140c9eadcd0d776d87fc1",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"1. Annotation Databases\"\ndescription: |\n  Constructing taxonomic & functional annotation databases. \nformat:\n  html:\n    mermaid:\n      theme: neutral\n---\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Click here for libraries and setup information.\"}\nknitr::opts_chunk$set(echo = TRUE, eval = FALSE)\nset.seed(919191)\nlibrary(ggplot2); packageVersion(\"ggplot2\")\nlibrary(Biostrings); packageVersion(\"Biostrings\")\npacman::p_load(tidyverse, gridExtra, grid,\n               formatR, reactable, gdata, patchwork,\n               kableExtra, magrittr, \n               install = FALSE, update = FALSE)\n\noptions(scipen = 999)\nknitr::opts_current$get(c(\n  \"cache\",\n  \"cache.path\",\n  \"cache.rebuild\",\n  \"dependson\",\n  \"autodep\"\n))\n#root <- find_root(has_file(\"_quarto.yml\"))\n#source(file.path(root, \"assets\", \"functions.R\"))\n```\n:::\n\n\n\n\n\nThere are two main types of annotations we are interested in for this metagenomic project---**taxonomic** and **functional**---and there are many, many ways to accomplish both of these goals. This next section involves building the databases and installing any additional tools we need for annotation.\n\nLet’s start with the tools and databases for **taxonomic** classification.\n\n# Taxonomic Classification\n\nThere are many algorithms and databases for taxonomic classification. We will use [Kraken2](https://github.com/DerrickWood/kraken2) to classify short reads. We will also use [Kaiju](https://github.com/bioinformatics-centre/kaiju) and [VirSorter2](https://github.com/jiarong/VirSorter2) for contigs. Anvi'o has methods of importing data from each of these approaches but if you have a have a favorite tool/database there are workarounds to get most results into the appropriate anvio database.\n\n## Kaiju\n\n*[Kaiju](https://github.com/bioinformatics-centre/kaiju) [@menzel2016kaiju]... finds maximum (in-)exact matches on the protein-level using the Burrows–Wheeler transform.* *Kaiju is a program for the taxonomic classification... of metagenomic DNA. Reads are directly assigned to taxa using the NCBI taxonomy and a reference database of protein sequences from microbial and viral genomes.*\n\nFirst we need to install Kaiju. Again, I will run kaiju in a separate conda environment. Now I can either install kaiju from the source code or as conda package. The latter is easier but often conda packages may lag behind the source code versions. I usually compare the release dates of the [conda package](https://anaconda.org/bioconda/kaiju) with the [source code](https://github.com/bioinformatics-centre/kaiju) and look at the number of downloads. In this case, the conda version of kaiju looks fine.\n\n\n\n\n::: {.cell}\n\n```{.zsh .cell-code}\n# create generic environment\nconda create -n kaiju\nconda activate kaiji\nconda install -c bioconda kaiju\n```\n:::\n\n\n\n\nAfter kaiju is installed, the next thing to do is generate the database. You can find a description of kaiju databases in the section on  [dreating the Kaiju index](https://github.com/bioinformatics-centre/kaiju?tab=readme-ov-file#creating-the-kaiju-index). I downloaded and formatted the `progenomes`  database, whic contains representative sets of genomes from the [proGenomes](http://progenomes.embl.de/) database and viruses from the NCBI RefSeq database.\n\nSimply run the `kaiju-makedb` command and specify a database.\n\n\n\n\n::: {.cell}\n\n```{.zsh .cell-code}\nsource activate kaiju\nmkdir kaiju\ncd kaiju\nkaiju-makedb -s progenomes\n```\n:::\n\n\n\n\nThe `progenomes` database is 95GB.\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n\n### Expand for the KAIJU_DB_BUILD Hydra script\n\n\n```\n# /bin/sh\n# ----------------Parameters---------------------- #\n#$ -S /bin/sh\n#$ -pe mthread 15\n#$ -q sThM.q\n#$ -l mres=225G,h_data=15G,h_vmem=15G,himem\n#$ -cwd\n#$ -j y\n#$ -N job_build_anvio_dbs\n#$ -o hydra_logs/job_build_kaiju.log\n#\n# ----------------Modules------------------------- #\nmodule load gcc/4.9.2\n#\n# ----------------Your Commands------------------- #\n#\necho + `date` job $JOB_NAME started in $QUEUE with jobID=$JOB_ID on $HOSTNAME\necho + NSLOTS = $NSLOTS\n#\n# ----------------CALLING ANVIO------------------- #\nexport PATH=/home/scottjj/miniconda3:$PATH\nexport PATH=/home/scottjj/miniconda3/bin:$PATH\n#\nsource activate kaiju\nmkdir kaiju\ncd kaiju\nkaiju-makedb -s progenomes\n```\n\n\n\n:::\n\n## Kraken2\n\n*[Kraken2](https://github.com/DerrickWood/kraken2) [@wood2019kraken2] is a novel metagenomics classifier that combines the fast k-mer-based classification of Kraken with an efficient algorithm for assessing the coverage of unique k-mers found in each species in a dataset.*\n\nInstalled in separate conda environment\n\n\n\n\n::: {.cell}\n\n```{.zsh .cell-code}\nconda create -n kraken2\nconda install kraken2\nconda activate kraken2\n```\n:::\n\n\n\n\nThe standard way of installing a Kraken2 database is to run `kraken2-build` and calling the `--standard` flag, which will construct a database containing Refeq archaea, bacteria, viral, plasmid, human data plus data from UniVec_Core. Here is the command. \n\n\n\n\n::: {.cell}\n\n```{.zsh .cell-code}\nkraken2-build --standard --db kraken2_db --threads $NSLOTS\n```\n:::\n\n\n\n\nThe first problem arises because of issues with the NCBI servers and `kraken2-build` use of `rsync`. So we add the flag `--use-ftp`. Like so...\n\n\n\n\n::: {.cell}\n\n```{.zsh .cell-code}\nkraken2-build --standard --db kraken2_db --threads $NSLOTS --use-ftp\n```\n:::\n\n\n\n\nBut this did not work either--it failed repeatedly. If you look at the Kraken2 GitHub [issues page](https://github.com/DerrickWood/kraken2/issues) you will see that this is a common issue.  So instead we tried using one of the [prebuilt database](https://benlangmead.github.io/aws-indexes/k2), however none of these contained files we needed for downstream analysis. Strike 2. So we tried a different approach. \n\nFirst, we followed [this suggestion](https://github.com/DerrickWood/kraken2/issues/515#issuecomment-949354093) to change line 16 of the file `PATH_to_KRAKEN2/download_taxonomy.sh` from this...\n\n\n\n\n::: {.cell}\n\n```{.zsh .cell-code}\nFTP_SERVER=\"ftp://$NCBI_SERVER\"\n```\n:::\n\n\n\n\n...to this\n\n\n\n\n::: {.cell}\n\n```{.zsh .cell-code}\nTO: FTP_SERVER=\"https://$NCBI_SERVER\"\n```\n:::\n\n\n\n\nSupposedly this is to help with the timeout issues from the NCBI servers. Next, we tried building our own database. The first step was to download the NCBI taxonomy. \n\n\n\n\n::: {.cell}\n\n```{.zsh .cell-code}\nkraken2-build --download-taxonomy --db kraken2_db --use-ftp\n```\n:::\n\n\n\n\nOnce this step was complete, we downloaded individual libraries in order to build a custom Kraken2 database. \n\n\n\n\n::: {.cell}\n\n```{.zsh .cell-code}\nkraken2-build --download-library archaea --db kraken2_db --threads $NSLOTS --use-ftp \nkraken2-build --download-library bacteria --db kraken2_db --threads $NSLOTS --use-ftp \nkraken2-build --download-library plasmid --db kraken2_db --threads $NSLOTS --use-ftp \nkraken2-build --download-library viral --db kraken2_db --threads $NSLOTS --use-ftp \nkraken2-build --download-library fungi --db kraken2_db --threads $NSLOTS --use-ftp \nkraken2-build --download-library protozoa --db kraken2_db --threads $NSLOTS --use-ftp \nkraken2-build --download-library UniVec_Core --db kraken2_db --threads $NSLOTS --use-ftp \n```\n:::\n\n\n\n\nAnd finally constructed the database. \n\n\n\n\n::: {.cell}\n\n```{.zsh .cell-code}\nkraken2-build --build --db kraken2_db\n```\n:::\n\n\n\n\nAnd it worked. The whole process took about 8 hours and the final DB is ~90GB. \n\n::: {.callout-note appearance=\"simple\" collapse=\"true\" .copy}\n\n### Expand for the KRAKEN2_DB_BUILD Hydra script\n\n\n```\n# /bin/sh\n# ----------------Parameters---------------------- #\n#$ -S /bin/sh\n#$ -pe mthread 30\n#$ -q mThM.q\n#$ -l mres=900G,h_data=30G,h_vmem=30G,himem\n#$ -cwd\n#$ -j y\n#$ -N job_00_kraken2-build\n#$ -o job_00_kraken2-build4.job\n#\n# ----------------Modules------------------------- #\nmodule load bioinformatics/blast\n#\n# ----------------Load Envs------------------- #\n#\necho + `date` job $JOB_NAME started in $QUEUE with jobID=$JOB_ID on $HOSTNAME\necho + NSLOTS = $NSLOTS\n#\nexport PATH=/home/scottjj/miniconda3/bin:$PATH\nexport PATH=/home/scottjj/miniconda3/envs/kraken2/bin:$PATH\n\n######################################################################\n# Changed line 16 of this file:\n# FROM: FTP_SERVER=\"ftp://$NCBI_SERVER\"\n# TO: FTP_SERVER=\"https://$NCBI_SERVER\"\n# /home/scottjj/miniconda3/envs/kraken2/share/kraken2-2.1.3-1/libexec/download_taxonomy.sh\n# per https://github.com/DerrickWood/kraken2/issues/515#issuecomment-949354093\n######################################################################\n\nsource activate kraken2\n\n######################################################################\n## STANDARD BUILD-FAILED\n######################################################################\n###kraken2-build --standard --db kraken2_db --threads $NSLOTS --use-ftp\n\n######################################################################\n## CUSTOM BUILD\n######################################################################\n\n## TAXONOMY\nkraken2-build --download-taxonomy --db kraken2_db --use-ftp\n\n## DATABASES\nkraken2-build --download-library archaea --db kraken2_db --threads $NSLOTS --use-ftp \nkraken2-build --download-library bacteria --db kraken2_db --threads $NSLOTS --use-ftp \nkraken2-build --download-library plasmid --db kraken2_db --threads $NSLOTS --use-ftp \nkraken2-build --download-library viral --db kraken2_db --threads $NSLOTS --use-ftp \nkraken2-build --download-library fungi --db kraken2_db --threads $NSLOTS --use-ftp \nkraken2-build --download-library protozoa --db kraken2_db --threads $NSLOTS --use-ftp \nkraken2-build --download-library UniVec_Core --db kraken2_db --threads $NSLOTS --use-ftp \n\n## BUILD\nkraken2-build --build --db kraken2_db\n\nconda deactivate\n\necho = `date` job $JOB_NAME done\n```\n\n\n\n:::\n\n## VirSorter2\n\n*[VirSorter](https://github.com/jiarong/VirSorter2) [@guo2021virsorter2], applies a multi-classifier, expert-guided approach to detect diverse DNA and RNA virus genomes. It has made major updates to its previous version.*\n\nI followed [this recipe](https://github.com/jiarong/VirSorter2?tab=readme-ov-file#option-1-bioconda-version) for installing VirSorter2. Piece of cake.\n\n\n\n\n::: {.cell}\n\n```{.zsh .cell-code}\nmamba create -n virsorter2 -c conda-forge -c bioconda virsorter=2\nmamba activate virsorter2\n```\n:::\n\n\n\n\nGood to go? Now time to build the VirSorter2 database. Pretty straightforward actually. The general instructions can be found [here](https://github.com/jiarong/VirSorter2?tab=readme-ov-file#download-database-and-dependencies). \n\n\n\n\n::: {.cell}\n\n```{.zsh .cell-code}\nsource activate virsorter2\nvirsorter setup --db-dir /pool/genomics/stri_istmobiome/dbs/virsorter2/ -j $NSLOTS\n```\n:::\n\n\n\n\nAnd then a quick test to make sure the database is ok.\n\n\n\n\n::: {.cell}\n\n```{.zsh .cell-code}\nmkdir TEST_virsorter2\ncd TEST_virsorter2\nwget -O test.fa https://raw.githubusercontent.com/jiarong/VirSorter2/master/test/8seq.fa\nvirsorter run -w test.out -i test.fa -j $NSLOTS --db-dir dbs/virsorter2/ --tmpdir TEST_virsorter2/tmp_vir --rm-tmpdir --min-length 1500 all\n```\n:::\n\n\n\n\nThe uncompressed database is a little over 10GB.\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n\n### Expand for the VIRSORTER2_DB_BUILD Hydra script\n\n\n```\n# /bin/sh\n# ----------------Parameters---------------------- #\n#$ -S /bin/sh\n#$ -pe mthread 15\n#$ -q sThM.q\n#$ -l mres=225G,h_data=15G,h_vmem=15G,himem\n#$ -cwd\n#$ -j y\n#$ -N job_build_anvio_dbs\n#$ -o hydra_logs/job_build_virsorter2.log\n#\n# ----------------Modules------------------------- #\nmodule load gcc/4.9.2\n#\n# ----------------Your Commands------------------- #\n#\necho + `date` job $JOB_NAME started in $QUEUE with jobID=$JOB_ID on $HOSTNAME\necho + NSLOTS = $NSLOTS\n#\n# ----------------CALLING ANVIO------------------- #\nexport PATH=/home/scottjj/miniconda3:$PATH\nexport PATH=/home/scottjj/miniconda3/bin:$PATH\n#\n\nsource activate virsorter2\nvirsorter setup --db-dir /pool/genomics/stri_istmobiome/dbs/virsorter2/ -j $NSLOTS\n\n############### TEST VIRSORTER 2 ###############\n\nmkdir TEST_virsorter2\ncd TEST_virsorter2\nwget -O test.fa https://raw.githubusercontent.com/jiarong/VirSorter2/master/test/8seq.fa\nvirsorter run -w test.out -i test.fa -j $NSLOTS --db-dir /pool/genomics/stri_istmobiome/dbs/virsorter2/ --tmpdir TEST_virsorter2/tmp_vir --rm-tmpdir --min-length 1500 all\n\nconda deactivate\n\necho = `date` job $JOB_NAME done\n```\n\n\n\n:::\n\n## Single Copy Gene (SCG) taxonomy\n\nAnvi'o has native support for building tRNA and Single Copy Gene (SCG) taxonomy from the [Genome Taxonomy Database (GTDB)](https://gtdb.ecogenomic.org/). \n\n\n\n\n::: {.cell}\n\n```{.zsh .cell-code}\nanvi-setup-scg-taxonomy -T $NSLOTS\nanvi-setup-trna-taxonomy -T $NSLOTS\n```\n:::\n\n\n\n\n# Functional Annotations\n\nNow we can install databases for **functional** annotation. Anvi'o has native support for building [Pfam](https://pfam.xfam.org/), [COG](https://www.ncbi.nlm.nih.gov/COG/),  [KEGG](https://www.genome.jp/tools/kofamkoala/), and [CAZymes](http://www.cazy.org/) databases.\n\nAnvi'o makes this super simple.\n\n\n\n\n::: {.cell}\n\n```{.zsh .cell-code}\nanvi-setup-pfams --pfam-data-dir dbs/pfam_db\nanvi-setup-kegg-data --mode all --kegg-data-dir dbs/kegg_kofam  -T $NSLOTS\nanvi-setup-ncbi-cogs --cog-data-dir dbs/cog_db -T $NSLOTS\nanvi-setup-cazymes --cazyme-data-dir dbs/cazymes\n```\n:::\n\n\n\n\nAnd that’s it. We can add more databases as we need them.\n\n# Concluding remarks\n\nAt this point we should be good to go with the main setup. We may need to install other tools along the way---we can add those instructions here. Or you may decide to use other tools instead. For example, we are using [megahit](https://github.com/voutcn/megahit) for the assemblies but you may want to use [metaspades](https://github.com/ablab/spades) or [idba_ud](https://github.com/loneknightpy/idba). These packages need to be installed separately.\n\n#### Source Code {.appendix}\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\nThe source code for this page can be accessed on GitHub `<svg aria-hidden=\"true\" role=\"img\" viewBox=\"0 0 496 512\" style=\"height:1em;width:0.97em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;\"><path d=\"M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z\"/></svg>`{=html} by [clicking this \nlink](https://github.com/istmobiome/trans-shrimp/blob/main/docs/mg-workflows/setup/index.qmd). \n\n\n\n\n#### Last updated on {.appendix}\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"2024-09-29 11:01:53 PDT\"\n```\n\n\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}