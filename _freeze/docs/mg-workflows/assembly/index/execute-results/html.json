{
  "hash": "5ce9d922e768e72edeb9aa142c65fda8",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"2. Co-Assembly & Annotations\"\ndescription: |\n  This section describes the steps we took to co-assemble the metagenomic samples, classify taxonomy, and assign functions.\nformat:\n  html:\n    mermaid:\n      theme: forest\nlightbox: true\n---\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Click here for libraries and setup information.\"}\nknitr::opts_chunk$set(echo = TRUE, eval = FALSE)\nset.seed(919191)\nlibrary(ggplot2); packageVersion(\"ggplot2\")\nlibrary(Biostrings); packageVersion(\"Biostrings\")\npacman::p_load(tidyverse, gridExtra, grid,\n               formatR, reactable, gdata, patchwork,\n               kableExtra, magrittr, \n               install = FALSE, update = FALSE)\n\noptions(scipen = 999)\nknitr::opts_current$get(c(\n  \"cache\",\n  \"cache.path\",\n  \"cache.rebuild\",\n  \"dependson\",\n  \"autodep\"\n))\n#root <- find_root(has_file(\"_quarto.yml\"))\n#source(file.path(root, \"assets\", \"functions.R\"))\n```\n:::\n\n\n\n\n\n# Data Availability\n\nPENDING\n\n# The Pipeline\n\nIn this section of the workflow we begin with raw, paired-end Illumina data. We will use a Snakemake [@koster2012snakemake] workflow within the anvi'o [@eren2015anvi; @eren2021community] environment for most of the steps, though we will at times call on additional tools for specific steps. The overall structure of our workflow was modeled after the one described by Delmont & Eren on [Recovering Microbial Genomes from TARA Oceans Metagenomes](http://merenlab.org/data/tara-oceans-mags/) [@delmont2018nitrogen].\n\nThe main steps of this workflow are:\n\n::: column-body-outset\n| Task                  | Tool/Description                                      | Input                  |\n|------------------|-------------------------------------|------------------|\n| QUALITY-FILTERING     | IU filter quality Minoche to QC trimmed reads.        | trimmed reads          |\n| CO-ASSEMBLY           | MEGAHIT to co-assemble metagenomic samples.           | QC reads               |\n| GENE CALLING          | PRODIGAL for gene calling. Contig db for results.     | assembled contigs      |\n| TAXONOMIC ANNOTATION  | KrakenUniq for short read classification.             | QC reads               |\n| RECRUITMENT           | BOWTIE2/SAMtools for mapping short reads to assembly. | QC reads, contigs db   |\n| PROFILING             | Profile mapping results. Store in profile dbs.        | BAM file               |\n| TAXONOMIC ANNOTATION  | KAIJU for classification of contigs.                  | contigs (dna format)     |\n| FUNCTIONAL ANNOTATION | Gene annotations against Pfams, COGS, KEGG, etc.      | genes (aa format)      |\n| MERGING               | Create single merged profile db.                      | individual profile dbs |\n:::\n\n\n\n\n```{mermaid}\nflowchart LR\n  A(QC <br/>metagenomic <br/>reads) --> B(\"Classify <br/>short reads <br/> (Kraken2)\")\n  A --> C(\"Co-assemble <br/> EP & WA<br/> (e.g. Megahit)\")\n  C --> D(Map Reads<br/>Bowtie & <br/>Samtools)\n  C --> E(fa:fa-database Contigs DB <br/> EP & WA)\n  D --> F(fa:fa-database Profile DBs<br/> all samples)\n  E --> G(\"Gene Annotations <br/> (e.g. HMMS, PFam, <br/>COG, etc.)\")\n  E --> H(\"Classify contigs <br/> (Kaiju)\")\n  F --> I(fa:fa-database Merge  <br/> Profile DBs <br/> EP & WA)\n  G --> J(fa:fa-database Annotated <br/>Contigs DBs <br/> EP & WA)\n  H --> J\n  I --> K\n  J --> K(\"Analysis <br/> (e.g., binning,<br/>MAGS<br/>,Phylogenomics,<br/>etc.)\")\n```\n\n\n\n\n\n\nWe have a total of 58 samples ...DETAILS\n\n# Snakemake Workflow\n\nThanks to the anvi'o implementation of Snakemake we can run many commands sequentially and/or simultaneously. There is plenty of good documentation on the anvi'o website about setting up Snakemake workflows [here](http://merenlab.org/2018/07/09/anvio-snakemake-workflows/) so we will refrain from any lengthy explanations.\n\nYou can see all the settings we used and download our configs file [here](include/default_mg.json) or grab it from the folded code below. Some commands we chose not to run but they remain in the workflow for posterity.\n\n\n\n\n::: {.cell}\n\n```{.zsh .cell-code}\nanvi-run-workflow --list-workflows\n```\n:::\n\n\n\n\n```         \nWARNING\n===============================================\nIf you publish results from this workflow, please do not forget to cite\nsnakemake (doi:10.1093/bioinformatics/bts480)\n\nHMM profiles .................................: 9 sources have been loaded: \nArchaea_76 (76 genes, domain: archaea), \nBacteria_71 (71 genes, domain: bacteria), \nProtista_83 (83 genes, domain: eukarya), \nRibosomal_RNA_12S (1 genes, domain: None), \nRibosomal_RNA_16S (3 genes, domain: None), \nRibosomal_RNA_18S (1 genes, domain: None), \nRibosomal_RNA_23S (2 genes, domain: None), \nRibosomal_RNA_28S (1 genes, domain: None),\nRibosomal_RNA_5S (5 genes, domain: None)\nAvailable workflows ..........................: contigs, metagenomics, pangenomics, phylogenomics, trnaseq, ecophylo, sra_download\n```\n\n\n\n\n::: {.cell}\n\n```{.zsh .cell-code}\nanvi-run-workflow --workflow metagenomics \\\n                  --get-default-config default_mg.json  \\\n                  --list-dependencies\n```\n:::\n\n\n\n\nYou can find a description of the metagenomic workflow [here](https://anvio.org/help/main/workflows/metagenomics/).\n\n\n\n\n::: {.cell}\n\n```{.bash .cell-code  code-fold=\"true\" code-summary=\"Show JSON-formatted configuration file.\"}\n{\n    \"fasta_txt\": \"\",\n    \"anvi_gen_contigs_database\": {\n        \"--project-name\": \"{group}\",\n        \"--description\": \"\",\n        \"--skip-gene-calling\": \"\",\n        \"--ignore-internal-stop-codons\": \"\",\n        \"--skip-mindful-splitting\": \"\",\n        \"--contigs-fasta\": \"\",\n        \"--split-length\": \"\",\n        \"--kmer-size\": \"\",\n        \"--skip-predict-frame\": \"\",\n        \"--prodigal-translation-table\": \"\",\n        \"threads\": 14\n    },\n    \"centrifuge\": {\n        \"threads\": 14,\n        \"run\": true,\n        \"db\": \"/pool/genomics/stri_istmobiome/dbs/centrifuge_dbs/p+h+v\"\n    },\n    \"anvi_run_hmms\": {\n        \"run\": true,\n        \"threads\": 14,\n        \"--also-scan-trnas\": true,\n        \"--installed-hmm-profile\": \"\",\n        \"--hmm-profile-dir\": \"\",\n        \"--add-to-functions-table\": \"\"\n    },\n    \"anvi_run_kegg_kofams\": {\n        \"run\": true,\n        \"threads\": 14,\n        \"--kegg-data-dir\": \"/pool/genomics/stri_istmobiome/dbs/kegg_kofam/\",\n        \"--hmmer-program\": \"\",\n        \"--keep-all-hits\": \"\",\n        \"--log-bitscores\": \"\",\n        \"--just-do-it\": \"\"\n    },\n    \"anvi_run_ncbi_cogs\": {\n        \"run\": true,\n        \"threads\": 14,\n        \"--cog-data-dir\": \"/pool/genomics/stri_istmobiome/dbs/cog_db/\",\n        \"--temporary-dir-path\": \"/pool/genomics/stri_istmobiome/dbs/cog_db/tmp_mega/\",\n        \"--search-with\": \"\"\n    },\n    \"anvi_run_scg_taxonomy\": {\n        \"run\": true,\n        \"threads\": 14,\n        \"--scgs-taxonomy-data-dir\": \"\"\n    },\n    \"anvi_run_trna_scan\": {\n        \"run\": true,\n        \"threads\": 14,\n        \"--trna-cutoff-score\": \"\"\n    },\n    \"anvi_script_reformat_fasta\": {\n        \"run\": true,\n        \"--prefix\": \"{group}\",\n        \"--simplify-names\": true,\n        \"--keep-ids\": \"\",\n        \"--exclude-ids\": \"\",\n        \"--min-len\": \"1000\",\n        \"--seq-type\": \"\",\n        \"threads\": 7\n    },\n    \"emapper\": {\n        \"--database\": \"bact\",\n        \"--usemem\": true,\n        \"--override\": true,\n        \"path_to_emapper_dir\": \"\",\n        \"threads\": \"\"\n    },\n    \"anvi_script_run_eggnog_mapper\": {\n        \"--use-version\": \"0.12.6\",\n        \"run\": \"\",\n        \"--cog-data-dir\": \"\",\n        \"--drop-previous-annotations\": \"\",\n        \"threads\": \"\"\n    },\n    \"samples_txt\": \"samples.txt\",\n    \"metaspades\": {\n        \"additional_params\": \"--only-assembler\",\n        \"threads\": 7,\n        \"run\": \"\",\n        \"use_scaffolds\": \"\"\n    },\n    \"megahit\": {\n        \"--min-contig-len\": 1000,\n        \"--memory\": 0.9,\n        \"threads\": 14,\n        \"run\": true,\n        \"--min-count\": \"\",\n        \"--k-min\": \"\",\n        \"--k-max\": \"\",\n        \"--k-step\": \"\",\n        \"--k-list\": \"\",\n        \"--no-mercy\": \"\",\n        \"--no-bubble\": \"\",\n        \"--merge-level\": \"\",\n        \"--prune-level\": \"\",\n        \"--prune-depth\": \"\",\n        \"--low-local-ratio\": \"\",\n        \"--max-tip-len\": \"\",\n        \"--no-local\": \"\",\n        \"--kmin-1pass\": \"\",\n        \"--presets\": \"meta-sensitive\",\n        \"--mem-flag\": \"\",\n        \"--use-gpu\": \"\",\n        \"--gpu-mem\": \"\",\n        \"--keep-tmp-files\": \"\",\n        \"--tmp-dir\": \"\",\n        \"--continue\": true,\n        \"--verbose\": \"\"\n    },\n    \"idba_ud\": {\n        \"--min_contig\": 1000,\n        \"threads\": 7,\n        \"run\": \"\",\n        \"--mink\": \"\",\n        \"--maxk\": \"\",\n        \"--step\": \"\",\n        \"--inner_mink\": \"\",\n        \"--inner_step\": \"\",\n        \"--prefix\": \"\",\n        \"--min_count\": \"\",\n        \"--min_support\": \"\",\n        \"--seed_kmer\": \"\",\n        \"--similar\": \"\",\n        \"--max_mismatch\": \"\",\n        \"--min_pairs\": \"\",\n        \"--no_bubble\": \"\",\n        \"--no_local\": \"\",\n        \"--no_coverage\": \"\",\n        \"--no_correct\": \"\",\n        \"--pre_correction\": \"\",\n        \"use_scaffolds\": \"\"\n    },\n    \"iu_filter_quality_minoche\": {\n        \"run\": true,\n        \"--ignore-deflines\": true,\n        \"--visualize-quality-curves\": \"\",\n        \"--limit-num-pairs\": \"\",\n        \"--print-qual-scores\": \"\",\n        \"--store-read-fate\": \"\",\n        \"threads\": 7\n    },\n    \"gzip_fastqs\": {\n        \"run\": true,\n        \"threads\": 7\n    },\n    \"bowtie\": {\n        \"additional_params\": \"--no-unal\",\n        \"threads\": 7\n    },\n    \"samtools_view\": {\n        \"additional_params\": \"-F 4\",\n        \"threads\": 7\n    },\n    \"anvi_profile\": {\n        \"threads\": 7,\n        \"--sample-name\": \"{sample}\",\n        \"--overwrite-output-destinations\": true,\n        \"--report-variability-full\": \"\",\n        \"--skip-SNV-profiling\": \"\",\n        \"--profile-SCVs\": true,\n        \"--description\": \"\",\n        \"--skip-hierarchical-clustering\": \"\",\n        \"--distance\": \"\",\n        \"--linkage\": \"\",\n        \"--min-contig-length\": \"\",\n        \"--min-mean-coverage\": \"\",\n        \"--min-coverage-for-variability\": \"\",\n        \"--cluster-contigs\": \"\",\n        \"--contigs-of-interest\": \"\",\n        \"--queue-size\": \"\",\n        \"--write-buffer-size-per-thread\": \"\",\n        \"--fetch-filter\": \"\",\n        \"--min-percent-identity\": \"\",\n        \"--max-contig-length\": \"\"\n    },\n    \"anvi_merge\": {\n        \"--sample-name\": \"{group}\",\n        \"--overwrite-output-destinations\": true,\n        \"--description\": \"\",\n        \"--skip-hierarchical-clustering\": \"\",\n        \"--enforce-hierarchical-clustering\": \"\",\n        \"--distance\": \"\",\n        \"--linkage\": \"\",\n        \"threads\": 14\n    },\n    \"import_percent_of_reads_mapped\": {\n        \"run\": true,\n        \"threads\": 7\n    },\n    \"krakenuniq\": {\n        \"threads\": 3,\n        \"--gzip-compressed\": true,\n        \"additional_params\": \"\",\n        \"run\": \"\",\n        \"--db\": \"\"\n    },\n    \"remove_short_reads_based_on_references\": {\n        \"delimiter-for-iu-remove-ids-from-fastq\": \" \",\n        \"dont_remove_just_map\": \"\",\n        \"references_for_removal_txt\": \"\",\n        \"threads\": \"\"\n    },\n    \"anvi_cluster_contigs\": {\n        \"--collection-name\": \"{driver}\",\n        \"run\": \"\",\n        \"--driver\": \"\",\n        \"--just-do-it\": \"\",\n        \"--additional-params-concoct\": \"\",\n        \"--additional-params-metabat2\": \"\",\n        \"--additional-params-maxbin2\": \"\",\n        \"--additional-params-dastool\": \"\",\n        \"--additional-params-binsanity\": \"\",\n        \"threads\": \"\"\n    },\n    \"gen_external_genome_file\": {\n        \"threads\": \"\"\n    },\n    \"export_gene_calls_for_centrifuge\": {\n        \"threads\": \"\"\n    },\n    \"anvi_import_taxonomy_for_genes\": {\n        \"threads\": \"\"\n    },\n    \"annotate_contigs_database\": {\n        \"threads\": \"\"\n    },\n    \"anvi_get_sequences_for_gene_calls\": {\n        \"threads\": \"\"\n    },\n    \"gunzip_fasta\": {\n        \"threads\": \"\"\n    },\n    \"reformat_external_gene_calls_table\": {\n        \"threads\": \"\"\n    },\n    \"reformat_external_functions\": {\n        \"threads\": \"\"\n    },\n    \"import_external_functions\": {\n        \"threads\": \"\"\n    },\n    \"anvi_run_pfams\": {\n        \"run\": true,\n        \"--pfam-data-dir\": \"/pool/genomics/stri_istmobiome/dbs/pfam_db\",\n        \"threads\": 14\n    },\n    \"iu_gen_configs\": {\n        \"--r1-prefix\": \"\",\n        \"--r2-prefix\": \"\",\n        \"threads\": 14\n    },\n    \"gen_qc_report\": {\n        \"threads\": \"\"\n    },\n    \"merge_fastqs_for_co_assembly\": {\n        \"threads\": \"\"\n    },\n    \"merge_fastas_for_co_assembly\": {\n        \"threads\": \"\"\n    },\n    \"bowtie_build\": {\n        \"additional_params\": \"\",\n        \"threads\": \"\"\n    },\n    \"anvi_init_bam\": {\n        \"threads\": \"\"\n    },\n    \"krakenuniq_mpa_report\": {\n        \"threads\": \"\"\n    },\n    \"import_krakenuniq_taxonomy\": {\n        \"--min-abundance\": \"\",\n        \"threads\": \"\"\n    },\n    \"anvi_summarize\": {\n        \"additional_params\": \"\",\n        \"run\": \"\",\n        \"threads\": \"\"\n    },\n    \"anvi_split\": {\n        \"additional_params\": \"\",\n        \"run\": \"\",\n        \"threads\": \"\"\n    },\n    \"references_mode\": \"\",\n    \"all_against_all\": \"\",\n    \"kraken_txt\": \"\",\n    \"collections_txt\": \"\",\n    \"output_dirs\": {\n        \"FASTA_DIR\": \"02_FASTA\",\n        \"CONTIGS_DIR\": \"03_CONTIGS\",\n        \"QC_DIR\": \"01_QC\",\n        \"MAPPING_DIR\": \"04_MAPPING\",\n        \"PROFILE_DIR\": \"05_ANVIO_PROFILE\",\n        \"MERGE_DIR\": \"06_MERGED\",\n        \"TAXONOMY_DIR\": \"07_TAXONOMY\",\n        \"SUMMARY_DIR\": \"08_SUMMARY\",\n        \"SPLIT_PROFILES_DIR\": \"09_SPLIT_PROFILES\",\n        \"LOGS_DIR\": \"00_LOGS\"\n    },\n    \"max_threads\": \"\",\n    \"config_version\": \"3\",\n    \"workflow_name\": \"metagenomics\"\n}\n\n```\n:::\n\n\n\n\nSince we are doing a co-assembly, we need a separate file called `samples.txt`, which can be downloaded [here](include/samples.txt). This file tells anvi'o where to find the **raw** fastq files for each sample and what group the sample belongs to. The file is a four-column, tab-delimited file.\n\n| sample              | group | r1            | r2            |\n|---------------------|-------|---------------|---------------|\n| EP_ALPH_AREN_GL_P01 | EP    | forward reads | reverse reads |\n| EP_ALPH_AREN_HP_P01 | EP    | forward reads | reverse reads |\n| WA_ALPH_BOUV_GL_P02 | WA    | forward reads | reverse reads |\n| WA_ALPH_BOUV_HP_P02 | WA    | forward reads | reverse reads |\n\nRemember, there are 4 fastq files *per* direction (forward or reverse) *per* sample. So each comma separated list in the `r1` column needs four file names and the same with the `r2` column. And you must include *relative path names*.\n\nTo see what will happen with the `config` file we can visualize the Snakemake workflow using a Directed acyclic graph (DAG) where edge connections represent dependencies and nodes represent commands. The workflow begins with trimmed reads and continues up to and including automatic binning of contigs. At the end of this workflow we then proceed with manual binning and MAG generation. We added nodes for VirSorter and Kaiju annotations since these are not part of the normal anvi'o workflow.\n\n\n\n\n::: {.cell}\n\n```{.zsh .cell-code}\nanvi-run-workflow --workflow metagenomics \\\n                  --save-workflow-graph \\\n                  -c default_mg_for_dag.json\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show DAG R Code script.\"}\nlibrary(DiagrammeR)\nlibrary(DiagrammeRsvg)\nlibrary(rsvg)\ndag <- grViz (\"\ndigraph boxes_and_circles {\n  graph [layout = dot, align=center]\n\n  node [shape = rectangle, style = 'rounded,filled' fontname=sans, fontsize=12, penwidth=4]\n  edge[penwidth=4, color=grey];\n\n0[label = 'metagenomics_workflow_target_rule', color = 'grey'];\n1[label = 'anvi_merge', color = '#CC79A7'];\n2[label = 'anvi_merge', color = '#CC79A7'];\n3[label = 'annotate_contigs_database', color = '#E69F00'];\n4[label = 'annotate_contigs_database', color = '#E69F00'];\n5[label = 'gen_qc_report', color = '#56B4E9'];\n6[label = 'anvi_cluster_contigs\\ndriver: concoct', color = '#F0E442'];\n7[label = 'anvi_cluster_contigs\\ndriver: concoct', color = '#F0E442'];\n8[label = 'anvi_gen_contigs_database', color = '#CC79A7'];\n9[label = 'anvi_profile', color = '#CC79A7'];\n10[label = 'anvi_profile', color = '#CC79A7'];\n11[label = 'import_percent_of_reads_mapped', color = '#009E73'];\n12[label = 'import_percent_of_reads_mapped', color = '#009E73'];\n13[label = 'import_krakenuniq_taxonomy', color = '#E69F00'];\n14[label = 'import_krakenuniq_taxonomy', color = '#E69F00'];\n15[label = 'anvi_gen_contigs_database', color = '#CC79A7'];\n16[label = 'anvi_profile', color = '#CC79A7'];\n17[label = 'anvi_profile', color = '#CC79A7'];\n18[label = 'import_percent_of_reads_mapped', color = '#009E73'];\n19[label = 'import_percent_of_reads_mapped', color = '#009E73'];\n20[label = 'import_krakenuniq_taxonomy', color = '#E69F00'];\n21[label = 'import_krakenuniq_taxonomy', color = '#E69F00'];\n22[label = 'anvi_import_taxonomy_for_genes', color = '#E69F00'];\n23[label = 'anvi_run_hmms', color = '#E69F00'];\n24[label = 'anvi_run_ncbi_cogs', color = '#E69F00'];\n25[label = 'anvi_run_scg_taxonomy', color = '#E69F00'];\n26[label = 'anvi_run_pfams', color = '#E69F00'];\n27[label = 'anvi_import_taxonomy_for_genes', color = '#E69F00'];\n28[label = 'anvi_run_hmms', color = '#E69F00'];\n29[label = 'anvi_run_ncbi_cogs', color = '#E69F00'];\n30[label = 'anvi_run_scg_taxonomy', color = '#E69F00'];\n31[label = 'anvi_run_pfams', color = '#E69F00'];\n32[label = 'iu_filter_quality_minoche\\nsample: EP_ALPH_PANA_HP_P07', color = '#56B4E9'];\n33[label = 'iu_filter_quality_minoche\\nsample: EP_ALPH_PANA_MG_P07', color = '#56B4E9'];\n34[label = 'iu_filter_quality_minoche\\nsample: WA_ALPH_FORM_HP_P07', color = '#56B4E9'];\n35[label = 'iu_filter_quality_minoche\\nsample: WA_ALPH_FORM_MG_P07', color = '#56B4E9'];\n36[label = 'anvi_script_reformat_fasta', color = '#56B4E9'];\n37[label = 'anvi_init_bam', color = '#009E73'];\n38[label = 'anvi_init_bam', color = '#009E73'];\n39[label = 'krakenuniq_mpa_report', color = '#E69F00'];\n40[label = 'krakenuniq_mpa_report', color = '#E69F00'];\n41[label = 'anvi_script_reformat_fasta', color = '#56B4E9'];\n42[label = 'anvi_init_bam', color = '#009E73'];\n43[label = 'anvi_init_bam', color = '#009E73'];\n44[label = 'krakenuniq_mpa_report', color = '#E69F00'];\n45[label = 'krakenuniq_mpa_report', color = '#E69F00'];\n46[label = 'centrifuge', color = '#E69F00'];\n47[label = 'centrifuge', color = '#E69F00'];\n48[label = 'iu_gen_configs', color = '#56B4E9'];\n49[label = 'anvi_script_reformat_fasta_prefix_only', color = '#56B4E9'];\n50[label = 'samtools_view', color = '#009E73'];\n51[label = 'samtools_view', color = '#009E73'];\n52[label = 'krakenuniq', color = '#E69F00'];\n53[label = 'krakenuniq', color = '#E69F00'];\n54[label = 'anvi_script_reformat_fasta_prefix_only', color = '#56B4E9'];\n55[label = 'samtools_view', color = '#009E73'];\n56[label = 'samtools_view', color = '#009E73'];\n57[label = 'krakenuniq', color = '#E69F00'];\n58[label = 'krakenuniq', color = '#E69F00'];\n59[label = 'export_gene_calls_for_centrifuge', color = '#E69F00'];\n60[label = 'export_gene_calls_for_centrifuge', color = '#E69F00'];\n61[label = 'megahit\\ngroup: EP', color = '#56B4E9'];\n62[label = 'bowtie', color = '#009E73'];\n63[label = 'bowtie', color = '#009E73'];\n64[label = 'gzip_fastqs\\nR: R1', color = '#56B4E9'];\n65[label = 'gzip_fastqs\\nR: R2', color = '#56B4E9'];\n66[label = 'gzip_fastqs\\nR: R1', color = '#56B4E9'];\n67[label = 'gzip_fastqs\\nR: R2', color = '#56B4E9'];\n68[label = 'megahit\\ngroup: WA', color = '#56B4E9'];\n69[label = 'bowtie', color = '#009E73'];\n70[label = 'bowtie', color = '#009E73'];\n71[label = 'gzip_fastqs\\nR: R1', color = '#56B4E9'];\n72[label = 'gzip_fastqs\\nR: R2', color = '#56B4E9'];\n73[label = 'gzip_fastqs\\nR: R1', color = '#56B4E9'];\n74[label = 'gzip_fastqs\\nR: R2', color = '#56B4E9'];\n75[label = 'bowtie_build', color = '#009E73'];\n76[label = 'bowtie_build', color = '#009E73'];\n77[label = 'virsorter2', color = '#E69F00', style = 'dashed'];\n78[label = 'virsorter2', color = '#E69F00', style = 'dashed'];\n79[label = 'kaiju', color = '#E69F00', style = 'dashed'];\n80[label = 'kaiju', color = '#E69F00', style = 'dashed'];\n\n1->0; 2->0; 3->0; 4->0; 5->0; 6->0; 7->0; 8->1; 9->1;\n10->1; 11->1; 12->1; 13->1; 14->1; 15->2; 16->2; 17->2;\n18->2; 19->2; 20->2; 21->2; 8->3; 22->3; 23->3; 24->3;\n25->3; 26->3; 15->4; 27->4; 28->4; 29->4; 30->4; 31->4;\n{32 33 34 35}->5;\n48->{32 33 34 35};\n32->{64 65};\n33->{66 67};\n{64 65 66 67}->61;\n{64 65}->52;\n{66 67}->53;\n34->{71 72};\n35->{73 74};\n{71 72 73 74}->68\n{71 72}->57;\n{73 74}->58;\n\n8->6; 1->6; 15->7; 2->7;\n36->8; 37->9; 8->9; 38->10; 8->10; 9->11; 10->12; 39->13;\n11->13; 9->13; 40->14; 12->14; 10->14; 41->15; 42->16; 15->16;\n43->17; 15->17; 16->18; 17->19; 44->20; 18->20; 16->20; 45->21;\n19->21; 17->21; 46->22; 8->22; 8->23; 8->24; 23->25; 8->25;\n8->26; 47->27; 15->27; 15->28; 15->29; 28->30; 15->30; 15->31;\n49->36; 50->37; 51->38; 52->39;\n53->40; 54->41; 55->42; 56->43; 57->44; 58->45; 59->46; 60->47;\n61->49; 62->50; 63->51; 68->54;\n69->55; 70->56; 8->59; 15->60;\n75->62; 64->62; 65->62; 75->63;\n66->63; 67->63; 76->69; 71->69; 72->69; 76->70; 73->70; 74->70;\n36->75; 41->76; 79->22; 80->27;\n15->77; 8->78; 77->4; 78->3; 59->79; 60->80;\n\n\tgraph [nodesep = 0.1]\n{ rank=same; 13, 14, 20, 21 }\n{ rank=same; 39, 40, 44, 45 }\n{ rank=same; 32, 33, 34, 35 }\n{ rank=same; 64, 65, 66, 67, 71, 72, 73, 74 }\n{ rank=same; 5, 49, 54 }\n\n}\n\")\n\nexport_svg(dag) %>%\n  charToRaw() %>%\n  rsvg() %>%\n  png::writePNG(\"include/dag.png\")\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![DAG of the metagenomic workflow.](include/dag.png){width=1008}\n:::\n\nColors indicate broad divisions of workflow:\nsky blue, short-read prep & co-assembly;\nblueish green, short-read mapping to assembly;\norange, taxonomic or functional classification;\nyellow, automatic binning;\nreddish purple, databases construction.\n\n:::\n\n\n\n\nAnd here are the commands we used to run the workflow.\n\n\n\n\n::: {.cell}\n\n```{.zsh .cell-code}\nanvi-run-workflow -w metagenomics -c default_mg.json \\\n                  --additional-params --jobs 28 \\\n                  --resources nodes=28 \\\n                  --keep-going \\\n                  --rerun-incomplete --unlock\n\nanvi-run-workflow -w metagenomics -c default_mg.json \\\n                  --additional-params --jobs 28 \\\n                  --resources nodes=28 --keep-going --rerun-incomplete\n```\n:::\n\n\n\n\nIf a Snakemake job fails the default behavior is to lock the workflow and, because large jobs can fail for a lot of reasons, we decided to always include an identical command first with the `--unlock` flag. This will unlock the workflow and then the second command will execute. You can find an explanation of this post on [anvi'o snakemake workflows](http://merenlab.org/2018/07/09/anvio-snakemake-workflows/#how-can-i-restart-a-failed-job).\n\n## Snakemake Citations\n\nThere are many tools used in the workflow that need to be cited.\n\n| Job               | Tool                                                         | Reference                                    |\n|----------------|--------------------------------|------------------------|\n| WORKFLOW          | [Snakemake](https://snakemake.readthedocs.io/en/stable/)     | [@koster2012snakemake]                       |\n| QUALITY-FILTERING | [Illumina Utils](https://github.com/merenlab/illumina-utils) | [@eren2013filtering; @minoche2011evaluation] |\n| CO-ASSEMBLY       | [Megahit](https://github.com/voutcn/megahit)                 | [@li2015megahit]                             |\n| GENE CALLING      | [Prodigal](https://github.com/hyattpd/Prodigal)              | [@hyatt2010prodigal]                         |\n| RECRUITMENT       | [BOWTIE2](https://github.com/BenLangmead/bowtie2)            | [@langmead2012bowtie]                          |\n|                   | [SAMtools](http://samtools.sourceforge.net/)                 | [@li2009samtools]                            |\n| CLASSIFICATION    | [CENTRIFUGE](https://github.com/DaehwanKimLab/centrifuge)    | [@kim2016centrifuge]                         |\n\nAlso, there are a few tools that we ran outside of the Snakemake workflow. Results from these steps need to be added to the individual `PROFILE.db`'s, merged `PROFILE.db`, or `CONTIGS.db`. Therefore, before the `anvi-merge` portion of the Snakemake workflow finished, we killed the job, ran the accessory analyses described below, and then restarted the workflow to finish the missing step. Cumbersome, yes, but it got the job done.\n\n# Pipeline Output\n\nWhen the workflow is complete, you should see the following directories within your project folder. \n\nDirectory names and descriptions:\n\n-   **00_LOGS/**: Individual log files for each step of the snakemake workflow.\n-   **01_QC/**: Merged forward (R1) & reverse (R2) QC'ed fastq files. QC STATS file for each sample. `qc-report.txt` file, a summary table of all QC results.\n-   **02_FASTA/**: Contig fasta files and reformat reports for each assembly.\n-   **03_CONTIGS/**: An annotated contig database for each assembly.\n-   **04_MAPPING/**: Short read BAM files.\n-   **05_ANVIO_PROFILE/**: Profile database for each sample.\n-   **06_MERGED/**: Single merged profile database.\n\nThe `contigs.db` contains all taxonomic and functional annotations. The `profile.db` contains information on coverage, detection, etc.\n\n# Annotations\n\nThis part of the workflow contains information for ancillary annotations (i.e., outside the Snakmake workflow). We will mainly cover taxonomic annotations as most functional annotations (e.g., KEGG, COG, Pfam, etc.) are run in the workflow.\n\n## Taxonomic \n\nIn this section we discuss taxonomic classification of short reads, contigs, and gene calls. We go through the steps of analyzing the data and getting the results into anvi'o databases. \n\n\n### Short-reads with Kraken2\n\nIn this section we use Kraken2 [@wood2019kraken2] to classify the **short reads**. Our goal is to classify short-reads, generate an input file for anvi'o, and create [Krona plots](https://github.com/marbl/Krona/wiki) for data visualization. Brace yourself.\n\n::: {.callout-important}\nSince Kraken2 annotation is performed on individual samples and the results are imported into the individual `profile.db`'s we will need to re-merge the all `profile.db` after these steps are completed. The merging step is basically the last step of the Snakemake workflow. \n:::\n\n\n\n\n::: {.cell}\n\n```{.zsh .cell-code}\nconda activate kraken2\nfor sample in `cat sampleskraken2.txt`\ndo\n    kraken2 --paired 01_QC/$sample-QUALITY_PASSED_R1.fastq.gz  \\\n                     01_QC/$sample-QUALITY_PASSED_R2.fastq.gz \\\n                     --db kraken2_db/ \\\n                     --use-names \\\n                     --threads $NSLOTS \\\n                     --output $sample-kraken.out \\\n                     --report $sample-kraken-report.txt\ndone\nconda deactivate \n```\n:::\n\n\n\n\nAfter this is finished we should have two files for each sample--a `.out` file containing the results of the Kraken2 annotation and a `.report.txt` file that summarizes the results. \n\nFirst, we generate the file that anvi'o needs--the format is very specific. For this task we use [KrakenTool](https://github.com/jenniferlu717/KrakenTools)--a suite of very handy scripts to Kraken 2 data. We will use a tool called [`kreport2mpa.py`](https://github.com/jenniferlu717/KrakenTools?tab=readme-ov-file#kreport2mpapy), which takes a Kraken report file and prints out a MPA (MetaPhlAn)-style TEXT file. \n\n\n\n\n::: {.cell}\n\n```{.zsh .cell-code}\nconda activate krakentools\nfor sample in `cat sampleskraken2.txt`\ndo\n    kreport2mpa.py -r $sample-kraken-report.txt -o $sample-kraken-mpa.txt    \ndone\nconda deactivate \n```\n:::\n\n\n\n\nEasy as that. Now we can import all MPA files into their respective contig databases. Here we had to split the sample list by co-assembly because I could not figure out an easier way. \n\n\n\n\n::: {.cell}\n\n```{.zsh .cell-code}\nfor sample in `cat EP_list.txt`\ndo \n    anvi-import-taxonomy-for-layers \\\n                   -p 05_ANVIO_PROFILE/EP/$sample/PROFILE.db \\\n                   --parse krakenuniq \\\n                   -i 07_TAXONOMY/KRAKEN_TAXONOMY/$sample-kraken-mpa.txt\ndone\n\nfor sample in `cat WA_list.txt`\ndo \n    anvi-import-taxonomy-for-layers \\\n                   -p 05_ANVIO_PROFILE/WA/$sample/PROFILE.db \\\n                   --parse krakenuniq -i \\\n                   07_TAXONOMY/KRAKEN_TAXONOMY/$sample-kraken-mpa.txt\ndone\n```\n:::\n\n\n\n\nRad. With this done we can re-merge the profile databases. \n\n\n\n\n::: {.cell}\n\n```{.zsh .cell-code}\nanvi-merge 05_ANVIO_PROFILE/WA/*/PROFILE.db \\\n          -c 03_CONTIGS/WA-contigs.db \\\n          -o 06_MERGED/WA\nanvi-merge 05_ANVIO_PROFILE/EP/*/PROFILE.db \\\n          -c 03_CONTIGS/EP-contigs.db \\\n          -o 06_MERGED/EP\n```\n:::\n\n\n\n\nAlright, time to make some Krona plots. This is a two-steo process. First, we use two scripts from  [metaWRAP](https://github.com/bxlab/metaWRAP). The first, called `kraken2_translate.py`, is used to generate full taxonomic lineages from a taxid. The input file for this is the output of the Kraken2 annotation. The next script is called `kraken_to_krona.py` which takes the output of the first script (translated Kraken file) and parses it into a format that Krona can use to produce plots.\n\n\n\n\n::: {.cell}\n\n```{.zsh .cell-code}\nconda activate metawrap\nfor sample in `cat sampleskraken2.txt`\ndo\n    kraken2_translate.py kraken2_db $sample-kraken.out $sample-kraken.trans\n    kraken_to_krona.py $sample-kraken.trans > $KRAKEN/$sample-kraken.krona\ndone\nconda deactivate \n```\n:::\n\n\n\n\nOnce that is complete, we can use the output files and a script called `ktImportText` from the Krona package to produce HTML Krona plots for each sample. \n\n\n\n\n::: {.cell}\n\n```{.zsh .cell-code}\nconda activate krona\nfor sample in `cat sampleskraken2.txt`\ndo\n    ktImportText $sample-kraken.krona -o $sample-kraken.krona.html\ndone\nconda deactivate \n```\n:::\n\n\n\n\nA plot for every sample. How great is that?\n\n### VirSorter Annotation\n\nTo classify any viral sequences, we ran [VirSorter2](https://github.com/jiarong/VirSorter2) [@guo2021virsorter2] on contigs from the co-assembly using our newly created `contig.db`. First, we need something for VirSorter2 to classify. For that we export fasta files from each anvi'o co-assembly. \n\n\n\n\n::: {.cell}\n\n```{.zsh .cell-code}\nanvi-export-contigs -c 03_CONTIGS/WA-contigs.db \\\n                    -o 03_CONTIGS/WA-splits.fa \\ \n                    --splits-mode --no-wrap\nanvi-export-contigs -c 03_CONTIGS/EP-contigs.db \\\n                    -o 03_CONTIGS/EP-splits.fa \\\n                    --splits-mode --no-wrap\n```\n:::\n\n\n\n\nAnd the code we used to run VirSorter2.\n\n\n\n\n::: {.cell}\n\n```{.zsh .cell-code}\nconda activate virsorter2\nvirsorter run --seqfile 03_CONTIGS/WA-splits.fa \\\n              --working-dir WA/ \\\n              --keep-original-seq \\\n              --prep-for-dramv \\\n              --hallmark-required-on-short \\\n              --db-dir virsorter2_db \nvirsorter run --seqfile 03_CONTIGS/EP-splits.fa \\\n              --working-dir EP/ \\\n              --keep-original-seq \\\n              --prep-for-dramv \\\n              --hallmark-required-on-short \\\n              --db-dir virsorter2_db\nconda deactivate\n```\n:::\n\n\n\n\nNow, to get Virsorter2 annotations into the anvi'o contig databases there are a few special steps that need to be taken. Please [see this post](https://github.com/simroux/VirSorter2_to_Anvio) and associated `virsorter_to_anvio.py` script for more details. Here we will only include the code with minimal explanation. Too long...\n\nFirst we export two tables that the `virsorter_to_anvio.py` script needs for import. \n\n\n\n\n::: {.cell}\n\n```{.zsh .cell-code}\nanvi-export-table 03_CONTIGS/WA-contigs.db  --table splits_basic_info \\\n                  --output-file WA_splits_basic_info.txt\nanvi-export-table 03_CONTIGS/EP-contigs.db  --table splits_basic_info \\\n                  --output-file EP_splits_basic_info.txt\n\nanvi-export-gene-calls -c 03_CONTIGS/WA-contigs.db \\\n                       --output-file WA_all_gene_calls.txt \\\n                       --gene-caller prodigal\nanvi-export-gene-calls -c 03_CONTIGS/EP-contigs.db \\\n                       --output-file EP_all_gene_calls.txt \\\n                       --gene-caller prodigal\n```\n:::\n\n\n\n\nTime to get messy. At the time of this writing,  the `gene_calls` file exported from anvi'o is a 10-column tab-delimited text file. The `virsorter_to_anvio.py` script needs only 8 of these, and they need to be in a specific order. No problem, we can use [`awk`](https://www.gnu.org/software/gawk/manual/gawk.html). \n\n::: {.column-margin}\nThese are the column values needed by the `virsorter_to_anvio.py` script:`gene_callers_id`, `contig`, `start`, `stop`, `direction`, `partial`, `source`, `version`\n:::\n\n\n\n\n::: {.cell}\n\n```{.zsh .cell-code}\nawk 'BEGIN {FS=\"\\t\"; OFS=\"\\t\"} {print $1, $2, $3, $4, $5, $6, $8, $9}' WA_all_gene_calls.txt\nawk 'BEGIN {FS=\"\\t\"; OFS=\"\\t\"} {print $1, $2, $3, $4, $5, $6, $8, $9}' EP_all_gene_calls.txt\n```\n:::\n\n\n\n\nAfter this we run the parsing script. \n\n\n\n\n::: {.cell}\n\n```{.zsh .cell-code}\nvirsorter_to_anvio.py -i WA/ -s WA/WA_splits_basic_info.txt \\\n                      -n WA/WA_all_gene_calls.txt \\\n                      -d virsorter2_db \\\n                      -A WA/WA_virsorter_additional_info.txt \\\n                      -C WA/WA_virsorter_collection.txt \\\n                      -F WA/WA_virsorter_annotations.txt\n\nvirsorter_to_anvio.py -i EP/ -s EP/EP_splits_basic_info.txt \\\n                      -n EP/EP_all_gene_calls.txt \\\n                      -d virsorter2_db \\\n                      -A EP/EP_virsorter_additional_info.txt \\\n                      -C EP/EP_virsorter_collection.txt \\\n                      -F EP/EP_virsorter_annotations.txt\n```\n:::\n\n\n\n\nAnd import the resulting files to anvi'o\n\n\n\n\n::: {.cell}\n\n```{.zsh .cell-code}\nanvi-import-misc-data WA-virsorter_additional_info.txt \n                      -p 06_MERGED/WA/PROFILE.db  \n                      --target-data-table items\nanvi-import-misc-data EP-virsorter_additional_info.txt \n                      -p 06_MERGED/EP/PROFILE.db  \n                      --target-data-table items\n\nanvi-import-collection WA-virsorter_collection.txt \n                      -c 03_CONTIGS/WA-contigs.db \n                      -p 06_MERGED/WA/PROFILE.db \n                      -C VIRSORTER2\nanvi-import-collection EP-virsorter_collection.txt \n                      -c 03_CONTIGS/EP-contigs.db \n                      -p 06_MERGED/EP/PROFILE.db\n                      -C VIRSORTER2\n```\n:::\n\n\n\n\nVirSorter2 annotation complete. \n\n## Kaiju Annotation\n\nHere we use [Kaiju](https://github.com/bioinformatics-centre/kaiju) [@menzel2016kaiju] to classify gene calls. We do this against the [progenomes databases](https://github.com/bioinformatics-centre/kaiju?tab=readme-ov-file#creating-the-kaiju-index), a r epresentative set of genomes from the [proGenomes](http://progenomes.embl.de/) database and viruses from the NCBI RefSeq database. We describe the contruction of this database [here](../setup/index.html#kaiju). \n\nStart by grabbing gene call fasta files. \n\n\n\n\n::: {.cell}\n\n```{.zsh .cell-code}\nanvi-get-sequences-for-gene-calls -c 03_CONTIGS/EP-contigs.db -o EP_gene_calls.fna\nanvi-get-sequences-for-gene-calls -c 03_CONTIGS/WA-contigs.db -o WA_gene_calls.fna\n```\n:::\n\n::: {.cell}\n\n```{.zsh .cell-code}\nconda activate kaiju\nkaiju -t kaiju_db/nodes.dmp \\\n      -f kaiju_db/kaiju_db_progenomes.fmi \\\n      -i EP_gene_calls.fna \\ \n      -o EP_kaiju_nr.out \\\n      -z $NSLOTS -v\n\nkaiju -t kaiju_db/nodes.dmp \\\n      -f kaiju_db/kaiju_db_progenomes.fmi \\\n      -i WA_gene_calls.fna \\\n      -o WA_kaiju_nr.out \\\n      -z $NSLOTS -v\n\nkaiju-addTaxonNames -t kaiju_db/nodes.dmp \\\n                    -n kaiju_db/names.dmp \\\n                    -i EP_kaiju_nr.out \\\n                    -o EP_kaiju_nr.names \\\n                    -r superkingdom,phylum,order,class,family,genus,species\n\nkaiju-addTaxonNames -t kaiju_db/nodes.dmp \n                    -n kaiju_db/names.dmp \\\n                    -i WA_kaiju_nr.out \\\n                    -o WA_kaiju_nr.names \\ \n                    -r superkingdom,phylum,order,class,family,genus,species\nconda deactivate\n```\n:::\n\n\n\n\nImport the output to anvi'o.\n\n\n\n\n::: {.cell}\n\n```{.zsh .cell-code}\nanvi-import-taxonomy-for-genes -c 03_CONTIGS/EP-contigs.db \\\n                               -p kaiju \\\n                               -i EP_kaiju_nr.names \n\nanvi-import-taxonomy-for-genes -c 03_CONTIGS/WA-contigs.db \\\n                               -p kaiju \\\n                               -i WA_kaiju_nr.names\n```\n:::\n\n\n\n\nAnd generate Krona plots of the data. A little dance between the Kaiju and Krona environments. \n\n\n\n\n::: {.cell}\n\n```{.zsh .cell-code}\nconda activate kaiju\nkaiju2krona -t kaiju_db/nodes.dmp \\\n            -n kaiju_db/names.dmp \\\n            -i $KAIJU/WA_kaiju_nr.out \\\n            -o WA_kaiju_nr.out.krona\n\nkaiju2krona -t kaiju_db/nodes.dmp \\\n            -n kaiju_db/names.dmp \\\n            -i EP_kaiju_nr.out \\\n            -o EP_kaiju_nr.out.krona\nconda deactivate\n```\n:::\n\n::: {.cell}\n\n```{.zsh .cell-code}\nconda activate krona\nktImportText -o WA_kaiju_nr.out.html WA_kaiju_nr.out.krona\nktImportText -o EP_kaiju_nr.out.html EP_kaiju_nr.out.krona\nconda deactivate \n```\n:::\n\n::: {.cell}\n\n```{.zsh .cell-code}\nconda activate kaiju\n\nkaiju2table -t kaiju_db/nodes.dmp \\\n            -n kaiju_db/names.dmp \\\n            -r class \\\n            -l phylum,class,order,family \\\n            -o WA_kaiju_nr.out.summary WA_kaiju_nr.out \\\nkaiju2table -t kaiju_db/nodes.dmp \\\n            -n kaiju_db/names.dmp \\\n            -r class \\\n            -l phylum,class,order,family \\\n            -o EP_kaiju_nr.out.summary EP_kaiju_nr.out\n\nconda deactivate\n```\n:::\n\n\n\n\nKaiju done.\n\n#### Source Code {.appendix}\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\nThe source code for this page can be accessed on GitHub `<svg aria-hidden=\"true\" role=\"img\" viewBox=\"0 0 496 512\" style=\"height:1em;width:0.97em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;\"><path d=\"M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z\"/></svg>`{=html} by [clicking this \nlink](https://github.com/istmobiome/trans-shrimp/blob/main/docs/mg-workflows/assembly/index.qmd). \n\n\n\n\n#### Data Availability {.appendix}\n\n\nRaw fastq files available on figshare at [10.25573/data.14686665](https://doi.org/10.25573/data.14686665). Trimmed fastq files (primers removed) available through the ENA under project accession number [PRJEB45074 (ERP129199)](https://www.ebi.ac.uk/ena/browser/view/PRJEB45074). Data generated in this workflow can be accessed on figshare at [10.25573/data.14687184](https://doi.org/10.25573/data.14687184).\n\n\n\n#### Last updated on {.appendix}\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"2024-09-29 11:02:02 PDT\"\n```\n\n\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}